---
layout: post
title:  "卷积与反卷积"
date:   2019-08-22 15:22:01 +0800
categories: 人工智能
tag: 深度学习基础
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- [Convolution arithmetic tutorial](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)

****

**以下只考虑 2-D 方形卷积。**

****

# 卷积运算(Convolution)

## 卷积运算过程

卷积运算的具体运算过程如下图所示，每次卷积时，将卷积核与对应的感受野内的元素分别相乘后相加，作为对应位置的输出：

<div style="text-align:center">
<img src="/images/卷积运算过程.gif" width="80%"/>
</div><br>

这里只是以单层输入通道为例，如果为多通道，则每层输入通道的卷积核和进行累加，作为输出通道对应位置的值。

## 运算参数

> - 输入尺寸          i
> - 卷积核尺寸     k
> - 步长                 s
> - Padding          p
> - 输出尺寸        o

##　卷积运算类型

### padding = 0,  stride = 1

<div style="text-align:center">
<img src="/images/p0s1.gif" width="30%"/>
</div><br>

### padding > 0,  stride = 1

<div style="text-align:center">
<img src="/images/p gt 0s1.gif" width="40%"/>
</div><br>


### padding = 0,  stride > 1

<div style="text-align:center">
<img src="/images/p0s gt 1.gif" width="30%"/>
</div><br>


### padding > 0,  stride > 1

<div style="text-align:center">
<img src="/images/p gt 0s gt 1.gif" width="40%"/>
</div><br>


## 输出特征尺寸计算

综上所述，卷积层的输出特征尺寸与输入特征尺寸以及各种参数的关系如下所示： 

$$
o = \lfloor \frac{i+2p-k}{s} \rfloor + 1
$$

其中，$$\lfloor x \rfloor$$ 表示对 $$x$$ 向下取整。

# 转置卷积运算（Transposed convolution）

## 卷积与反卷积的关系

由于某些场合需要一般意义上卷积运算的反向过程，因此，对于转置卷积的需求日益增高。例如，我们**可以将转置卷积运算作为卷积自编码器的解码层**，或者**将特征图映射到更高维度上**。

**反卷积主要用于神经网络可视化，常用于场景分割，生成模型等领域**。一般来说，反卷积又称之为转置卷积，即：`Transposed Convolution`，`Fractional Strided Convolution` 等等。

反卷积运算与卷积运算是正好相反的过程。下面来看一下，卷积运算的反向传播，其推导如下：  

$$
\begin{aligned}
\frac{\partial Loss}{\partial x_j} &= \sum_i \frac{\partial Loss}{\partial y_i} \frac{\partial y_i}{\partial x_j} \\
&= \sum_i \frac{\partial Loss}{\partial y_i} C_{i,j} \\
&= \frac{\partial Loss}{\partial y}  C_{*,j} \\
&= C_{*,j}^{T}\frac{\partial Loss}{\partial y}
\end{aligned}
$$

也就是说，反卷积实际上是左乘 $$C^T$$ 的过程。综

上所述：卷积运算的前向传播与反向传播分别乘以矩阵 C 和 $$C^T$$，而反卷积运算则分别乘以 $$C^T$$ 和 C。

## 运算参数

> - 输入尺寸          i
> - 卷积核尺寸     k
> - 步长                 s
> - Padding          p
> - 输出尺寸        o

## 卷积运算的矩阵化

实际上，卷积运算可以表示为矩阵相乘。如下图所示，其中，$$ i = 4, k = 3, s = 1, p = 0, o = 2 $$：

<div style="text-align:center">
<img src="/images/卷积运算矩阵化.gif" width="30%"/>
</div><br>

在这里，假设输入 `X`，输出位 `Y` 。在卷积核每一次滑动窗口时，进行一次卷积运算。可以在每次卷积运算中，将卷积核映射到与输入同尺寸，然后展开成一维向量，则整个卷积运算过程可以展开成一个 $$ [4,16] $$ 的稀疏矩阵 `C`，其中非 `0` 元素 $$w_{i,j}$$ 表示卷积核的第 `i` 行和第 `j` 列。如下所示：

<div style="text-align:center">
<img src="/images/卷积运算矩阵化 2.jpg" width="98%"/>
</div><br>


经过上面的转换，则整个卷积层的运算可以表示为：$$ Y=CX $$，最后得到输出向量 `Y` 的尺寸为 $$ [4, 1] $$，再将其还原为 $$ [2, 2] $$ 的矩阵，即为输出特征图。

通过这种方式，可以通过 $$ C^T $$ 很容易的计算出反向过程。也就是说，误差可以通过将 `loss` 乘以 $$ C^T $$ 来进行反向传播。更重要的是，正向传播和反向传播所使用的 $$ C, C^T $$ 都是由权值参数 `w` 构成。（`有点儿不理解，暂时放着`）

## 转置卷积的概念

下面来看看如何将一个 $$ [4,1] $$ 的向量映射为 $$ [16, 1] $$ 的向量，同时还保持卷积运算的连接模式。方法是通过转置卷积（`Transposed convolution`）。

`Transposed convolution` 有时候也被称为 `fractionally strided convolutions`，本质上就是卷积运算的逆过程。

要理解这个并不难，因为卷积的本质是由 `kernel` 定义的，而具体的是卷积运算（`direct convolution`）还是转置卷积，只不过是前向运算还是反向运算的区别而已。  

例如，当 `kernel` 参数 $$ w $$ 确定时，正向传播和反向传播可以通过分别乘以 C 和 $$ C^T $$ 进行。而同时，转置卷积的正向和反向运算分别乘以： $$ C^T $$ 和 $$ (C^T)^T $$ 即可。  

转置卷积一定程度上可以视作卷积相对于输入的梯度，这也是转置卷积的实现方式。

## 转置卷积的类型

基于上面的介绍，下面来看一下各种类型的卷积对应的转置卷积。其中，条件表示卷积运算的条件。实际上，处理转置卷积最简单的方式就是计算卷积的输入输出尺寸，然后将其分别作为转置卷积的输出和输入尺寸。  

### padding = 0, stride = 1, transposed

**direct convolution**

$$ i = 4, k = 3, s = 1, p = 0, o = 2 $$，对应的运算示意图如下：

<div style="text-align:center">
<img src="/images/transposed   i4 k3 s1 p0 o2 .gif" width="30%"/>
</div><br>

**transposed convolution** 

根据上面的卷积过程，可以得到对应的转置卷积的相关尺寸为：$$ i^{'} = o = 2, o^{'} = i = 4 , k^{'} = k = 3, s^{'} = s = 1 $$。现在问题在于，如何确定 $$ p^{'} $$。实际上可以直接通过其他参数推导出来。

问题是，怎么理解呢？  

可以这样想，在卷积运算中，原始输入最左上角的一个元素只参与过一次运算，也就是说，只对输出的最左上角元素有贡献。因此，反向过程中，要得到原始输入最左上角的元素，对应的对输出的单次卷积必须只能涉及到原始输出矩阵的最左上角的那个元素。如下图所示，要做到这一点，`padding` 尺寸必须为： $$ k^{'} - 1 $$。

<div style="text-align:center">
<img src="/images/transposed   i4 k3 s1 p0 o2 2 .gif" width="40%"/>
</div><br>

除此之外，其余部分与普通卷积运算完全一致，因此，代入相应的数值之后，得到输出尺寸为：

$$
o^{'} = \lfloor \frac{i^{'} + 2p^{'} - k^{'}}{s^{'}} \rfloor + 1
$$

### padding > 0, stride = 1, transposed

根据上面的推理，当 $$ p > 0 $$ 时，原理是一样的，只不过对输入的 `padding` 会减少。

**direct convolution** 

$$
i = 5, k = 4, s = 1, p = 2, o = 6
$$

<div style="text-align:center">
<img src="/images/transposed   i4 k3 s1 p0 o2 3.gif" width="40%"/>
</div><br>

**transposed convolution**

易得，转置卷积的相关尺寸为：$$ i^{'} = o = 6, o^{'} = i = 5 , k^{'} = k = 4, s^{'} = s = 1 $$，问题还是，怎么计算 `padding` 尺寸，之即计算很容易得出，但是怎么理解呢？  

可以这样想，还是看原始输入的最左上角的一个元素，在整个卷积层运算过程中，它被包含了 $$ 3 \times 3 = 9 $$ 次，也就是说，对于原始输出最左上角的 9 个元素有贡献。

所以，根据原始输出反向获取原始输入时，应该只有这 9 个元素参与该原始输入最左上角的元素的还原。而卷积核尺寸为 4，所以，显然 `padding` 尺寸为：$$ k^{'} - (p + 1)  = 4 - (2 + 1) = 1$$。如下所示：

<div style="text-align:center">
<img src="/images/transposed   i5 k4 s1 p2 o6 2.gif" width="40%"/>
</div><br>


输出尺寸为：

$$
o^{'} = \lfloor \frac{i^{'} + 2p^{'} - k^{'}}{s^{'}} \rfloor + 1
$$

### padding = 0, stride > 1, transposed

**direct convolution**

$$ i = 5, k = 3, s = 2, p = 0, o = 2 $$：

<div style="text-align:center">
<img src="/images/transposed   i5 k3 s2 p0 o2 1.gif" width="30%"/>
</div><br>

**transposed convolution**

易得，转置卷积的相关尺寸为：$$ i^{'} = o = 2, o^{'} = i = 5 , k^{'} = k = 3 $$。那么这里怎么实现呢？

> 首先**在每个元素之间插入 $$ s - 1 $$ 个空格**，然后用**步长  1 进行卷积**，且 **padding 尺寸为 $$ k - 1 $$**。如何理解呢？

可以看正卷积的示意图：输入特征图的左上角 4 个方格只对输出的第一个方格有贡献。所以反之，反卷积中，输出特征图中的左上角 4 个方格只能从输入的第一个格点获取输入。如下图所示：

<div style="text-align:center">
<img src="/images/transposed   i5 k3 s2 p0 o2 2.gif" width="40%"/>
</div><br>


输出尺寸为：

$$
o^{'} = \lfloor \frac{i^{'} + 2p^{'} - k^{'}}{s^{'}} \rfloor + 1
$$

### padding > 0, stride > 1, transposed

**direct convolution**

$$ i = 5, k = 3, s = 2, p = 1, o = 3 $$：

<div style="text-align:center">
<img src="/images/transposed   i5 k3 s2 p1 o3 1.gif" width="40%"/>
</div><br>

**transposed convolution**

易得，转置卷积的相关尺寸为：$$ i^{'} = o = 3, o^{'} = i = 5 , k^{'} = k = 3 $$。那么这里怎么实现呢？

首先**在每个元素之间插入 $$ s - 1 $$ 个空格**，然后用**步长为 1 **进行卷积，且 **padding 尺寸为 $$ k^{'} - (p + 1)  = 3 - (1 + 1) = 1 $$**。如何理解呢？  

看正卷积中，输入特征图中最左上角的格点只被包含过一次，所以在反卷积中，输出特征图的最左上角的节点应该只能由输入节点的最左上角节点参与。

再看正卷积中第一行第二列的节点，在卷积过程中，它参与过两次卷积，分别对输出特征图的第一个和第二个节点产生影响，所以在反卷积过程中，输出特征图的第二个节点的生成应该包含且仅包含输入特征图的开始两个节点。如下图所示：

<div style="text-align:center">
<img src="/images/transposed   i5 k3 s2 p1 o3 2.gif" width="40%"/>
</div><br>


输出尺寸为：

$$
o^{'} = \lfloor \frac{i^{'} + 2p^{'} - k^{'}}{s^{'}} \rfloor + 1
$$

# Pytorch 接口

对于 `2-D` 数据，卷积与转置卷积接口分别为：  

```Python
# 卷积
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)

# 转置卷积
torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,output_padding=0, groups=1, bias=True, dilation=1)
```

其中要注意的是，对于转置卷积中，`padding` 不是按照上面的方式选定的。实际操作中，内部已经封装完成。比方说，`Conv2d(16,32,3,2,0,1)` 对应的转置卷积为 `ConvTranspose2d(32,16,3,2,0,1)`。其中通道数随意，但是 `stride` 等参数是对应的，内部自动进行处理，不用人工计算。