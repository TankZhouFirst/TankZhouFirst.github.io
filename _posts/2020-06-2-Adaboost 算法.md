---
layout: post
title:  "Adaboost 算法"
date:   2020-06-02 12:28:01 +0800
categories: 人工智能
tag: 机器学习
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**：

- [大话 AdaBoost 算法](<https://zhuanlan.zhihu.com/p/37671791>)
- [手把手教你实现一个 AdaBoost](<https://www.ibm.com/developerworks/cn/analytics/library/machine-learning-hands-on6-adaboost/index.html>)
- [Adaboost 算法的原理与推导](https://blog.csdn.net/v_JULY_v/article/details/40718799)
- [adaboost 原理图解](<https://blog.csdn.net/YUAN1125/article/details/72757752>)

****

# Adaboost 的基本概念

## 基本定义

`Adaboost` 即自适应增强学习(`Adaptive Boosting`)，是集成学习算法的一种，主要用于二分类问题的算法。

> Adaboost 只是一种框架，内部的弱分类器可以是任何模型。Adaboost 不用担心过拟合！

> Adaboost 的**核心思想**是：关注之前被错分的样本，准确率高的弱分类器有更大的权重。

`Adaboost` 的自适应性体现在：

1. 下一个弱分类器会着重关注前一个分类器分错的样本。
2. 最后的强分类器由多个弱分类器通过某总组合而成。
3. 一直迭代训练新的弱分类器，直到强分类器满足一定的准确度，或者用尽迭代次数为止

## 强  / 弱分类器

强分类器的计算公式为：

$$
F(\mathrm{x})=\sum_{i=1}^{T} \alpha_{t} f_{t}(\mathrm{x})
$$

其中 $$x$$ 是输入向量，$$F(x)$$ 是强分类器， $$ f_t(x) $$ 是弱分类器， $$ \alpha_t $$ 是弱分类器的权重，$$T$$ 为弱分类器的数量，弱分类器的输出值为 $$+1$$ 或 $$-1$$，分别对应正样本和负样本。

弱分类器及其权重通过训练算法得到。之所以叫弱分类器，是因为它们的精度不用太高，对于二分类问题，只要保证准确率大于 $$0.5$$ 即可，即比随机猜测强。

## Adaboost 的优缺点

### 优点

- `Adaboost` 提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选。
- `Adaboost` 算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。
- 训练的错误率上界，随着迭代次数的增加，会逐渐下降
- `Adaboost` 算法不容易出现过拟合的问题

### 缺点

- 在 `Adaboost` 训练过程中，`Adaboost` 会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致 `Adaboost` 算法易受噪声干扰。
- 此外，`Adaboost` 依赖于弱分类器，而要强分类器达到足够的性能，需要足够的弱分类器，所以总时间会较长。

# Adaboost 算法解析

## 算法流程图解

假设要对下图中的点进行分类。

<div style="text-align:center">
<img src="/images/Adaboost 1.png" width="30%"/>
</div><br>

如上图所示，不同形状颜色分别表示不同的类别。假设弱分类器只有横线或者竖线。

首先，用一个弱分类器 $$h_1$$ 进行分类，得到新的样本 $$D2$$。其中，尺寸增大的表明权值增大；反之表示权值减小。如下所示：

<div style="text-align:center">
<img src="/images/Adaboost 2.png" width="70%"/>
</div><br>

后面的步骤一样，如下所示：

<div style="text-align:center">
<img src="/images/Adaboost 3.png" width="70%"/>
</div><br>

<div style="text-align:center">
<img src="/images/Adaboost 4.png" width="40%"/>
</div><br>

最后，将训练好的弱分类器进行整合，得到强分类器。如下所示：

<div style="text-align:center">
<img src="/images/Adaboost 5.png" width="80%"/>
</div><br>

## 算法流程精解

以二分类为例，给定训练样本 $$ (x_i, y_i) $$ ，其中 $$ x_i $$ 是特征向量， $$ y_i $$ 为类别标签，其值为 +1 或 -1。

> 以下公式都是基于二分类，且标签为 $$\{-1, +1\}$$ 的情况讨论的。但是其基本思路是通用的。

### 初始化

根据最大熵原理，在没有任何先验知识的前提下作等概率假设是最合理的。因此，可以初始化所有样本的权重为：

$$
w_{i}^{0}=\frac{1}{N}, i=1, \ldots, N
$$

其中，$$w_{i}^{ 0 }$$ 表示在第 0 次迭代中，第 $$i$$ 个样本的权重，$$N$$ 表示样本数目。

### 训练弱分类器 $$G^{m}$$

假设根据上一个弱分类器，调整了样本分布，这里先称之为当前样本分布。假设其样本 $$i$$ 的权重为 $$ w^{m}_{i} $$，现需要根据当前样本分布，训练一个新的弱分类器 $$G^{m} (x) \to \{-1, +1\}$$。

每一个弱分类器的训练，与一般模型的训练别无二致。在多次参数迭代中，选定使得模型在当前样本分布上误差率最小的作为当前样本分布对应的弱分类器。

误差率的计算方式如下所示：

$$
\begin{aligned}
e^{m} &=\sum_{i=1}^{N} P\left(G^{m}\left(x_{i}\right) \neq y_{i}\right) \\
&=\sum_{i=1}^{N} w^{m}_{i} I\left(G^{m}\left(x_{i}\right) \neq y_{i}\right)
\end{aligned}
$$

其中，样本 $$i$$ 出现的概率即为其对应的权重。因此，弱分类器在训练集上的误差率就是被其误分类的样本的的权值之和。

### 计算当前弱分类器的权重 $$\alpha^m$$

弱分类器的权重值根据它的误差率构造，精度越高的弱分类器权重越大。如下所示：

$$
\alpha^{m}=\frac{1}{2} \log \frac{1-e^{m}}{e^{m}}
$$

由上式可知，当 $$ e^{m} \leq \frac{1}{2} $$ 时，$$ \alpha^{m} \geq 0 $$ ，并且 $$ \alpha^m $$ 随着 $$ e^m $$ 的减小而增大，所以**在本轮分类误差率越小的基本分类器在最终分类器中的作用越大。**

### 更新样本分布

对于每个样本，通过如下方式来更新权重：

$$
w_i^{m+1} = \frac{w_i^m}{Z^m}exp(-\alpha^m y_i G^m(x_i)), i = 1, 2, \dots, N
$$

其中，$$Z^m$$ 为规范因子：

$$
Z^{m}=\sum_{i=1}^{N} w^{m}_{i} \exp \left(-\alpha^{m} y_{i} G^{m}\left(x_{i}\right)\right)
$$

上面权值更新的计算，其中，$$y_i G^m(x_i)$$ 即预测正确为 1，否则为 -1，因此可以写成如下形式：

$$
w^{m+1}_{i}=\left\{\begin{array}{ll}{\frac{w^{m}_{i}}{Z^{m}} e^{-\alpha^{m}},} & {G^{m}\left(x_{i}\right)=y_{i}} \\ {\frac{w^{m}_{i}}{Z^{m}} e^{\alpha^{m}},} & {G^{m}\left(x_{i}\right) \neq y_{i}}\end{array}\right.
$$

换句话说，就是分类正确的样本，其权值将会减小；而分类错误的样本，其权值将会增加。

### 构建强分类器

每一次创建新的弱分类器之后，都需要重新构建强分类器，来判定是否满足要求。

强分类器的创建如下所示：

$$
G(x)=sign(f(x)) = sign(\sum_{m=1}^{M} \alpha^{m} G^{m}(x))
$$


### 终止条件

每次更新强分类器之后，需要验证强分类器在训练集上的准确度。弱准确度满足要求，或者到达指定迭代次数，则终止迭代。

否则，重复进行 `新的弱分类器训练 -> 弱分类器权重计算 -> 样本权值更新` 等过程。

# Adaboost 实例

设定现在有如下数据：

| x    | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| y    | 1    | 1    | 1    | -1   | -1   | -1   | 1    | 1    | 1    | -1   |

现在需要用 `Adaboost` 算法计算得出一个强分类器。

## 初始化样本权重

首先初始化样本权重为（0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1）。

## 第一轮迭代

### 选择最优弱分类器 

假定选定如下值作为切分点：$$ 2.5, 5.5, 8.5 $$。其对应误差率分别如下所示：

| 切分点 | 错误点     | 误差率 |
| ------ | ---------- | ------ |
| 2.5    | 6，7，8    | 0.3    |
| 5.5    | 3，4，5，9 | 0.4    |
| 8.5    | 3，4，5    | 0.3    |

因此，选择误差率最小的，这里选 2.5 为阈值，作为本轮迭代的弱分类器，即：

$$
G^1(x) = \begin{cases}
 1 \quad & \text{if x < 2.5} \\
 -1  \quad & \text{if x > 2.5} \\
\end{cases}
$$


### 计算最优弱分类器的权重

$$
alpha = 0.5 * ln((1 – 0.3) / 0.3) = 0.4236
$$

### 更新样本权重

对于分类正确的样本  $$0，1，2，3，4，5，9$$，样本权重为：

$$
0.1 \times exp(-0.4236) = 0.0655
$$

对于分类错误的样本 $$ 4 $$，其样本权重为：

$$
0.1 \times exp(0.4236) = 0.1528
$$

然后进行规范化：

$$
\begin{aligned}
Z = 0.0655 * 7 + 0.1528 * 3 &= 0.9169 \\

0.0655 / 0.9169 &= 0.0714 \\

0.1528 / 0.9169 &= 0.1666
\end{aligned}
$$

因此，最终权值为：

| 样本点              | 样本权重                    | 规范化权重 |
| ------------------- | --------------------------- | ---------- |
| 0，1，2，3，4，5，9 | 0.1 * exp(-0.4236) = 0.0655 | 0.0715     |
| 6，7，8             | 0.1 * exp(0.4236) = 0.1528  | 0.1666     |

### 更新强分类器，并计算正确率

此时强分类器为：

$$
G(x) = sign(0.4236 * G^1(x))
$$

将训练样本依次输入 $$G(x)$$，得到 3 个错误分类。因此，强分类器的错误率为 `3/10 = 0.3`。

## 第二轮迭代

### 选择最优弱分类器 

选择同样的切分点：$$ 2.5, 5.5, 8.5 $$。其对应误差率分别如下所示：

| 切分点 | 错误点     | 误差率              |
| ------ | ---------- | ------------------- |
| 2.5    | 6，7，8    | 0.1666 * 3          |
| 5.5    | 0，1，2，9 | 0.0715 * 4          |
| 8.5    | 3，4，5    | 0.0715 * 3 = 0.2143 |

因此，选择误差率最小的，这里选 8.5 为阈值，作为本轮迭代的弱分类器，即：

$$
G^2(x) = \begin{cases}
 1 \quad & \text{if x < 8.5} \\
 -1  \quad & \text{if x > 8.5} \\
\end{cases}
$$


### 计算最优弱分类器的权重

$$
alpha = 0.5 * ln((1 – 0.2143) / 0.2143) = 0.6496
$$

### 更新样本权重

| 样本点     | 样本权重              | 规范化权重 |
| ---------- | --------------------- | ---------- |
| 0，1，2，9 | 0.0715 * exp(-0.6496) | 0.0455     |
| 3，4，5    | 0.0715 * exp(0.6496)  | 0.1667     |
| 6，7，8    | 0.1666 * exp(-0.6496) | 0.1060     |

### 更新强分类器，并计算正确率

此时强分类器为：

$$
G(x) = sign(0.4236 * G^1(x) + 0.6496 * G^2(x))
$$

将训练样本依次输入 $$G(x)$$，得到 3 个错误分类。因此，强分类器的错误率为 `3/10 = 0.3`。

## 第三轮迭代

### 选择最优弱分类器 

选择同样的切分点：$$ 2.5, 5.5, 8.5 $$。其对应误差率分别如下所示：

| 切分点 | 错误点     | 误差率             |
| ------ | ---------- | ------------------ |
| 2.5    | 6，7，8    | 0.1060 * 3         |
| 5.5    | 0，1，2，9 | 0.0455 * 4 = 0.182 |
| 8.5    | 3，4，5    | 0.1667 * 3         |

因此，选择误差率最小的，这里选 5.5 为阈值，作为本轮迭代的弱分类器，即：

$$
G^3(x) = \begin{cases}
 -1 \quad & \text{if x < 5.5} \\
 1  \quad & \text{if x > 5.5} \\
\end{cases}
$$

### 计算最优弱分类器的权重

$$
alpha = 0.5 * ln((1 – 0.182) / 0.182) = 0.7514
$$

### 更新样本权重

| 样本点     | 样本权重              | 规范化权重 |
| ---------- | --------------------- | ---------- |
| 0，1，2，9 | 0.0455 * exp(0.7514)  | 0.125      |
| 3，4，5    | 0.1667* exp(-0.7514)  | 0.102      |
| 6，7，8    | 0.1060 * exp(-0.7514) | 0.065      |

### 更新强分类器，并计算正确率

此时强分类器为：

$$
G(x) = sign(0.4236 * G^1(x) + 0.6496 * G^2(x) + 0.7514 * G^3(x))
$$

将训练样本依次输入 $$G(x)$$，分类完全正确，误差率为 0，停止迭代。

# Adaboost 相关公式推导

## Adaboost 误差界推导

根据上面的计算方式，可得 `Adaboost` 最终的训练误差上界为：

$$
\frac{1}{N} \sum_{i=1}^{N} I\left(G\left(x_{i}\right) \neq y_{i}\right) \leq \frac{1}{N} \sum_{i} \exp \left(-y_{i} G\left(x_{i}\right)\right)=\prod_{m} Z^{m}
$$

其中，对于前半部分，因为 $$G(x_i) \neq y_i$$ 时，$$y_i G(x_i) < 0$$，因此：$$ exp(-y_i G(x_i)) \geq 1$$，得证！

对于后半部分，因为：

$$
w^{m+1}_{i}=\frac{w^{m}_{i}}{Z^{m}} \exp \left(-\alpha^{m} y_{i} G^{m}\left(x_{i}\right)\right)
$$

所以有：

$$
\exp \left(-\alpha^{m} y_{i} G^{m}\left(x_{i}\right)\right) = Z^{m}\frac{w^{m+1}_{i}}{w^{m}_{i}}
$$

又因为初始化时，权值参数为：$$w^1_i = \frac{1}{N}$$。所以：

$$
\begin{aligned}
f &= \frac{1}{N} \sum_i exp(-y_i G(x_i)) \\
&= w_i^1 \sum_i exp(-\sum_{m=1}^M \alpha^m y_i G^m(x_i)) \\
&= w_i^1 \sum_i \prod_{m=1}^M exp(-\alpha^m y_i G^m(x_i)) \\
&= w_i^1 \sum_i \prod_{m=1}^M Z^{m}\frac{w^{m+1}_{i}}{w^{m}_{i}} \\
&= w_i^1 \sum_i \prod_{m=1}^M \frac{w^{m+1}_{i}}{w^{m}_{i}} \prod_{m=1}^M Z^{m} \\
&= w_i^1 \sum_i \frac{w^{M+1}_{i}}{w^{1}_{i}} \prod_{m=1}^M Z^{m} \\
&= \sum_i w^{M+1}_{i} \prod_{m=1}^M Z^{m} \\
&= \prod_{m=1}^{M+1} Z^{m}
\end{aligned}
$$

由此可得，可以在每一轮迭代时，选取适当的弱分类器，使得总的 $$Z^m$$ 最小，从而快速减小强分类器的误差。

接这上面的累积，来求上界。对于二分类而言，有：

$$
\prod_{m=1}^{M+1} Z^{m}=\prod_{m=1}^{M+1}\left(2 \sqrt{e^{m}\left(1-e^{m}\right)}\right)=\prod_{m=1}^{M+1} \sqrt{\left(1-4 (\gamma^{m})^{2}\right)} \leq \exp \left(-2 \sum_{m=1}^{M+1} (\gamma^{m})^{2}\right)
$$

其中，$$\gamma^m = \frac{1}{2} - e^m$$：

$$
\begin{aligned}
Z^{m}&=\sum_{i=1}^{N} w^{m}_{i} \exp \left(-\alpha^{m} y_{i} G^{m}\left(x_{i}\right)\right) \\
&= \sum_{y_{i}=G^{m}(x_i)} w^{m}_{i} e^{-\alpha^{m}}+\sum_{y_{i} \neq G^{m}\left(x_{i}\right)} w^{m}_{i} e^{\alpha^{m}} \\
&= \left(1-e^{m}\right) e^{-\alpha^{m}}+e^{m} e^{\alpha^{m}} \\
&= 2 \sqrt{e^m(1-e^m)} \\
&= \sqrt{1-4(\gamma^m)^2}
\end{aligned}
$$

其中，后半段使用泰勒展开证明。

这个结论表明，`Adaboost` 的训练误差以指数速率下降的。另外，`Adaboost` 算法不需要事先知道下界 `γ`，`Adaboost` 具有自适应性，它能适应弱分类器各自的训练误差率 。



## 其他推导

参考博客 [Adaboost 算法的原理与推导](https://blog.csdn.net/v_JULY_v/article/details/40718799)。