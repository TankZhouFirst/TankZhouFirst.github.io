---
layout: post
title:  "强化学习的基本概念"
date:   2019-08-19 17:33:01 +0800
categories: 人工智能
tag: 强化学习
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

-   [吴恩达 机器学习](https://www.bilibili.com/video/av29430384/?p=16)
-   [从零开始认识强化学习](http://sealzhang.tk/iQuant/chapter_5.html)

****

# 强化学习的基本概念

## 强化学习的基本定义

机器学习分为**监督学习**，**非监督学习**等。而强化学习属于中间的状态，它是**通过与环境之间的交互和反馈来学习的**。强化学习的基本模型就是**个体——环境的交互**。

原因之一在于，它不是一个一次性的决策过程，必须随着时间的推移不断做出决策，这是一个渐进的决策过程。

>   **强化学习的核心**在于，通过价值函数来表征模型学习的好坏。
>
>   **强化学习的目标**是希望个体从环境中获得的总奖励最大，即我们的目标不是短期的某一步行动之后获得最大的奖励，而是希望长期地获得更多的奖励。
>
>   也就是说，**强化学习的本质**在于搞清楚，奖励产生的原因。比如说，踩完刹车后还是撞车了，无法区分是因为踩刹车还是因为刹车前的一系列动作导致的撞车。

## 强化学习的组成部分

-   **个体 `agent`**

    决策的主体，即：依据某一策略，采取一系列行动，并期望获取最高收益的行动者。

-   **环境 `environment`**

    对个体的行动进行反馈的另一部分。

-   **行动 / 状态 / 奖励**

    个体可以采取一定的**行动**（`action`），这样的行动是施加在环境中的。环境在接受到个体的行动之后，会反馈给个体环境目前的**状态**（`state`）以及由于上一个行动而产生的**奖励**（`reward`）

## 任务类型

### 回合制任务

对于像下围棋这样**存在一个终止状态**，并且所有的奖励会在这个终止状态及其之前结算清的任务我们称之为**回合制任务**（`episodic task`）。

### 连续任务

还存在另外的一类任务，它们并**不存在一个终止状态**，即原则上它们可以永久地运行下去，这类任务的奖励是分散地分布在这个连续的一连串的时刻中的，我们称这一类任务为**连续任务**（`continuing task`）。

# 马可夫决策过程 MDP

**马可夫决策过程**（`Markov dicision process, MDP`）指的是，在每个状态下，可以由个体从可能的**行动空间** `A(s)` 中选择一个**行动** `a`，紧接着的**状态转移概率**随着所选择行动的不同而不同。

## 马科夫性

对于一个离散有限的状态空间 $$S^+$$ 中的每一个状态 S，它**转移到其他状态的概率只取决于该状态本身**，即所说的**马可夫性**，（**理论基础**），即：

$$
P_r[S_{t+1} \mid S_0, S_1, \dots, S_t] = P_r[S_{t+1} \mid S_t]
$$


>   马科夫性使得我们可以仅仅利用当前状态来估计接下来的收益，即：仅仅使用当前状态来估计的策略并不比使用所有历史的策略差。

可以说马可夫性带给我们了极大的计算上的便利，我们不用每一步都去处理所有的历史步骤，而是只需要面对当前的状态来进行处理。

同时注意到，可能有些信息并没有被完整的包含到模型的状态信号 $$S_t$$ 中，这使得模型并不满足马可夫性。不过我们一般还是认为它们是**近似地**满足马可夫性的，因此仅仅使用当前状态的信息来做出对于未来收益的预测和行动的选择并不是很差的策略。

同时，如果选取不仅仅是当前时刻的状态状态，而是之前的多个时刻的状态叠加在一起作为当前状态，一般可以**增强马可夫性**。

## MDP 组成成分

`MDP` 由一个 `tuple` 组成，即：$$(S, A, {P_{sa}}, γ, R)$$。

-   **状态集**(`set of state`) — S

    表示所有可能的状态。在**有限状态的马可夫决策过程**（`finite MDP`）中，该集合元素格式是有限的；

-   **动作集**(`set of actions`) — A(s)

    表示处于状态 `S` 下时，可能的行动空间，在有限状态的马可夫决策过程中，每一个动作集集合元素个数都是有限的；

-   **状态转换概率分布分布**(`state transition distribution`) — $$P(s^{'} \mid s, a)$$ 

    表示在状态 `s` 下，采取行动 `a ∈ A(s)` 时跳转到状态 $$s^{’}$$ 的概率，有：

    $$
    \sum_{S^`}P_{SA}(S^`)=1  \quad\quad\quad  P_{SA}(S^`) \geq 0
    $$

-   **即时奖励** — R

    $$S \times A  \to \Bbb{R}$$  奖励函数，也可以写作：$$S   \to \Bbb{R}$$。$$R(s^{'}, s, a)$$ 表示在状态 `s` 下，采取行动 $$a \in A(s)$$，跳转到状态 $$s^{'}$$ 时，取得的**即时奖励**。

    $$
    R(s^{'}, s, a) = \Bbb{E}[R_{t + 1} \mid S_t = s, A_t = a, S_{t + 1} = s^{'}]
    $$

-   **贴现因子**(`discount factor`) — $$ \gamma $$

    用于权衡奖励随时间的衰减程度，通常额定值为 `0` ，严格小于 `1`，即：$$ \gamma \in [0, 1) $$ 。

## 收益

由于我们的目标是希望获得的总奖励最大，因此我们希望量化地定义这个总奖励，这里我们称之为**收益**（`return`）。

对于**回合制任务**而言，我们可以很直接定义收益为：

$$
G_t = R_{t+1} + R_{t+2} + \dots + R_{T}
$$

其中 $$T$$ 为回合结束的时刻，即 $$S_T$$ 属于终止状态。

对于**连续任务**而言，不存在一个这样的终止状态，因此，这样的定义可能会在连续任务中发散。因此我们引入另外一种收益的计算方式，称之为**衰减收益**（`discounted return`）。

$$
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} +  \gamma^2 R_{t+3}  + \dots \\
&= \sum_{k=0}^{\infin} \gamma^k R_{t+k+1}
\end{aligned}
$$

其中**衰减率**（`discount factor`）$$ \gamma $$ 满足 $$ 0 \leq \gamma \leq 1 $$。这样的定义也很好理解，相比于更远的收益，我们会更加偏好临近的收益，因此对于离得较近的收益权重更高。

## 策略

强化学习的结果是就是这样一个行动决策，我们称**策略**（`policy`）。

**策略**表示的是状态到行动的映射，其给出了在每个状态下我们应该采取的行动，我们可以把这个策略记做 $$\pi(a \mid s)$$ ，它表示在状态 `s` 下采取行动 `a` 的概率。

如果我们可以计算出每个状态或者采取某个行动之后收益，那么我们每次行动就只需要采取收益较大的行动或者采取能够到达收益较大状态的行动。

特别的，**最佳决策**( `optimal policy` )可以表示为：$$ \pi^* $$

## 价值函数

价值函数表示的是，在状态 `s` 起始，使用某策略 $$ \pi $$，可以获取的预期收益，即：

$$
V^\pi_{(s)} = E[R(s_0) + \gamma(R(s_1) + \gamma R(s_2) + ... \mid \pi, S_0 = s]
$$

特别的，**最佳决策** $$ \pi^* $$ 对应的值函数为：$$V^*$$。

将上式换一种形式，可以得到如下表示：

$$
V^\pi_{(s)} = R(s) + \gamma \sum_{S^` \in S} P_{s\pi(S)}(S^`) V^\pi(s^`)
$$

上面的表达式表明，贴现奖励由两部分组成，前面的称为**即时奖励**（`immediate reward`），后面的部分为**未来奖励**（`future discounted rewards`）。

其中，由于状态转移概率之和为 `1`，所以第二部分可以写成：

$$
\gamma E_{s^{'} \sim P_{s \pi (s)}}[V^{\pi}(s^{'})]
$$


## 贝尔曼方程

根据上面的价值函数部分，可以得到**贝尔曼方程**(`Bellman's equations`)：

$$
\begin{align}
V^\pi_{(S)} &= R(s) + \gamma \sum_{S^` \in S} P_{s\pi(S)}(S^`) V^\pi(s^`) \\
&= R(S) + \gamma V^\pi_{(S^`)}
\end{align}
$$

有了贝尔曼方程，就可以求解已知策略的总回报。贝尔曼方程相当于在价值函数上添加线性约束，即 $$V^\pi_{(S^`)}$$ 前面的系数。

# 有限状态的的马可夫决策过程

到这里，再来具象化一下强化学习的目标：

>   **动态的计算出一个 `policy`，决定基于当前状态下，下一步应该采取的行动，从而使得预期总收益最大。**

## 根据已知策略，求解价值函数

根据贝尔曼方程，我们可以写下所有状态的 $$V^{\pi}(s)$$，将会得到 `s` 个线性方程，其中包含所有状态的 $$V^{\pi}(s)$$，求解线性方程组，即可求出每个状态的 $$V^{\pi}(s)$$。

## 最佳价值函数与最佳值函数

若一个策略 $$\pi^{''}$$ 满足，在任一状态 $$s \in S$$ 下，均有：$$V_{\pi^{'}}(s) \leq V_{\pi}(s)$$，则我们称策略 $$\pi^{'}$$ 优于策略 $$\pi$$。其中，存在最优策略 $$\pi_{*}$$，满足：

$$
V_*(S) = max_{\pi}V_{\pi}(x)
$$

对应的贝尔曼方程为：

$$
V^*_{(s)} = R(s) + max_{a \in A}\gamma \sum_{S^` \in S} P_{sa(S)}(S^`) V^*(s^`)
$$

对应的决策为最佳策略：

$$
\pi^*(s) = argmax_{a \in A} \sum_{s^{'} \in S} P_{sa}(s^{'})V^*(s^{'})
$$

>   最优决策对应的值函数即为最优值函数；反之，由最优值函数，就可以直到每一步应该如何决策，也就是得到了最优决策。

## 值迭代和策略迭代

那么问题来了，如何根据**已有的状态转移概率分布和每一个动作的即时奖励**，求解最优决策呢？主要由以下两种方式：值迭代和策略迭代（`Value iteration and policy iteration`）。

这里要注意一点，并不是说，每一步沿着即时奖励最大的方向，就是最佳决策，我们要的是，最后的总收益最高。

### 贝尔曼方程实例

如下所示，机器人从网格 `(4, 1)` 出发，不能经过障碍物(阴影)，目的地是 `(4, 3)`，每个格点内，对应着该状态的即时奖励。

<div style="text-align:center">
<img src="/images/贝尔曼方程实例.png" width="65%">
</div><br>

如上图所示，状态 `(3, 1)` 对应的贝尔曼方程为：

$$
V^{\pi}((3, 1)) = R((3, 1)) + \gamma [P_1 V^{\pi}((3, 2)) + P_2 V^{\pi}((4, 1)) + P_3 V^{\pi}((2, 1))]
$$

其中，$$P_1, P_2, P_3$$ 分别表示采取向左移动的动作时， `(3,1) --> (3, 2)`，`(3,1) --> (4， 1)`，`(3,1) --> (2， 1)` 的状态转移概率，即：向左，向右，向上的概率。

这里有 `11` 个状态，所以可以构建 `11` 个价值函数线性方程组。

### 值迭代

#### **算法主体**

值迭代的步骤如下：

<div style="text-align:center">
<img src="/images/值迭代.png" width="80%">
</div><br>

上面的算法，将使得 `V(s)` 收敛于 $$ V^*(s) $$。

#### **算法实例**

第二步的实例，如下所示：

假设现在在状态 `(3, 1)`，即时奖励为 `R(s) = 0.71`，那么后半部分怎么求（其中，`0.8` 和 `0.1` 表示状态转移概率）？

如果**向左**，则：

$$
\sum_{s^{'}}P_{sa}(s^{'})V^*(s^{'}) = 0.8 * 0.75 + 0.1 * 0.69 + 0.1 * 0.71 = 0.78
$$

如果**向上**，则：

$$
\sum_{s^{'}}P_{sa}(s^{'})V^*(s^{'}) = 0.8 * 0.69 + 0.1 * 0.75 + 0.1 * 0.71 = 0.676
$$

所以，这里选择动作收益较高的，即向左运动，并计算得到对应的价值函数。

#### 更新方式

还有一个问题，就是计算和更新的先后顺序，有两种方式，分别为同步和异步。

-   **同步更新（`synchronous update`）**

    用现有的值函数，去计算所有状态的新的值函数；然后同时将所有状态的值函数，更新为新的值。

-   **异步更新**（`asynchronous updates`）

    选定状态的顺序，然后每计算一个状态的值函数，就将其进行更新；并用更新后的值，参与下一个状态的值函数的计算与更新

### 策略迭代

#### 算法主体

<div style="text-align:center">
<img src="/images/策略迭代.png" width="80%">
</div>

#### 具体步骤

-   随机初始化一个策略 `P`，求解其对应的贝尔曼方程，得到所有状态的价值函数
-   将当前价值函数参与运算。在每一个状态上，确定使得该状态对应的未来奖励最大的动作，作为新策略的一部分。(具体计算方式，参照上面的值迭代部分)
-   将上面得到的策略，作为新的策略，重新计算所有状态的价值函数，不断循环，直至收敛

### 总结

两种迭代方式都是解决 `MDP` 的标准方式，没有优劣之分。

对于较小规模的 `MDP` 使用策略迭代更快，且收敛更快。而对于大规模的 `MDP`，将会包含较多的线性方程组运算，计算较慢。此时，用值迭代更好。

# 学习 MDP 模型

**前面讲到的 `MDP` ，都是假设状态转移概率分布和每个状态的即时奖励已知**。但实际情况下，往往并不知晓，只能通过重复试验，**利用极大似然估计进行评估**。

假设现在**不知道状态转移概率，如何求解最佳策略**。步骤如下：

1.  此时，可以先随机初始化一个策略 `P`，并重复进行该策略的 `MDP` 过程，从而得到大量的数据。

2.  根据实验数据，利用极大似然估计，计算状态转移概率分布：
3.  
    $$
    P_{sa}(s^{'}) = \frac{s 状态下，采取行动 a 到达状态 s^{'} 的次数}{s 状态下，采取的行动总次数}
    $$

    对于实验中未出现的部分，直接设置为 $$\frac{1}{\mid S \mid}$$。

4.  现在，知晓了状态转移概率分布，就可以使用值迭代或者策略迭代，得到基于当前状态转移概率分布的最佳策略

5.  用新的策略，重复执行 `1 ~ 3`，直到收敛。

总结一下，将模型学习以及值迭代放在一起，在状态转移概率分布未知的情况下，学习 `MDP` 模型算法过程如下所示：

<div style="text-align:center">
<img src="/images/MDP模型学习.png" width="80%">
</div>

# 动态规划方法

**动态规划**（`dynamic programming, DP`）算法其实就是直接从前面的值循环得到的，得到最优情形下的状态价值函数之后，输出相应的最优策略。

<div style="text-align:center">
<img src="/images/动态规划算法.png" width="80%">
</div><br>

动态规划算法也可以看做把前面推导的 `Bellman` 方程化为了增量的形式进行计算，当算法收敛到不动点的时候，这时得到的状态价值函数就满足最优情形下的`Bellman` 方程，对应的贪心策略就是最优策略。

其中，核心的算法在第 `6` 行，它迭代地将状态价值函数 `V(s)` 逼近最优状态价值函数 $$ V_*(S) $$。算法的循环中没有显式地定义每一步对应的策略，我们可以对于每一步迭代的价值函数  $$ V(S) $$ 都定义相应的贪心策略。

利用以下定理，我们可以知道，经历每一步的迭代，之后策略都不比之前的策略差；从而当算法收敛的时候，我们可以得到最优策略。

`Policy Improvement Theorem` ：

-   如果存在一对确定性的策略 $$ \pi $$ 和 $$ \pi ^{'}$$ ，使得 $$q_{\pi}(s, \pi^{'}) \geq v_{\pi} (s),  \forall s \in S$$，那么 $$ \pi^{'} $$ 不比 $$ \pi $$ 差。
-   如果存在一对确定性的策略 $$ \pi $$ 和 $$ \pi ^{'}$$ ，使得 $$v_{\pi}^{'}(s) \geq v_{\pi} (s),  \forall s \in S$$，那么 $$ \pi^{'} $$ 不比 $$ \pi $$ 差。

上述算法的第 `6` 行中可以看到，`V(s)` 是单调不减的，因此，其对应的策略也应该是单调提升的。

从第 `5` 行到第 `7` 行可以看出，在每一轮迭代中，都需要对于状态空间中所有的状态访问一次。当状态空间庞大的时候，这样的循环会十分消耗时间。可以采用异步 `DP` 的算法，每轮迭代仅更新一部分的状态，只要保证在多轮迭代中，每个状态都能被访问到，这样的异步 `DP` 算法一样能收敛到最优策略上。

>   **`DP` 算法对于每一个状态价值函数的估算都依赖于前一时刻各状态价值函数的数值，这种特性我们称之为自举(`bootstrap`)。**

在 `DP` 算法中，我们认为反映环境的所有信息 $$ p(s^{'}, r \mid s, a) $$ 是已知的，即我们已经拥有了对于环境的建模，这种利用反映环境信息的模型来进行计算的特性我们称之为 **model-based**。从某种意义上来说，`DP` 方法本质上还并没有涉及到对于环境的“学习”过程，因为 `DP` 没有通过与环境的交互来获取关于环境的信息。

# 蒙特卡洛算法

>   如果说**动态规划类方法**主要从迭代的角度出发，通过反复把 `Bellman` 算子作用到价值函数上期望收敛到最优情形下的价值函数；那么**蒙特卡洛方法**（`Monte Carlo methods`）则是直接从最优价值函数的定义出发，通过采样来直接对于最优价值函数进行**无偏估计**。

具体来说就是先进行采样，即随机地从一些状态出发，然后按照行动策略（`behavior policy`）$$\mu$$ 采取行动，直到达到终止状态；然后进行最有价值函数的估计，即回溯地利用探索到的这部分信息来更新目标策略（`target policy`）$$ \pi $$ 下的价值函数。**当每一轮的行动策略与目标策略一致的时候，我们称这样的方法为 `on-policy` 的；反之，我们称之为 `off-policy` 的。**

## on-policy

<div style="text-align:center">
<img src="/images/on-policy.png" width="80%"/>
</div><br>

对于蒙特卡洛算法，只有满足如下两个条件的时候才能够保证算法收敛到最优策略：

-   主循环被执行无穷多次
-   当循环次数趋向无穷的时候，每个状态被遍历的次数也趋向无穷

对于第一点，我们只能在资源允许的情况下尽可能多地运行主循环。

对于第二点来说，我们的解决方法主要有两个思路；要么在每次主循环开头时，保证选择任意起始状态的概率都大于零，这种方法称之为 `exploring start`；要么在算法运行过程中保证我们行动策略 $$\mu$$ 具有足够的随机性，即能始终以一个大于零的几率访问任意可能的行动。

>   **特别地，$$ \forall s \in S $$，如果某个策略 $$\mu$$ 能够保证对于每个行动的访问概率不低于 $$ \epsilon / \mid A(s) \mid $$，则称这样的策略是 `ϵ-soft` 的。**

对于算法五第 `14` 行中定义的策略，我们称之为 `ϵ-greedy` 的，它是一种 `ϵ-soft`  策略。算法五中第 `10` 行是对于当前策略对应价值函数的估计，第 `14` 行是基于当前价值函数对于策略进行改进，由于探索使用的策略（第 `6` 行）和目标输出的策略（第 `14` 行）是同样的策略，因此这种方法是 `on-policy` 的。

## off-policy

行动策略也可以和目标策略不相同，即得到下面的 `off-policy` 的蒙特卡洛算法。

<div style="text-align:center">
<img src="/images/off-policy.png" width="80%"/>
</div><br>

探索所使用的策略 $$ \mu $$ 是任意固定的 `ϵ-soft` 策略（第 `6` 行），比如它可以是在行动集中等概率选取行动的随机策略。而目标输出的策略 $$ \pi $$ 是一个关于价值函数的贪心策略，因此这种方法是 `off-policy` 的。

由于行动策略与目标策略不一致，因此在更新价值函数的时候我们需要把行动策略对应的收益投影到目标函数对应的收益上。直观地来说，如果行动策略选择了某个当前目标策略大概率会选择的路径，那么对应的价值函数的改变会更多一些；反之，应该尽可能少一些地改变价值函数。

这里我们使用了重要性采样（`importance sampling`）的技术。即目标策略对应的价值函数 `V(s)` 与行动策略得到的收益 $$G_t$$ 应该满足如下关系。

$$
V(s) = \frac{\sum_t \rho_t^TG_t}{\sum_t \rho_t^T}
$$

其中**重要性采样率**（`importance-sampling ratio`）：

$$
\rho_t^T = \prod_{k=t}^{T-1}\frac{\pi(A_k \mid S_k)}{\mu(A_k \mid S_k)}
$$

其中，`T` 表示终止状态的时刻。第 `12、13、16` 行所描述的就是这样的重要性采样的增量实现。

## 总结

蒙特卡洛方法能够通过与环境的交互来学习一个好的策略，它不需要对于环境的建模，即它是 **model-free** 的。

不同于之前的动态规划算法，每一步中价值函数的更新并不依赖于之前的价值函数，而是直接更新到采样得到收益的平均值上，因此它不是 `bootstrap` 的。由于蒙特卡洛方法要求一直采样到最末尾，因此它只能用于回合制的任务。