---
layout: post
title:  "Pytorch 迁移学习"
date:   2019-08-20 10:42:01 +0800
categories: 人工智能
tag: Pytorch
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

深度学习大多数应用中，往往得不到足够的训练样本，如果我们直接用网络来进行训练很容易过拟合，很难找到与之匹配的同等规模的网络。同时，从零开始训练模型代价较大。

因此，我们通常是在一个较大的数据集上进行预训练，然后用该预训练的模型作为初始化或者特征提取器，来针对特定任务，进行下一步的训练。

之所以这种迁移有用，是因为此前的模型从大规模的数据中进行的训练，学习到了边界检测，曲线检测，明暗对象检测等低层次的信息，能够很好地适用于其他任务。

这两种场景分别称为：

- **Finetuning the convnet**：用预训练的参数初始化网络，剩余的训练步骤不变。此即**微调**（`fine-tune`）。
- **ConvNet as fixed feature extractor**：冻结网络的部分层，作为特征提取器，并只对部分层进行训练。此即**预训练**（`pre-training`）。

## Finetuning the convnet

这种方式指的是，将预训练的模型参数作为初始化参数，而不用随机初始化参数的方式。然后针对当前样本，对整个模型进行训练。

```python
model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)
```

## ConvNet as fixed feature extractor

使用 `requires_grad == False` 参数来冻结部分层的参数，使之在 `backward()` 阶段，不会发生参数更新。

也就是说，其它层进行随机初始化，或者沿用之前的预训练参数，且在反向传播过程中，只更新未冻结的层的参数，冻结的参数不改变。

```python
model_conv = torchvision.models.resnet18(pretrained=True)
for param in model_conv.parameters():
    param.requires_grad = False

# Parameters of newly constructed modules have requires_grad=True by default
num_ftrs = model_conv.fc.in_features
model_conv.fc = nn.Linear(num_ftrs, 2)

model_conv = model_conv.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that only parameters of final layer are being optimized as
# opposed to before.
optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)
```