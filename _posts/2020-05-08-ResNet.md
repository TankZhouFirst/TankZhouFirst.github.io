---
layout: post
title:  "ResNet"
date:   2020-05-08 21:43:01 +0800
categories: 人工智能
tag: 图像分类
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

-   [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)

****

# 引入

## 要解决的问题

神经网络往往可以从深度中汲取益处，但是随着神经网络的深度加深时，网络更难训练。其主要有如下几点原因：

1.  **梯度消失与梯度爆炸**（**vanishing / exploding gradients  **）

    从训练的开始就阻碍网络的收敛，但是通过标准化（**normalized initialization   和 intermediate normalization layers  **）基本可以解决

2.  **梯度衰减**（**degradation**）

    深度网络能够开始收敛，但是随着深度的加深，网络的精度开始饱和，甚至急剧下降，但这并不是由于过拟合导致，因为在训练集上亦是如此。如下如所示：

<div style="text-align:center">
<img src="/images/梯度衰减.png" width="85%"/>
<p>图 1：梯度衰减现象（非过拟合导致）</p>
</div><br>

>   本文的主要目标，就是解决梯度衰减的问题。

## 主要思路

假设某一部分网络需要学习输入 **x** 到输出 **y** 的映射 $$\mathcal H(x)$$，通过恒等映射的思想，引入short connection，构建如下网络，使得网络学习某种残差映射 $$\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$$。

<div style="text-align:center">
<img src="/images/基本残差块.png" width="55%"/>
<p>图 2：基本残差块</p>
</div><br>

实际上，**残差函数的学习比原始函数的学习更加简单**，详细数学推导见详解部分。

## 主要成就

提出来残差网络的思想，有效的解决了深度网络梯度衰减的问题，而且残差网络还具有如下优点：

1.  更易于优化，训练收敛更快，精度更高
2.  能够叠加更深层的网络，而不会再出现梯度衰减等问题

# 详解

## 思路动机

在浅层网络的基础上，构建深层网络。最糟糕的情况就是，后续网络只是简单复制前层网络的特征图（**恒等映射，identity mapping  **），即使不学习新的特征，此时，网络的精度应该至少与对应的浅层网络保持一致。但是实验表明，如此简单的目标也不能达到。

显然在实际情况中，我们构建深层网络的目标并非简单地恒等映射，否则还不如直接使用简单的浅层网络。但是恒等映射的这一思路借鉴还是有意想不到的效果的。

## 数学原理

实际上，梯度衰减的真正原因是因为：网络累积到一定程度上时，累乘也会导致梯度减小，这极大地限制了网络的加深。

先来看一下直观感受，再看数学推导。

假设原始网络要作如下映射：$$\mathcal F(x) = 5.1$$。若果实际输出为 $$5.2$$，对于原始网络而言，变化仅为 $$0.1$$，偏差为 $$\frac{5.2 - 5.1}{5.1}$$。而使用残差网络后，相当于残差部分的目标映射为 $$\mathcal F(5) = \mathcal H(5) - 5 = 0.1$$，而输出为 $$5.2$$ 时，残差映射的偏差为 $$\frac{0.2 - 0.1}{0.1}$$，偏差变为 $$100 \%$$。

因此，残差网络对输出的偏差更为敏感！！！下面，再来看一下数学推导，模型的优化的关键，在于梯度的计算。

首先，残差单元可以表示为：

$$
\begin{aligned}
y_l &= h(x_l) + F(x_l, W_l) \\
x_{l + 1} &= f(y_l)
\end{aligned}
$$

其中，$$x_l$$ 和 $$x_{l + 1}$$ 分别表示第 $$l$$ 个残差单元的输入和输出。$$\mathcal {F}$$ 为残差函数，表示学习到的残差；$$h(x_l) = x_l$$ 为恒等映射，$$f$$ 为 `ReLU` 激活函数。

因此，我们可以求得从浅层 $$l$$ 到深层  $$L$$ 的学习特征为：

$$
x_{L}=x_{l}+\sum_{i=l}^{L-1} F\left(x_{i}, W_{i}\right)
$$

根据链式求导法则，可得梯度的反向传播为：

$$
\frac{\partial \operatorname{loss}}{\partial x_{l}}=\frac{\partial \operatorname{loss}}{\partial x_{L}} \cdot \frac{\partial x_{L}}{\partial x_{l}}=\frac{\partial \operatorname{loss}}{\partial x_{L}} \cdot\left(1+\frac{\partial}{\partial x_{L}} \sum_{i=l}^{L-1} F\left(x_{i}, W_{i}\right)\right)
$$

上式中第一个因子 $$\frac{\partial \operatorname{loss}}{\partial x_{L}}$$ 表示损失函数到达 $$L$$ 的梯度，小括号中的 $$1$$ 表示该层 $$shortcut$$ 机制可以无损的传播梯度。

因为梯度计算时，多了一个恒等传播项，导致网络的梯度对 `loss` 的变化更为敏感，能够更好地检测扰动，从而更新参数。这是残差网络有效的本质原因！！！

## 网络结构

本文中，用于 `ImageNet` 的几个模型的网络结构如下：

<div style="text-align:center">
<img src="/images/网络结构对照.png" width="95%">
<p>图 3：网络结构</p>
</div><br>

其中，基准网络变为残差网络时，对于 **shortcut** 的处理如下：

1.  对于特征图尺寸不变的部分，直接用 **shortcut**
2.  对于特征图尺寸减半，通道数加倍的部分，先通过步长为 2 的卷积对旁路的 **x** 进行下采样，然后有两种选择：
    1.  依旧使用 **shortcut**，但是对于旁路输入使用 **0** 进行通道上的 **padding**，
    2.  使用 $$1 \times 1$$ 的卷积进行线性变换（$$W_s$$），得到与 **y** 匹配的通道数，即：$$\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+W_{s} \mathbf{x}$$

## Bottleneck

考虑到计算成本，利用 $$1 \times 1$$ 卷积减少计算量，构建如下 **bottleneck** 结构：

<div style="text-align:center">
<img src="/images/bottleneck.png" width="70%">
<p>图 4：bottleneck</p>
</div><br>

# 试验和结果

>   详细实验细节，参考原论文。

## ImageNet

### 残差的影响

实验使用的对比网络如下所示，其中 `conv3_1`，`conv4_1`， `conv5_1` 进行下采样，步长为 `2`：

<div style="text-align:center">
<img src="/images/Architectures for ImageNet.png" width="95%">
<p>表 1：ImageNet 上的网络架构</p>
</div><br>

结果如下：

<div style="text-align:center">
<img src="/images/Training on ImageNet.png" width="95%">
<p>图 5：ImageNet 上的训练曲线</p>
</div><br>

<div style="text-align:center">
<img src="/images/Top-1 error.png" width="40%">
<p>表 2：ImageNet 上的实验结果</p>
</div><br>

>   从上面的结果可以发现，基准网络出现了梯度衰减的现象，而 ResNet 解决了该问题。

### 线性变换的影响

下面看一下，**shortcut** 部分的线性变换的影响，设置如下配置：

1.  **A**：仅在尺寸变换时，进行 **zero-padding**，其他部分直接短接
2.  **B**：仅在尺寸变换时，进行线性变换，其他部分直接短接
3.  **C**：所有部分均进行线性变换

<div style="text-align:center">
<img src="/images/Error rates on ImageNet validation.png" width="50%">
<p>表 3：10-crop 时几种配置的误差率对比</p>
</div><br>

从上面的结果可以发现，几种配置有略微影响。**AB** 之间的提升，是因为短接部分，**A** 的 `padding` 方式未进行学习；而 **BC** 之间，是因为 **C** 引入了更多参数。考虑到计算复杂度，**B** 是最优解，尤其是使用 **bottleneck** 时。

其他几种测试模式详见论文。

## Cifar-10

<div style="text-align:center">
<img src="/images/Training on CIFAR-10.png" width="95%">
<p>图 6： cifar-10 训练曲线</p>
</div><br>

>   结论与上面实验一致。

<div style="text-align:center">
<img src="/images/Standard deviations (std) of layer responses on CIFAR10.png" width="95%">
<p>图 7： 层响应分析</p>
</div><br>

>   当层数更多时，每一层对信号的更改更小。结合前面的分析，表明误差越来越小！！！