---
layout: post
title:  "Inception-V1"
date:   2019-06-29 22:28:01 +0800
categories: 人工智能
tag: 图像分类
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考 :**

- **Paper :** [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)
- **Code :** [GitHub](https://github.com/antspy/inception_v1.pytorch/blob/master/inception_v1.py)
- **Blog :** [深入理解GoogLeNet结构（原创）](https://zhuanlan.zhihu.com/p/32702031)

****

# 引入

## 问题背景

在过去几年，图像识别和目标检测领域的深度学习研究进步神速，其原因不仅在于强大的算力，更大的数据集以及更大的模型，更在于新颖的架构设计思想和改良算法。

另一个需要关注的点在于，移动设备的逐渐流行，对算法的运算量和硬件资源使用提出了新的要求。将算法用于实际，这也是本文的架构的出发点。在本文所有的实验中，模型推理阶段的运算量保持在 `15`亿次乘加运算内。

## 本文的主要贡献

1. 前瞻性的关注深度学习算法在移动设备上的实用性
2. 提出 `Inception` 结构，人为构建稀疏连接，引入多尺度感受野和多尺度融合
3. 使用 $$1 \times 1 $$ 卷积层进行降维，减少计算量
4. 使用均值池化取代全连接层，大幅度减少参数数目和计算量，一定程度上引入了正则化，同时使得网络输入的尺寸可变

# 动机和灵感来源

改善深度神经网络最直接的办法就是增加网络的尺寸。它包括增加网络的**深度**和**宽度**两个方面。深度层面，就是增加网络的层数，而宽度方面，就是增加每层的 `filter bank` 尺寸。但是，这种方式有两点不足：

1. 更大的尺寸通常意味着更多的参数，也更容易导致网络的过拟合，尤其是样本不足的情况下
2. 即使均匀的增加网络每层的尺寸，也会急剧（指数形式）增加总的运算量。且网络容量利用不足时，将会导致大量的运算被浪费。比如说将 `filter bank` 尺寸扩大为两倍，即 $$ 3 \times 3 \times 16 \times 32$$ 变为 $$3 \times 3 \times 16 \times 64$$，则其将导致计算量变为原来的 `4` 倍。

解决这一问题的根本方法在于，最终将 `dense` 连接转换为为**稀疏连接**。

## 稀疏连接

稀疏连接有两种方法：

1. 空间（`spatial`）上的稀疏连接，也就是 `CNN` 。其只对输入图像的局部进行卷积，而不是对整个图像进行卷积，同时参数共享降低了总参数的数目并减少了计算量
2. 在特征（`feature`）维度上的稀疏连接进行处理，也就是在通道的维度上进行处理。

> 上面第二点，就是 `Inception` 结构的灵感来源。
>
> 如果数据集的概率分布能够被一个大的稀疏神经网络进行表示，那么最优网络拓扑可以通过如下方式逐层构建：分析上一层的激活输出的统计特性，并将具有高度相关性输出的 `filter` 进行聚类 (`cluster`)，来获得一个稀疏的表示。

尽管严格的数学证明需要较强的条件，但是结合 `Hebbian principle`，实际上，即使在弱条件下，该论断也是成立的。

## Hebbian Principe

`Hebbian Principe` 是一个很通俗的现象。

先摇铃铛，之后给一只狗喂食，久而久之，狗听到铃铛就会口水连连。这时狗的**听到**铃铛的神经元与**控制**流口水的神经元之间的连接被加强了。

`Hebbian principle` 的精确表达就是，如果两个神经元常常同时产生动作电位，或者说同时激活（`fire`），这两个神经元之间的连接就会变强，反之则变弱（`neurons that fire together, wire together`）。

对应到神经网络的 `filter bank`，就是将强化具有相似特征的 `filter` 之间的关联。具体做法就是，用更少的 `filter` 来提取相关的特征，但是通过多个尺度的 `filter bank` 进行不相关的特征组。实际上就是预先把相关性强的特征汇聚，就能起到加速收敛的作用。

## 稀疏矩阵的分解

当处理非均匀稀疏数据结构的数值计算时，如今的计算库效很低效。即使运算量减小 `100` 倍，查找表和缓存的缺失将占据主导，使得稀疏矩阵的运算并不实用。当使用到针对 `GPU` 或 `CPU` 硬件架构细节，进行高度定制，稳定优化的，支持密集矩阵的高速运算的数值运算库时，这种差距将更为明显。

但是，如今的计算机视觉领域的系数运算仅限于卷积空间层面，通过卷积运算来利用稀疏性。而大量的文献表明，可以将系数矩阵聚类分解为相对密集的子矩阵进行运算，从而达到当前稀疏矩阵运算领域，业界最佳的运算性能。

如下图所示，为系数矩阵分解为密集矩阵运算的例子，可以发现，运算量确实降低不少。

<div style="text-align:center">
<img src="/images/矩阵分解运算.png" width="90%">
<p>矩阵分解运算</p>
</div><br>


## filter bank 维度处理

上面系数矩阵对应到 `Inception` 中，就是在通道维度上，将稀疏连接转换为密集连接。

比方说 $$3 \times 3$$ 的卷积核，提取 `256` 个特征，其总特征可能会均匀分散于每个 `feature map` 上，可以理解为一个稀疏连接的特征集。可以极端假设，`64` 个 `filter` 提取的特征的密度，显然比 `256` 个 `filter` 提取的特征的密度要高。

因此，通过使用 $$1 \times 1$$， $$3 \times 3$$， $$5 \times 5$$ 等，卷积核数分别为 `96`, `96`, `64` 个，分别提取不同尺度的特征，并保持总的 `filter bank` 尺寸不变。这样，同一尺度下的特征之间的相关性更强，密度更大，而不同尺度的特征之间的相关性被弱化。

综上所述，可以理解为将一个包含 `256` 个均匀分布的特征，分解为了几组强相关性的特征组。同样是 `256` 个特征，但是其输出特征的冗余信息更少。（感觉有点儿类似于分组卷积）

# Inception 模块

## 原始 Inception 模块

如下图所示，为 `Inception` 模块：

<div style="text-align:center">
<img src="/images/Inception V1 结构 1.png" width="80%">
<p>原始 Inception v1</p>
</div><br>


如上图所示，为基本的 `Inception` 模块，其通过不同尺度的卷积核并联，来提取更密集的特征。也可以用其他尺寸的卷积核，但是这几种更方便。

不同尺寸的卷积核，可以提取不同尺度的特征，代表不同的相关的特征，并同时引入了多尺度。由于网络的越深层，需要提取更抽象的特征，因此，相应的 $$3 \times 3$$ 和 $$5 \times 5$$ 的卷积核数量要相应的增多。

在所有卷积核运算结束之后，通过 `concatenate` 运算（在 `depth` 维度上进行），将结果进行组合。整个 `GoogLeNet` 网络通过大量 `Inception` 模块堆叠而成。

<div style="text-align:center">
<img src="/images/Inception 3 维图示.png" width="70%">
<p>Inception 3 维图示</p>
</div><br>

## 改进的 Inception 模块

在上面的 `Inception` 模块中，存在一定的问题 —— 卷积运算运算量过大。当上一层的输出通道数较大时，当前 `Inception` 模块的运算量将很大，且由于多组卷积核并联运算，因此这是随着层数的堆叠而爆炸式增长的！

针对这一问题，我们使用 $$ 1 \times 1$$ 卷积作为 `reduction` 层，来缩减通道数，从而减少计算量。具体做法为：在每一个 $$3 \times 3$$ 和 $$5 \times 5$$ 的卷积层之前，加上 $$1 \times 1$$ 的卷积层。其在减少计算量的同时，还引入了非线性，进一步增加了网络的表达能力。

新的结构如下所示：

<div style="text-align:center">
<img src="/images/Inception V1 结构 2.png" width="75%">
<p>改进的 Inception V1</p>
</div><br>


假设上一层的输出通道数为 `192`，输出特征图尺寸为 $$28 \times 28$$，`Inception` 中，卷积核数分别为 `64`, `128`, `32`，则具体运算相关数据的比较如下所示：

原始结构中：

- 参数数目：$$(1 \times 1 \times 192 \times 64) + (3 \times 3 \times 192 \times 128) +  (5 \times 5 \times 192 \times 32) = 387072$$
- 输出特征数：$$64 + 128 + 32 + 192 = 416$$

新结构中：

- 参数数目：$$1 \times 1 \times 192 \times 64 + (1 \times 1 \times 192 \times 96 + 3 \times 3 \times 96 \times 128) + (1 \times 1 \times 192 \times 16 + 5 \times 5 \times 16 \times 32) = 15872$$
- 输出特征数：$$ 64 + 128 + 32 + 32 = 256 $$

从上面的数据可以看出，参数数量大幅度减少，虽然具体的计算量未给出，但是很显然会大幅度降低。

## Inception 模块的特性

综上所述，`Inception` 模块具有如下特性：

1. 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合
2. 之所以卷积核大小采用 `1、3` 和 `5` ，主要是为了方便对齐。设定卷积步长 `stride=1` 之后，只要分别设定`pad = 0、1、2`，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了
3. 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，$$3 \times 3$$ 和 $$ 5 \times 5 $$卷积的比例也要增加

这种设计的优点有：

1. 它使得可以显著增加网络每一层的神经元数量，同时不会导致计算量的不可控增长
2. 维度减约的设计，使得在网络层之间传递大量的特征层
3. 间接的引入了多尺度处理，使得网络可以同时综合考虑不同尺度的信息

这种计算方式的改善，使得在不增加计算量的同时，拓宽每一层的宽度，并增加网络的深度。

# GoogLeNet

## 主网络配置

如下表所示，为 `GoogLeNet` 的网络结构参数：

<div style="text-align:center">
<img src="/images/GoogLeNet V1.png" width="99%">
<p>GoogLeNet 网络结构参数</p>
</div><br>

在 `GoogLeNet` 中：

1. 输入尺寸为 $$ 224 \times 224$$，使用 `RGB` 格式的图像，并分别对每个通道减去各通道的均值。
2. 所有的激活函数使用 `ReLU`
3. 上表中，其中，$$ \text{#} 3 \times 3$$ 和 $$\text{#} 5 \times 5$$ 分别表示在对应卷积核之前使用的 $$ 1 \times 1 $$ 的卷积和通道数。
4. 网络设计始终考虑在移动设备上的实用性，最终网络深度为 `22` 层 (不包含池化层)。总的网络层数大约为 `100` 层。
5. 当将最后的全连接层，替换为均值池化层，将会使得 `top-1` 精度改善 `0.6%`。因为全连接层占据大部分计算量，且容易过拟合。但即使移除全连接层，`dropout` 层也必不可少。

## 辅助分类器 auxiliary classifiers

当网络深度相对较大时，可能需要担忧反向传播的能力。但是实验发现，在相对浅层的中间层，生成的特征具有较高的辨识度。

因此我们考虑添加辅助分类器 (`auxiliary classifiers`)。我们期望用它们来增加辨识度，从而补充梯度，并增加额外的正则化。辅助分类器由小的 `CNNs` 组成，接在 `Icneption` 模块之后。

事实上在较低的层级上这样处理基本没作用，作者在后来的 `inception v3` 论文中做了澄清。

在训练过程中，辅助分类器（**Auxiliary classifiers**） 的 `loss` 将会添加到总的 `loss` 中，但是权重较小 (本文为 `0.3`)。而在推理阶段，不需要这些 `auxiliary classifiers`。

其中，`auxiliary classifiers` 的具体结构如下：

1. 均值池化层，后接 $$5 \times 5$$ ，步长为 `3` 的卷积层，对应输出分别为：$$ 4 \times 4 \times 512 $$ 和 $$ 4 \times 4 \times 528$$。
2. 输出通道为 `128` 的 $$ 1 \times 1$$ 的卷积层，用于维度缩减和非线性引入
3. 全连接层，`1024` 个神经元，后接一个 `ReLU`
4. 一个 `dropout` 层，$$ratio = 0.7$$
5. 全连接层，后接一个 `softmax` 作为分类器，预测 `1000` 类输出

## 最终模型

最终的模型结构如下图所示：

<div style="text-align:center">
<img src="/images/GoogLeNet V1 结构.png" width="99%">
<p>GoogLeNet 模型</p>
</div><br>



# 训练及结果

我们的模型使用 `DistBelief` 分布式机器学习系统进行训练，使用适量的并行。尽管我们只实现了 `CPU` 版本，但是可以用 `GPU` 在一周内完成训练，主要约束在于内存的限制。训练过程中，使用异步随机梯度下降，动量系数为 `0.9`，固定学习速率，且每 `8` 个 `epochs`，下降 `4%`。

`ILSVRC 2014 Classification` 任务包含 `1000` 类图像，共 `1.2 million` 张训练图片，`50000` 张验证图片，以及 `100000` 张测试图片。每张图片对应一个 `label`。使用 `top-1` 和 `top-5` 错误率来衡量性能。

我们的模型未使用额外的数据进行训练，只是在测试时，使用了如下技巧。

1. 独立训练了 `7` 个版本的 `GoogLeNet` 模型，并使用集成预测。这些模型使用相同的初始化，学习速率策略，其仅仅是样本采样方式不同，以及图像输入的随机顺序不同
2. 在测试过程中，我们采用更激进的 `crop` 策略。具体说来，就是将图像进行缩放，使其短边分别为 `256`， `288`， `320`， `352`，并在其 `left`, `center`, `right` 进行切割，使之成为方形 (对于肖像，则是 `top`, `center`, `bottom`)。然后再切割后的方形的四个角以及中心进行 `crop`，尺寸为 `224`。再加上将切割后的图进行缩放到 `224` 尺寸的图片，以及其镜像版本(共 `7` 张)。因此，每张图片进行了多次 `crop`。
3. 将所有的 `crops` 的 `softmax` 概率输出值，以及所有的 `classifier` 输出进行平均，得到当前输入的最终的预测值。在我们的实验中，我们分析了不同的验证数据处理方式，例如对 `crops` 进行 `max pooling`，对 `classifiers` 进行平均等，但是其相对于简单的平均，性能更差。

我们最终提交的版本中，在验证集和测试集上，`top-5` 错误率均为 `6.67%`，比赛中排名第一。

<div style="text-align:center">
<img src="/images/不同网络的性能对比.png" width="80%">
<p>不同网络的性能对比</p>
</div><br>



<div style="text-align:center">
<img src="/images/不同 crop 方式对 GoogLeNet v1 的影响.png" width="80%">
<p>不同 crop 方式对 GoogLeNet v1 的影响</p>
</div><br>