---
layout: post
title:  "Pytorch 计算图"
date:   2019-08-20 11:02:01 +0800
categories: 人工智能
tag: Pytorch
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

# 图的基本概念

## 引入

基本所有的深度学习框架，都是基于图结构的。

## 图的实例

### 创建计算图

举个列子： $$y=(a+b)(b+c)$$  在这个运算过程就会建立一个如下的计算图：

<div style="text-align:center">
<img src="/images/计算图 1.png" width="60%">
</div><br>

在这个计算图中，节点就是参与运算的变量，在 `pytorch` 中就是 `Tensor` ，而边就是变量之间的运算关系，如：`torch.mul()`，`torch.mm()`，`torch.div()` 等。

注意图中的 **leaf_node**，叶子结点就是由用户自己创建的 `Tensor`，在这个图中仅有 $$a，b，c$$ 是 **leaf_node**。

### backward

为什么要关注 **leaf_node**? 因为在网络 **backward** 时候，需要用链式求导法则求出网络最后输出的梯度，然后再对网络进行优化。

如下就是网络的求导过程：

<div style="text-align:center">
<img src="/images/计算图的 backward.png" width="60%">
</div><br>

# 动态图与静态图

## 基本概念

**静态计算图**先定义再运行（`define and run`），一次定义多次运行；而**动态计算图**是在运行过程中被定义的，在运行的时候构建（ `define by run`），可以多次构建多次运行。 

## 静态图与动态图的对比

静态图**一旦创建就不能修改**，而且静态图定义的时候，使用了特殊的语法，就像新学一门语言。这还意味着你无法使用 `if`、 `while` ，`for-loop` 等语句。因此静态图框架不得不为这些操作专门设计语法，同时在构建图的时候必须把所有可能出现的情况都包含进去，这也导致了静态图过于庞大，可能占用过高的显存。

动态图框架就没有这个问题，它可以使用 `Python` 的 `if`，`while`、`for-loop` 等条件语句，最终创建的计算图取决于你执行的条件分支。

静态图类似于 `C++`，必须先编译，才能运行；而动态图类似于 `Python`，实时运行。

## Tensorflow 和 Pytorch

`PyTorch` 和 `TensorFlow` 都是基于有向无环图（`DAG`）的深度学习框架， `PyTorch` 使用的是动态图，而 `TensorFlow` 使用的是静态图。在 `PyTorch` 中，每一次前向传播（每一次运行代码）都会创建一幅新的计算图。

`TensorFlow` 遵循“**数据作为代码和代码是数据**”的习惯用法。在 `TensorFlow` 中，您可以在模型运行之前静态定义图形。所有与外部世界的通信都通过 **tf.Session** 对象和 **tf.Placeholder** 进行，这些张量将在运行时被外部数据替代。

在 `PyTorch` 中，事情更为迫切和动态：您可以随时定义，更改和执行节点，不需要特殊的会话接口或占位符。总的来说，这个框架与 `Python` 语言更紧密地结合在一起，并且在大多数时候感觉更加自然。

当你在 `TensorFlow` 中写作时，有时候你会觉得你的模型是在一个有几个小孔的砖墙后面进行交流。因此，调试的时候，`Pytorch` 更方便，而 `tensorflow` 则必须先定义 `graph`，然后在 `session` 中运行后，才知道是否有错，几乎很难找到出错的地方。

同时，`pytorch` 支持 **ONNX** （`Open Neural Network Wxchange`）标准，即开放神经网络交换格式。该标准使得模型可以在不同的框架之间进行转移。

`PyTorch` 和 `TensorFlow` 之间另一个主要的区别就是图的表示方法的不同。

`TensorFlow` 是静态流图，流图构建完毕才可计算（利用 **session**），且其间不可变；而 `Pytorch` 为动态流图，每次迭代都将重新创建流图。

`TensorFlow` 方式需要将整个图构建成静态的，即：每次运行的时候图都是一样的，是不能够改变的，只要定义一次即可一遍又一遍地重复执行该图，所以不能直接使用 `Python` 的 `while` 循环语句，需要使用辅助函数 `tf.while_loop` 写成 `TensorFlow` 内部的形式。

而  `Pytorch` 的动态流图更符合程序的思维。但是由于每次计算都更新流图。

静态图很好，因为您可以预先优化图，例如：一个框架可能决定为了效率而融合某些图操作，或者想出一个在许多 `GPU` 或许多机器上的分布运行计算图的策略。 如果您一遍又一遍地重复使用同一个图，那么这个潜在的昂贵的前期优化可以在同一个图重复运行的情况下分摊。

## 一个实例

下面用静态图和动态图实现统一功能，代码如下所示：

### 静态图 Tensorflow

```python
import tensorflow as tf
 
first_counter = tf.constant(0)
second_counter = tf.constant(10)
 
def cond(first_counter, second_counter, *args):
    return first_counter < second_counter
 
def body(first_counter, second_counter):
    first_counter = tf.add(first_counter, 2)
    second_counter = tf.add(second_counter, 1)
    return first_counter , second_counter
 
c1, c2 = tf.while_loop(cond, body, [first_counter, second_counter])
 
with tf.Session() as sess:
    counter_1_res, counter_2_res = sess.run([c1,c2])
 
print(counter_1_res)
print(counter_2_res)
```

### 动态图

```python
import torch
first_counter = torch.Tensor([0])
second_counter = torch.Tensor([10])
 
while (first_counter < second_counter)[0]:
    first_counter += 2
    second_counter += 1
 
print(first_counter)
print(second_counter)
```

# Pytorch 中的一些细节

## 计算图的自动创建与销毁

`pytoch` 构建的计算图是**动态图**，为了节约内存，所以**每次一轮迭代（`backward`）完之后计算图就被在内存释放**，所以当你想要多次`backward` 时候就会报错。

```python
net = nn.Linear(3, 4)                                    
input = torch.randn(2, 3)
output = net(input)
loss = torch.sum(output)
loss.backward()                                          
# 到这计算图已经结束，计算图被释放了

# 再次调用将会报错
loss.backward()
```

> **grad 在反向传播过程中，是累加的，因此，每次反向传播之前，需要先将梯度清零**。

但是可以通过 **retain_graph** 参数，强行保留计算图。

```python
...
loss.backward(retain_graph=True)                # 添加retain_graph=True标识，让计算图不被立即释放

loss.backward()
```

## backward 指定节点的梯度

> `backward` 只能是标量。

那么在实际运用中，如果我们只需要求图中某一节点的梯度，而不是整个图的，又该如何做呢？

```python
x = torch.FloatTensor([[1, 2]])
y = torch.FloatTensor([[3, 4],[5, 6]])
loss = torch.mm(x, y)

# 变量之间的运算
loss.backward(torch.FloatTensor([[1, 0]]), retain_graph=True)  # 求梯度，保留图

print(x.grad.data)

# 求出 x_1 的梯度
x.grad.data.zero_()

# 最后的梯度会累加到叶节点，所以叶节点清零
loss.backward(torch.FloatTensor([[0, 1]]))

# 求出 x_2的梯度
print(x.grad.data)
```
