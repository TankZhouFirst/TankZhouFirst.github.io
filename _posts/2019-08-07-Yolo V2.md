---
layout: post
title:  "Yolo V2 论文笔记"
date:   2019-08-07 14:35:01 +0800
categories: 人工智能
tag: 目标检测
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

1. [YOLO9000:Better, Faster, Stronger](<https://arxiv.org/pdf/1612.08242.pdf>)
2. [HomePage](<http://pjreddie.com/yolo9000/>)
3. [重温 yolo v2](<https://zhuanlan.zhihu.com/p/40659490>)

****

# 引入

## 行业现状

目前的大多数目标识别系统，面临如下问题：

1. 识别的目标种类极其有限。
2. 相比于 `classification` 和 `tagging` 任务，`detection` 任务对应的数据集规模极其有限。

## 本文的主要贡献

本文针对 `Yolo V1` 做出了如下改进：

1. **自适应输入尺寸**：同一模型，可以适应各种尺寸的图片。通过输入不同的尺寸，进行速度和精度之间的切换折中。
2. **提出联合训练**：针对 `detection` 数据集数量少的问题，本文提出了一种联合训练新思路，可以用已有的 `classification` 数据集来拓展目标检测系统。
3. **重新定义 Anchor box 和坐标格式**：本文重新定义了 `Anchor box`，使用 `k-means cluster` 算法自动生成。并重新定义了坐标的预测格式，使得模型收敛更快，更稳定，精度更高。

# 更好 Better

## Yolo V1  的不足

`Yolo V1` 相比于当前最先进的实时系统，仍有许多不足：

1. `Error nalysis` 结果表明：`Yolo` 存在较大的 `location errors`。
2. 相比于基于候选区的算法，`Yolo V1` 的召回率较低 (`low recall`)。

因此，`Yolo V2` 对 `Yolo V1` 的主要改进在于：**保证分类精度的同时，改善召回率和定位误差**。

## Yolo V2 的改进

### 改进思路

期望 `Yolo V2` 更精准，但是仍旧很快！

1. 重构网络结构，使用 `DarkNet-19` 作为基础网络，速度更快
2. 重新定义 `Anchor box`，重构 `loss` ，使得模型更易于优化！
3. 多输入尺寸自适应，提供可调性能
4. 多输出尺度融合，识别小目标
5. 引入一些其他 `trick`，完善模型性能
6. 构建 `Wordtree`，引入联合训练，可充分利用多数据集，扩充识别目标类型

加入到 `Yolo` 的新尝试结果如下所示：

<div style="text-align:center">
<img src="/images/Yolo V2 的改进措施.png" width="95%">
</div><br>
如上图所示，为每一步对 `Yolo V1` 的改进，带来的性能的提升。可以发现，绝大多数的改进都带来了较大的提升。

改变 `anchor box style` 虽未改变 `mAP`， 但是增加了召回率。而新的网络结构则减少了 33% 的计算量。

### 改进尝试

#### 引入 Batch Normalization

> `BN` 可以较大的改善收敛，同时省去其他正则化步骤。

对 `Yolo V1` 的每一层卷积层之后，添加 `BN`，`mAP` 大约提升了 2%。同时，由于 `BN` 的正则化效果，可以移除 `dropout` 而不会发生过拟合。

#### 使用高分辨率图像预训练

之前的大多数 `classifier` 使用 $$ 256 \times 256 $$ 甚至更小的尺寸作为输入。在 `Yolo V1` 的预训练中，使用输入尺寸为 $$ 224 \times 224 $$，然后直接用于 `detection` 的训练。

而在 `Yolo V2` 中，在预训练之后，将输入尺寸调整为 $$ 448 \times 448 $$，然后在 `ImageNet` 上训练了 10 个 `epochs`。这使得网络的 `filters` 能够在高分辨率图片上表现更好。

随后，将网路用于 `detection` 训练。最后的 `mAP` 提升了将近 4%。

#### 重新定义 Anchor box 格式

`Yolo V1` 使用全连接层来直接预测目标的 `bounding box` 的坐标。

而在 `Faster R-CNN` 中，使用全卷积网络 `RPN` 来预测 `bounding box` 相对于 `anchor box` 的坐标的偏移量。由于预测网络是卷积网络，因此 `PRN` 在 `feature map` 网络的每个位置预测这些 `offset`。

> 相比于直接预测坐标，预测 `offset` 更简单，误差更小，可以简化问题，使得网络更容易学习。

> 因此，在 `Yolo V2` 中，移除全连接层，并使用 `anchor box` 来预测 `bounding box`。

**【移除池化层】**

首先，移除 `pooling layer`，使得网络的卷积层输出分辨率更高。

**【改变输入尺寸】**

同时，将输入尺寸变为 $$ 416 \times 416 $$。之所以这么做，是希望最后的输出特征尺寸上，只有奇数个格点单元，从而只有一个中心格点单元。

因为大多数目标，尤其是大目标，更倾向于占据图像的中心位置。因此，奇数个格点单元相较于四个紧邻的格点单元，能更好的预测目标的位置。

`Yolo V2` 的全卷积网络的下采样，总体缩减幅度为 `32`，因此最后的输出特征图尺寸为 $$ 13 \times 13 $$。

**【每个 `box` 对应一个目标】**

使用 `anchor box` 后，我们要消除 `class prediction` 与局部空间之间的耦合。

在 `Yolo V1` 中，每个格点单元只负责于测一个目标，而不管该格点单元有多少 `bounding box`。而在 `Yolo v2` 中，每个 `bounding box` 负责预测一个目标。

使用 `Anchor box` 之后，精度略微下降。但是，每张图中，`Yolo V1` 只能预测 `98` 个 `box`，而 `Yolo V2` 可以预测上千个。

因此，不用 `Anchor box` 对应 `69.5 mAP` 以及 `81%` 的召回率；而使用 `Anchor box` ，则可以达到 `69.2 mAP` 以及 `88%` 的召回率。

尽管精度略微下降，但是召回率大幅度提升意味着模型有较大的提升空间。

如下图所示，为每个格点单元对应的预测输出格式：

<div style="text-align:center">
<img src="/images/Yolo V2 输出格式.jpg" width="80%">
</div><br>

#### 自动提取先验框尺寸

使用 `Anchor box` 有两个问题，其中一个就是 `Anchor box` 的选定。

使用手工设定的 `Anchor box`，虽然网络可以进行学习适应。但是如果一开始就选定更好的 `Anchor box`，那么网络学起来更快更容易。

> 因此，我们对训练集的 `bounding box` 使用 `k-means clustering` 算法来自动选定合适的先验框 (priors boxes)。

如果使用标准的欧氏距离(`Euclidean distance`)，较大的 `box` 将会对应较大的 `loss`。而我们真正想要的是使得 `IOU score` 最大的先验框，这与 `box` 的尺寸无关。因此，对应的距离计算方式如下，可以看到，IOU 越大，距离越小：

$$
d(\text { box, centroid })=1-\text { IOU (box, centroid) }
$$

对于与不同的 `k`，在 `VOC` 和 `COCO` 数据集上，平均 `IOU` 表现如下所示：

<div style="text-align:center">
<img src="/images/k-means cluster IOU.png" width="90%">
</div><br>
最后，综合考虑模型复杂度和召回率，选定 `k=5`。实验表明，通过聚类算法得到的 `anchor box` 和手工选定的差别很大，聚类算法更倾向于高瘦的 `box`。

在 `VOC 2007` 上，不同的 `box` 生成方式表现如下所示：

<div style="text-align:center">
<img src="/images/不同 box 生成方式的表现.png" width="60%">
</div><br>

#### 坐标预测格式的改进

使用 `Anchor box` 遇到的第二个问题就是：模型不稳定，尤其是在早期迭代过程中。其中，主要的不稳定性源自 `box` 的 $$ (x, y) $$ 的预测。

在候选区方案中，预测值 $$t_x, ty$$ 和 $$(x, y)$$ 之间，计算方式如下：

$$
\begin{aligned} 
x &=\left(t_{x} * w_{a}\right)-x_{a} \\ 
y &=\left(t_{y} * h_{a}\right)-y_{a} 
\end{aligned}
$$

例如，若 $$t_x = 1$$，则表明 `box` 向右偏移量为 `anchor box` 的宽度。这种方式没有任何约束，将会导致不管在图像的什么位置进行预测， `bounding box` 都将可能出现在图像的任意位置。

> 因此，随机初始化的模型，需要训练较长时间，才能逐渐稳定，得到可用的偏移量。

因此，借鉴 `Yolo V1`，我们预测相对于网络格点的坐标偏移。这将限制 `ground truth` 处于 `0~1` 之间。我们用 `sigmoid` 函数来约束网络的预测值处于该区间。

对于输出特征图的每个格点单元，`Yolo V2` 预测 `5` 个 `bounding box`。对于每个 `bounding box`，网络预测 `5` 个坐标：$$ t_x, t_y, t_w, t_h, t_o $$。

其坐标换算方式如下所示：

$$
\begin{aligned} 
b_{x} &=\sigma\left(t_{x}\right)+c_{x} \\ 
b_{y} &=\sigma\left(t_{y}\right)+c_{y} \\ 
b_{w} &=p_{w} e^{t_{w}} \\ 
b_{h} &=p_{h} e^{t_{h}} \\ 
\operatorname{Pr}(\text { object }) * I O U(b, \text { object }) &=\sigma\left(t_{o}\right) \end{aligned}
$$

在上面公式中，$$ (c_x, c_y) $$ 表示格点单元相对于图像左上角的坐标；$$p_w, p_h$$ 表示先验框的尺寸 (`bounding box prior`)，预测值为 $$ t_x, t_y, t_w, t_h, t_o $$。

- 对于预测的 `bbox` 的中心，需要压缩到 `0-1` 之间，再加上 `anchor` 相对于`grid` 在 `x` 和 `y` 方向上的偏移。这一点，和 `yolo v1` 是一致的
- 对于预测的 `bbox` 的宽高，这个和 `faster RCNN` 一样，是相对于 `anchor` 宽高的一个放缩。`exp(w)` 和 `exp(h)` 分别对应了宽高的放缩因子
- 对于预测的 `bbox` 的置信度，则需要用 **sigmoid** 压缩到 `0-1` 之间。这个很合理，因为置信度就是要 $$0-1$$ 之间。
- 对于预测的每个类别，也是用你 **sigmoid** 压缩到 $$0-1$$ 之间。这是因为类别概率是在 $$0-1$$ 之间

最后通过换算得到的为在当前特征图尺寸上的坐标和尺寸，需要乘以整体缩放因子(32)，方可得到在原图中的坐标和尺寸。

> 这种参数化的方式，使得神经网络更加稳定。

使用 `dimension clusters` 加上直接预测 `bounding box` 中心坐标的方式，相较于直接使用手工设定 `Anchor box` 的方式，性能提升将近 `5%`。

坐标变换如下所示：

<div style="text-align:center">
<img src="/images/Yolo V2 坐标变换.png" width="90%">
</div><br>

#### 多输出尺度融合

$$13 \times 13$$ 的输出特征图，可以很好的预测较大尺寸的目标，但是对于小尺寸的目标，可能并不太好。

`Fast R-CNN` 和 `SSD` 通过在多尺度的特征图上进行 `RPN`，来获取不同的分辨率。我们这里采用另一种方式：添加 `passthrough layer` 从之前的 $$26 \times 26$$ 尺寸上的特征图获取更高分辨率的特征。 

这种 `passthrough layer` 通过 `stacking adjacent features` 的方式 `concatenate` 较高分辨率的特征图以及低分辨率的特征图，与 `ResNet` 类似。对于 $$26 \times 26 \times 512$$ 的特征图，将会被映射成 $$13 \times 13 \times 2048$$ 的特征图。最后性能提升 `1%`。

如下所示，为 $$26 \times 26 \times 512$$ 到 $$13 \times 13 \times 2048$$ 的转换方式：

<div style="text-align:center">
<img src="/images/Multi-Scale Training.jpeg" width="90%">
</div><br>


#### 多输入尺寸训练

由于网络只用到了卷积层和池化层，因此可以支持不同的输入尺寸。我们在训练的时候就使用不同尺寸图像，使得模型更加稳定健壮。

我们的做法是，每迭代几次，就改变一次输入尺寸。实际训练中，每 `10` 个 `batches` 就随机选定一个新的尺寸。

> 由于网络的总下采样系数为 32，所以尺寸只能是 32 的倍数，因此设定待选尺寸列表为：$$ \{ 320, 352, \dots, 608 \} $$。

这种模式使得神经网络能够学习预测不同分辨率的图像。由于较小的输入尺寸，网络运行更快。所以，`Yolo V2` 可以提供速度和精度之间的折中。

> 输入图像分辨率越高，则速度越慢；反之分辨率低，则速度快。

在 $$288 \times 288$$ 的输入尺寸下，运行速度为：$$ 90 FPS$$，`mAP` 与 `Fast R-CNN` 相当。这种尺寸很适用于较小的 `GPU`，处理较高帧率的视频或者多视频流处理。

高分辨率模式的 `Yolo V2` 性能最好，在 `VOC 2007` 上可达到 `78.6 mAP` ，速度仍满足实时的要求。与其他模型的对比如下所示：

<div style="text-align:center">
<img src="/images/Yolo V2 与其他模型的对比.png" width="85%">
</div><br>

<div style="text-align:center">
<img src="/images/Yolo V2 与其他模型的对比 2.png" width="85%">
</div><br>

#### 进一步的评测结果

下图是在 `VOC 2012` 测试集上的验证结果：

<div style="text-align:center">
<img src="/images/PASCAL VOC2012  测试集上的结果.png" width="98%">
</div><br>

下图是在 `COCO` 上 `test-dev 2015` 的验证结果：

<div style="text-align:center">
<img src="/images/COCO test-dev 2015 上的结果.png" width="98%">
</div>


# 更快 Faster

大多数 `detection` 系统以 `VGG-16` 作为基础的特征提取器 (`base feature extractor`)。但是 `vgg-16` 较为复杂，计算量较大。

`Yolo V2` 使用一个定制的神经网络，它基于 `Googlenet` 架构，运算量更小，速度更快，然而其精度相较于 `VGG-16` 略微下降。在 `ImageNet` 数据集上，$$ 224 \times 224 $$ 上，`top-5` 精度达到 88%，而 `VGG-16` 达到 90%。

## 新的基础架构 Darknet-19

> 在 `Yolo V2` 中，使用 `Darknet-19` 作为基础特征提取网络。

与 `VGG` 模型类似，大多数使用 $$ 3 \times 3 $$ 的卷积层，并在每一次 `pooling` 后，通道数加倍。此外，使用全局池化 ( `GAP`，`global average pooling` ) 来进行预测。同时，在 $$ 3 \times 3 $$ 的卷积层之间，使用 $$ 1 \times 1 $$ 的卷积层来压缩特征表达。此外，使用 `batch normalization` 来稳定训练，加速收敛以及正则化模型。

最后训练好的模型，即 `DarkNet-19`，有 `19` 层卷积层，`5` 层最大池化层。处理每张图像，`Darknet-19` 只需要 `5.58 billion` 次操作 。在 `ImageNet` 上，其 `top-1` 准确率可达 `72.9%`，`top-5` 准确率可达 `91.2%`。

完整的网络层如下所示：

<div style="text-align:center">
<img src="/images/DarkNet-19 网络层.png" width="70%">
</div><br>

## Training for classification

我们在标准 `ImageNet 1000 classification` 数据集上，训练 `DarkNet-19` `160` 个 `epoch`。训练使用 `SGD` 算法 ，初始学习速率为 0.1，`polynomial rate decay` 为 4，权值衰减系数为 `0.0005`，动量系数为 `0.9`。

在训练过程中，使用标准的数据增强，包括：`random crops`，`rotations`，`hue`，`saturation`，`exposure shifts` 等。

一开始使用 $$224 \times 224$$ 的输入尺寸训练，接着使用 `448` 的输入尺寸进行微调。在微调阶段，参数不变，但是初始学习速率为 `0.001`，只训练 10 个 `epochs`。在这种高分辨率模式下，`top-1` 的准确率为 76.5%，`top-5` 的准确率为 93.3%。

## Training for detection

在前面的预训练结束之后，移除最后的卷积层，添加 3 个 $$ 3 \times 3 $$ 的卷积层。每个卷积层包含 `1024` 个 `filters`，且后面都接上一个 $$ 1 \times 1 $$ 的卷积层作为输出层，其通道数为需要预测的输出数目。如下所示：

<div style="text-align:center">
<img src="/images/1 x 1 卷积核输出.png" width="90%">
</div><br>

对于 `VOC` 数据集，需要预测 5 个 `boxes`，每个 `box` 对应 5 个坐标，以及 20 个类别概率，共 125 个 `filters`。

同时在输出层（$$ 3 \times 3 \times 512 $$）与倒数第二个卷积层之间添加 `passthrough` 层，从而模型可以使用 `fine grain features`。

对于该修正的模型，训练 160 个 `epochs`，初始学习速率为 `0.001`，在第 60 和 90 个 `epochs` 的时候，分别除以 10。训练过程中，`weight decay` 为 `0.0005`，`momentum` 为 0.9。

对于输入数据，使用与 `Yolo` 和 `SSD` 类似的数据增强，包含：`random crops`，`color shifting`，`rotations`，`hue`，`saturation`，`exposure shifts` 等。

在 `COCO` 和 `VOC` 上，使用的策略相同。

# 更强 Stronger 

## 拓展数据集的基本思路

### jointly training 的基本概念

为了充分利用已有数据集，我们提出了 `jointly training` 的概念，即：使用具有 `detection` 标记的数据来学习检测相关的信息，比如说 `bounding box` 坐标预测等；使用不包含 `detection` 标记的数据来学习通用目标的分类。通过这种方式，可以充分拓展系统可检测的目标类别。

在训练过程中，我们将 `detection` 和 `classification` 数据集进行混合。当模型遇到具有 `detection` 标记的数据时，计算完整的 `Yolo V2 loss` ，并进行反向传播。而当遇到只有 `classification` 标记的图像时，只对 `loss` 中，与类别相关的部分进行反向传播。

### 数据集的合并问题

这种方式面临着一个新的问题。`Detection` 数据集包含常见的目标，以及通用的 `label`，如 `dog` 和 `boat` 等。而 `classification` 数据集包含更加宽泛的，更深层次的 `label`。比方说，`ImageNet` 包含 100 多种 `dog`，包括 `Norfolk terrier`，`Norfolk terrier`，`Bedlington terrier` 等。因此，如果想在两个数据集上进行训练，首先需要对数据集的 `label` 以一种合理的方式进行合并。

> 大多数 `classification` 模型使用 `softmax` 层来计算最后的类别概率分布。但是使用 `softmax` 的前提是：各个类别之间是互斥的。这使得数据集之间的合并并不那么容易。

因此，我们使用 `multi-label` 模型来合并 `label` 不互斥的数据集。

## `WordTree` 的构建方式

### WordNet 的概念

`ImageNet` 数据集的 ` label` 是从 `WordNet` 中提取的。`WordNet` 是一个语言的数据库，它对所有的概念 (词) 进行了结构化，并整理了各个概念之间的联系。

在 `WordNet` 中，`Norfolk terrier` 和 `Yorkshire terrier` 同级，都是 `terrier` 的子类。而 `terrier` 又是 `hunting dog` 的一种。`hunting dog` 又是 `dog` 的子类，`dog` 又是 `canine` 的一种。

大多数的 `classification` 方式，都设定在同一层级进行分类。而进行数据集合并的话，是需要 `label` 的结构化信息的。

`WordNet` 是一个有向图 (` directed graph `)结构，而不是一个树结构，因为语言是复杂的。例如：`dog` 是 `canine` 的一种，同时也属于 `domestic animal`，而这两者在 `WordNet` 中，是同级的。

### 根据 labels 创建 WordTree

> 下面，我们期望将所有 `label` 中出现的词汇，依据 `WordNet` 的有向图结构，构建用于数据集整合的树结构 (`WordTree`)。

为了构建一棵树，我们检查 `label` 中的每一个概念（词），沿着 `WordNet` 图，到根节点的路径，在这里为 `physical object`。大多数的 `label` 沿着 `WordNet` 到根节点，只有一条路径，因此，我们先将它们添加到树结构 `WordTree` 中。

接着，我们反复检查 `labels` 中剩余 的概念。对于在 `WordNet` 中有多路径的概念，选择其中使得 `WordTree` 增长最慢的那条路径，添加到结构树中。

比方说，如果一个概念，在 `WordNet` 中有 2 条到达根节点的路径。沿着其中一条，添加到 `WordTree` 时，`WordTree` 的长度增加一个；而另一条路路径使得 `WordTree` 的长度增加 2 个，则选择第一个长度增加较短的路径。

经过上面的迭代，最后得到一个树结构 `WordTree`，包含所有的 `label` 中出现的概念。

### 类别的概率

当使用 `WordTree` 进行 `classification` 时，我们预测每一节点对应的条件概率，例如，对于 `terrier` 节点，我们将预测：

$$
\begin{aligned}
P_r(\text{Norfolk terrier} &| terrier) \\

P_r(\text{Yorkshire terrier} &| terrier) \\

P_r(\text{Bedlington terrier} &| terrier) \\

\dots

\end{aligned}
$$

如果想计算某一节点的绝对概率，需要沿着 `WordTree` 到根节点的路径，累乘每一个节点上的条件概率。比方说要计算 `Norfolk terrier` 的概率：

$$
\begin{aligned}
P_r(\text{Norfolk terrier}) = P_r(\text{Norfolk terrier} &| terrier) \\
* P_r(terrier &| \text{hunting dog}) \\
* \dots * \\
* P_r(\text{mammal} &| animal) \\

* P_r(\text{animal} &| \text{physical object}) \\
\end{aligned}
$$

对于 `classification` 任务，我们假设图像一定包含目标，因此：$$ P_r(\text{physical object}) = 1$$。

为了验证这种方式的可行性，我们在用 1000 类的 `ImageNet` 构建的 `WordTree-1000` 上训练了 `DarkNet-19`。为了构建 `WordTree-1000`，我们加入了所有的中间节点，因此总结点数为 `1369`。

在训练过程中，我们沿着 `WordTree` 对 `ground truth` 进行回溯。因此，如果一个图像的 `label` 为 `Norfolk terrier`，那么它同样会被 `labeled` 为 `dog`，`mammal` 等等。

为了计算条件概率，模型预测一个长度为 1369 的向量。我们只对同一个词的所有下一级词汇使用 `softmax` 对应词汇的条件概率。因此，一共会进行多次  `softmax`。最后总长度为 1369。如下图所示：

<div style="text-align:center">
<img src="/images/WordTree softmax.png" width="85%">
</div><br>
### 实验结果

使用与之前一样的训练参数，这个 `DarkNet-19` 达到了 71.9% 的 `top-1` 精度，以及 90.4% 的 `top-5` 精度。尽管多增加了 369 个类别，并预测一个 `tree` 结构的概率，我们的整体精度下降并不明显。但是，这种方式下，模型可以很轻松地识别出新的未知类型的目标。比方说，如果模型看到了一个 `dog`，其不确定具体的品种时，仍可以以较高的 `confidence` 预测为 `dog`。

这种方式同样适用于 `detection`。因此，`Yolo V2` 预测一个 `bounding box` 以及概率树。向下遍历该概率树，每次选定置信度最高的那条支路，直到条件概率的累积达到某一阈值。

## 使用 WordTree 进行数据集合并

可以利用 `WordTree` 来以一种可接受的方式组合多个数据集。只需要将数据集中的 `categories` 映射到 `WordTree` 即可。`WordTree` 多样性丰富，可支持大多数的数据集合并。下图是一个 `combine` `ImageNet` 和 `COCO` 数据集的示例。

<div style="text-align:center">
<img src="/images/ImageNet & COCO combine.png" width="95%">
</div><br>

## 用 `classification` 和 `detection` 数据集进行训练

使用 `WordTree` 对数据集进行 `combine` 之后，就可以在 `classification` 和 `detection` 上进行组合训练了。

我们使用 `COCO detection` 数据集和 `full ImageNet release` 数据集中的 `top 9000` 类来训练模型。同时使用 `ImageNet detection` 任务中，不与 `COCO` 数据集种类别重复的部分数据对模型进行评估。

最终，`WordTree` 包含 9418 个节点类。由于 `ImageNet` 相较于 `COCO` 规模大的多，所以对 `COCO` 进行重复采样，最终 `ImageNet` 和 `COCO` 的样本比例为 $$ 4:1 $$。

我们使用上面的数据集训练 `Yolo9000`。将 `Yolo V2` 中 5 个先验框换成 3 个来限制输出尺寸，便得到了 `Yolo9000`。当网络检测到了 `detection image`，照常反向传播。对于其中的 `classification loss` 时，只计算相应的 `label` 层级的 `loss`，并进行反向传播。

当网络检测到 `classification image` 时，我们只对 `classification loss` 进行反向传播。我们只需要找到 `bounding box` 预测的最高概率的类别，并计算该类别在 `WordTree` 上对应的 `loss`。

```
We also assume that the predicted box overlaps what would be the ground truth label by at least :3 IOU and we backpropagate objectness loss based on this assumption
```

> 使用这种联合训练的方式，`Yolo9000` 可以学会使用 `COCO` 中的 `detection` 数据来推理图像中目标的位置，同时是使用 `ImageNet` 中的 `classification` 数据来进行类别区分。

## 实验结果

我们在 `ImageNet` 的 `detection task` 数据集上对 `yolo9000` 进行评估。`ImageNet` 的 `detection task` 与 `COCO` 之间有 44 个类别的重合，这意味着，`Yolo9000` 只学习过大多数测试图像上的 `classification` 信息。

经过测试，`Yolo9000` 在完整的 200 类目标上，取得了 `19.7 mAP` 的精度；而在不重合的 156 类目标上，取得了 `16.0 mAP` 的精度。

`Yolo9000` 的检测是实时的，可同时检测所有 9000 类的目标。