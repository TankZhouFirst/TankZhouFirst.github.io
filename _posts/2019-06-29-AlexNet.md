---
layout: post
title:  "AlexNet"
date:   2019-06-29 21:45:01 +0800
categories: 人工智能
tag: 图像分类
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考：**

- **Paper :** [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- **Code :** [GitHub](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py)

****


# 引入

## 问题背景

本文在如下背景下展开研究：

1. 当前图像分类任务主要是通过传统机器学习的方法进行的，模型容量小，且不易于实际使用，容易过拟合
2. 实际目标多样性丰富，标记好的数据集的样本数越来越大，需要更高容量的模型进行学习。而卷积神经网络可以通过调节深度和宽度来控制模型的容量，且充分利用了自然图像的局部空间相关性的特性
3. `GPUs` 等硬件以及高度优化的 `2-D` 卷积运算的实现发展成熟，以足够强大，可用于训练较大的 `CNNs`，结合如今的大数据集，不用过分担心过拟合

## 本文主要贡献

本文的模型，即 `AlexNet`，其由多伦多大学，`Geoff Hinton` 实验室设计，夺得了 `2012` 年 `ImageNet ILSVRC` 比赛的冠军，且其 `top-5` 错误率远低于第二名，分别为 `15.3%` 和 `26.2%`。

`AlexNet` 在深度学习发展史上的**历史意义远大于其模型的影响**。在此之前，深度学习已经沉寂了很久。在此之后，深度学习重新迎来春天，卷积神经网络也成为计算机视觉的核心算法模型。

本文的主要内容有：

1. 使用 `ReLU` 激活函数加速收敛
2. 使用 `GPU` 并行，加速训练。也为之后的分组卷积（`group convolution`）理论奠定基础。
3. 提出局部响应归一化（`Local Response Normalization`, `LRN`）增加泛化特性 (虽然被后人证明无效 )
4. 使用交叠池化 (`Overlapping Pooling`) 防止过拟合
5. **提出** `Dropout`，数据增强等手段防止过拟合

# ImageNet 数据集

## ImageNet 和 ILSVRC

（截至论文发表时）`ImageNet` 数据集包含 `15 million` (`1500` 万) 张标记好的高分辨率的图像，其包含仅 `22000` 个类别。从 `2010` 年始，作为 `Pascal Visual Object Challenge` 的一部分，一个新的年度比赛 `ImageNet Large-Scale Visual Recognition Challenge` (`ILSVRC`) 拉开帷幕。

## ILSVRC 数据集

`ILSVRC` 使用 `ImageNet` 数据集的子集，包含 `1000` 类目标，每类目标约 `1000` 张图片，共 `1200` 万张训练图像，`50000` 张验证图像，`15000` 张测试图像。

`ILSVRC-2010` 是唯一一届公开测试集 `label` 的`ILSVRC`，因此我们的实验测试在该数据集上进行。评判标准为 `top-1` 和 `top-5` 错误率，分别表示预测类别正确，以及正确值位于预测概率最高的 `5` 个类别时的错误率。

## 本文中对图像的预处理

由于 `ImageNet` 包含多种分辨率，而我们的模型要求输入维度固定，因此，我们对图像进行处理，使之固定为 $$256 \times 256$$。具体做法是，对于给定的图像，我们先将其进行缩放，使其短边尺寸为 `256`，然后进行中心裁剪，得到输入图像。除了在每个像素上，减去整个数据集对应像素上的均值之外，我们并未对输入图像进行其他预处理。

# 网络架构

## ReLU 激活函数

在此之前，激活函数主要使用的是 $$f(x) = tanh(x)$$ 以及 $$f(x) = \frac{1}{1 + (e^{-x})^{-1}}$$，但是这些都是饱和激活函数，输入值处于饱和区时，其梯度几乎为 0，因此收敛极慢！

针对这一问题，我们使用线性整流单元 (`Rectified Linear Units，ReLU`) 作为激活函数，即：$$f(x) = max(0, x)$$。其不存在饱和区，导数始终为 1，梯度更大，计算量也更少，因此收敛更快。

如下图所示，为 `tanh` 和 `ReLU` 的收敛速度对比：

<div style="text-align:center">
<img src="/images/ReLU 和 tanh 的收敛速度对比.png" width="50%">
<p>tanh 和 ReLU 收敛速度对比</p>
</div><br>

## GPU 并行训练

单个 `GTX 580 GPU` 只有 `3GB` 的显存，这将限制可训练的网络的最大尺寸和 `batch size` 大小。因此，我们将模型分为两部分，分布到两个 `GPU` 上进行训练。由于 `GPU` 之间可以直接进行数据交换，而无需经过主机的内存，因此可以很容易进行并行。

该方式大大的加快了训练速度，具体细节详见[模型的详细结构](#模型的详细结构)。

该技术使得 `top-1` 和 `top-5` 误差分别降低 `1.7%` 和 `1.2%`。

## 局部响应归一化 (LRN)

在神经生物学有一个概念叫做**侧抑制**（`lateral inhibitio`），指的是被激活的神经元抑制相邻神经元。归一化（`normalization`）的目的是“抑制”，局部归一化就是借鉴了**侧抑制**的思想来实现局部抑制。

当使用 `ReLU` 时，这种**侧抑制**很管用，因为 `ReLU` 的响应结果是无界的，所以需要归一化。使用局部归一化的方案有助于**增加泛化能力**。

局部响应归一化 (`Local Response Normalization`) 的核心思想就是利用近邻数据进行归一化，其公式如下所示：  

$$
b_{x, y}^{i}=a_{x, y}^{i} /\left(k+\alpha \sum_{j=\max (0, i-n / 2)}^{\min (N-1, i+n / 2)}\left(a_{x, y}^{j}\right)^{2}\right)^{\beta}
$$  

其中：

1. $$a_{x,y}^i$$ 表示第 $$i$$ 个卷积核，作用于位置 $$(x,y)$$ ，然后进行 `ReLU` 后，得到的神经元输出。
2. `N` 表示该层卷积核的总数目
3. $$ k, n, \alpha, \beta $$ 为超参数，其值通过验证集确定
4. $$n$$ 表示同一位置上，邻近的卷积核的数目

我们使用 $$ k = 2, n = 5, \alpha = 10^{-4}, \beta = 0.75 $$。我们在 `ReLU` 层之后使用 `LRN`。局部响应归一化使得 `top-1` 和 `top-5` 误差分别降低 `1.4%` 和 `1.2%`。

`LRN` 的直观解释就是，在第 $$i$$ 个特征图上的 $$(x, y)$$ 处的神经元的值，通过其邻近的 $$n$$ 个特征图上，同一位置的值平方和的相关运算，最后得到的值作为该特征图上，对应位置的新值。

可想而知，其计算量不小！后面的研究者也发现，`LRN` 并无实际作用，反而增加不少的计算量，因此一般都不再使用。

## 交叠池化 (Overlapping Pooling)

在一般池化中，池化窗口 $$z$$ 与滑动步长 $$s$$ 相等。而交叠池化指的就是 $$s < z$$ 的池化，此时相邻的滑窗之间会有重叠。在本文实现中，我们使用 $$z = 3, s = 2$$，这时的我们的 `top-1` 和 `top-5` 误差率分别降低 `0.4%` 和 `0.3%`。

同时我们发现，使用 `overlapping pooling` 方式，更不易发生过拟合。

## 模型的详细结构

`AlexNet` 的整体结构如下所示：

<div style="text-align:center">
<img src="/images/AlexNet 模型的并行结构.png" width="95%">
<p>AlexNet 模型结构</p>
</div><br>

如上图所示，`AlexNet` 共包含 `8` 个可学习的层，其中前 `5` 层为卷积层，后为全连接层，最后接一个 `1000` 路的 `softmax` 层，用于分类。

同时，为了进行多 `GPU` 并行训练，我们将几乎所有层均分，分别放置于两个 `GPU` 上进行训练。其中，两个 `GPU` 之间仅在特定层上进行通信：

1. 在第 `3` 层卷积层的时候，同时以前一层在两个 `GPU` 上的输出进行联合输入，其他卷积层中，`GPU` 之间数据不互通
2. 在全连接层，`GPU` 数据始终互通

具体的结构和参数如下所示：

<div style="text-align:center">
<img src="/images/AlexNet 详细层.jpg" width="99%">
<p>AlexNet 参数和特征图尺寸</p>
</div><br>

# 减缓过拟合

我们的神经网络包含 `60 million` 参数。尽管 `1000` 累的 `ILSVRC` 数据集较为庞大，但是仍不足以抵制模型的过拟合。因此，我们使用如下技巧来减缓过拟合。

## 数据增强

最简单常用的减缓过拟合的方式就是人工扩充数据集。我们使用了两种不同的方式进行数据增强，且不额外增加过多的计算量，因此可以在线转换。在我们的实现中，我们使用 `Python` 在 `CPU` 上进行数据增强，而 `GPU` 用于训练数据，因此无需占用额外的 `GPU` 计算资源。

第一种方式是，从 $$256 \times 256$$ 的图像及其水平镜像中，随机 `crop` $$224 \times 224$$，然后在这种 `patches` 上训练模型。相当于训练集的增长倍数为 $$ (256 - 224)^2 \times 2 = 2048$$。不使用这种方式时，模型会发生过拟合，因此我们不得不使用更小的模型。在测试期间，分别对输入图像及其水平翻转抽取 `5` 个 $$ 224 \times 224$$ 的 `patches`（`4` 角 + 中心），进行预测。然后将 `10` 个预测值进行平均。

另一种方式是，对 `RGB` 空间做 `PCA`，然后对主成分进行 $$(0, 0.1)$$ 的高斯扰动，也就是对颜色，光照等做变换。通过这种方式，`top-1` 误差率减少了 `1%`。(这一段不太懂，所以未解释清楚)

## Dropout

结合不同模型的预测值，是一种减小测试误差的不错的方式，但是其代价极其昂贵。因此，使用 `dropout` 技术，以 `0.5` 的概率，将每个隐藏神经元的值设定为 `0`。被 `dropout` 的神经元将不参与前向和反向传播。

在训练阶段的每次前向传播中，都会重新进行 `dropout`。因此，每次有新的输入时，模型会被随机采样成不同的架构，但是所有的架构共享权值。该技术可以减少神经元之间的相互依赖性。因此，模型被强制学习更加稳健的特征。

在测试期间，我们使用所有的神经元，只不过将其输出值乘以 `0.5`，以保证总的等效输出值不变。我们在网络的的前两层全连接层之间使用 `dropout`。

如果不使用 `dropout` ，模型可能会过拟合，但是使用 `dropout`，模型训练将需要近两倍的迭代次数。

`dropout` 图示如下所示：

<div style="text-align:center">
<img src="/images/Dropout.png" width="80%">
<p>Dropout 原理图示</p>
</div><br>


# 训练细节

我们使用随机梯度下降来训练模型，`batch size = 128`，动量系数为 `0.9`，权值衰减系数为 `0.0005`。我们发现，这种小的权值衰减对于模型的学习很重要。换言之，权值衰减不仅仅是一个正则器，其同时降低了训练误差。

权值 $$w$$ 的更新规则如下：  

$$
\begin{aligned} v_{i+1} & :=0.9 \cdot v_{i}-0.0005 \cdot \epsilon \cdot w_{i}-\epsilon \cdot\left\langle\left.\frac{\partial L}{\partial w}\right|_{w_{i}}\right\rangle_{D_i} \\ w_{i+1} & :=w_{i}+v_{i+1} \end{aligned}
$$

其中，$$i$$ 为迭代索引，$$v$$ 表示动量值，$$\epsilon$$ 表示学习速率，下式表示第 $$i$$ 个 `batch` $$ D_i $$ 中，目标函数对 $$w_i$$ 的梯度的均值。

$$ \left\langle\left.\frac{\partial L}{\partial w}\right|_{w_{i}}\right\rangle_{D_{i}}$$ 

我们初始化每一层的权值为 `0` 均值，标准差为 `0.01` 的高斯分布。我们初始化第 `2，4，5` 层卷积层，以及全连接隐藏层的偏置为 `1`。这种初始化通过给 `ReLU` 输出正值，加速了网络早期的训练。我们初始化剩余层的偏置为 `0`。

对于每一层，我们使用相同的学习速率，在训练过程中手动调节：每次当验证误差不再下降时，学习速率减小为原来的十分之一。学习速率初始化为 `0.01`，并共减小三次。我们的模型最终在 `1.2 million` 张训练图片上，训练了 `90` 个周期，在两张 `NVIDIA GTX 580 3GB GPUs` 上，花费了 `5` 到 `6` 天的时间。

# 实验结果

我们在 `ILSVRC-2010` 上的结果如下表所示。其中，`top-1` 和 `top-5` 的测试误差分别为 `37.5%` 和 `17.0%`。

<div style="text-align:center">
<img src="/images/AlexNet 在 ILSVRC-2010 测试集上的表现.png" width="50%">
</div><br>

我们同时将模型在 `ILSVRC-2012` 上进行了测试，如下表所示。由于 `ILSVRC-2012` 测试集的 `label` 并未公开，因此无法知晓所有我们测试过的模型的误差。

<div style="text-align:center">
<img src="/images/Comparison of error rates on ILSVRC-2012 validation and test sets.png" width="70%">
</div><br>

如上表所示，为各模型在 `ILSVRC-2012` 验证集和测试集上的误差率。其中，$$*$$ 表示在 `ImageNet 2011` 完整数据集上进行预训练的模型。

1. `1 CNN` 表示一个上文中的模型，其 `top-5` 的误差率为 `18.2%`
2. `5 CNNs` 表示 `5` 个上文中的模型组成的集成模型，其 `top-5` 的误差率为 `16.4%`
3. $$1 CNN^*$$ 表示在上述模型的最后一层 `pooling` 之后，额外加一个卷积层得到的模型。先在整个 `ImageNet 2011` 数据集上预训练，然后在 `ILSVRC-2012` 数据集上进行 `fine-tune`，得到的模型的 `top-5` 误差率为 `16.6%`。
4. 将 `3` 中的模型，与 `5` 个前面提到的模型的预测值进行平均，对应的 `top-5` 误差率为 `15.3%`。
