---
layout: post
title:  "梯度更新算法"
date:   2019-08-20 16:39:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式转载**

****

## 随机梯度下降

### 批量梯度下降（Batch Gradient Descent, BGD）

一开始的梯度下降算法，称为批量梯度下降（`batch gradient`），每次损失函数的计算都是所有样本的共同结果。因而梯度计算时，必须等待所有的样本计算完毕才可以进行迭代，参数更新太慢，效率很低。此外，如果训练样本数较大，则运算时对 `GPU` 资源消耗较大，显存需求很高。同时，批量梯度下降也不支持在线学习。

$$
\begin{aligned}
h_{\theta}(x) &= \sum_{j=0}^n \theta_jx_j \\

J_{train}(\theta) &= \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})^2 \\

\theta_j &:= \theta_j - \alpha \frac{\partial J}{\partial \theta_j} = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}
\end{aligned}
$$


- 第一个公示表式计算一个样本的输出；
- 第二个公式表示计算所有样本的平均误差；
- 第三个公式表示对每一个参数进行更新。

### 随机梯度下降(stochastic Gradient Descent, SGD)

> 每次迭代只训练一个样本，也就是说，每计算完一个样本，就进行梯度计算，并更新参数，直到收敛。

该方式可作为 `online learning`。其优点是参数更新很快，且不需要占用大的显存。但是缺点也很明显，因为不同训练样本的差距和噪声的影响，每次参数更新方向未必是正确的优化方向，但实践证明总体趋势一般是朝着最优点方向的。

`SGD` 不仅仅效率高，而且随机性有时候反而是好事。对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。

### 小批量随机梯度下降（Mini-batch Gradient Descent）

> 梯度更新的时候，使用批量梯度下降，会导致参数更新太慢，效率低下；而使用随机梯度下降算法，则可能由于样本的随机性，一直处于最优解附近徘徊。

所以，为了提高效率，并减少样本的随机性，可以使用小批量随机梯度下降算法。每次计算完小批量后，就计算梯度，更新参数。具体的 `batch_size` 选取，要看数据集大小和显卡性能，太大或太小都不好，应该选择一个适中的尺寸来达到训练效果最优。

如下图所示，左边为批量随机梯度下降算法，损失平滑下降。而右边的曲线则为小批量随机梯度下降算法的损失变化，较为曲折，但整体趋势下降。

<div style="text-align:center">
<img src="/images/随机梯度下降.png" width="90%">
</div>

> 此外，一般 `batch size` 为 2 的幂时，计算机处理起来更快。一般为 `64 ~ 512`，并且要看 `CPU / GPU` 而定，若 `CPU / GPU` 缓存不够，则可能效果更糟。

### 集中梯度下降算法图示

<div style="text-align:center">
<img src="/images/梯度下降图示.png" width="80%">
</div>

<div style="text-align:center">
<img src="/images/梯度下降图示2.png" width="80%">
</div>

<div style="text-align:center">
<img src="/images/梯度下降直观感受.png" width="80%">
</div>

### Python 实现

```python
def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    np.random.seed(seed)
    m = X.shape[1]       # 训练样本数
    mini_batches = []
    
    # Step 1: 打乱顺序
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1,m))
    
    # Step 2: 开始划分
    num_complete_minibatches = math.floor(m/mini_batch_size)
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[:,k*mini_batch_size: (k + 1)*mini_batch_size]
        mini_batch_Y = shuffled_Y[:,k*mini_batch_size: (k + 1)*mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        
        # 处理剩余不足部分
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[:,k*mini_batch_size:]
        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        
    return mini_batches
```

## 梯度校验（Gradient Checking）

在使用神经网络时，使用梯度下降算法时，可能表面会工作，代价函数一直在下降，但实际上结果可能和参考值有较大偏差。针对这种情况，可以使用梯度检验的方法来消除。

### 梯度近似

可以用如下所示公式来求近似偏导，一般 $$\epsilon = 10^{-4}$$：

$$
\frac{dJ(\theta)}{d\theta} \approx \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}
$$

上面是单个元素的计算，对于神经网络，需要对权值矩阵中的每个元素进行这样的计算。

### 梯度校验

经过上面的近似，最后得到 `gradApprox` 向量，然后将其与通过反向传播算法得到的向量 `Dvec` 进行比对，如果近似，则说明我们的模型训练良好。可以将 `Dvec` 带入到后续操作中。注意：梯度检验仅用于检查模型训练好后，梯度是否计算正确，正常使用中不要引入，因为计算很慢 ！！！

## 在线学习（online learning）

在拥有连续数据流，需要一个算法来从中学习时，可以使用在线学习。比如说某网站有一个连续的用户流带动的数据流，网站可以使用在线学习机制，从数据流中学习用户的偏好，便于优化一些关于网站的决策。

假设现在运营一个快递公司网站，当用户指定起始和目的地，网站提供一个价格，用户可以选择接受或者拒绝。我们用 $$x$$ 表征用户的特征，起始地，目的地以及价格等。现在网站要做的就是，训练一个模型来优化价格。首先，网站必须一直运营，所以外层必须是死循环。当某用户访问时，获取用户的特征向量，网站要做的就是使用刚刚的特征变量来更新模型参数。

对于在线学习机制，由于没有提前知晓样本数，只是有源源不断地数据流，所以针对每个数据，都需要进行一次更新。

在这种机制下，我们抛弃样本集的概念，每次获取一个样本，处理一个样本后就丢弃，不再使用该样本。所以，该算法只针对拥有大量数据流的网站适用，因为样本是无限的。但对于用户数较少，流量小的网站，需要将每次的数据进行保存，使用本地算法进行拟合。

此外，由于算法始终根据当前特征值进行调整，因此能够很好地适应用户的喜好的变化。也就是说，模型将会舍弃以前的样本，只拟合当前最近的喜好。

在线学习算法的另一个应用就是学习预测的点击率，估计点击率 `CTR（Click Through Rate）`。首先根据用户搜索的内容，假设有 `100` 个结果，但只能展示 `10` 个。对于每个结果，都会有一个特征向量与之对应。当用户点击某个结果时，就会根据用户的模型参数和产品的特征值来预测产品可能被点击的概率，推荐其中最高的 `10` 个进行推送。每次点击后，都会重新计算，并更新推荐列表。

## 映射减约（map reduce）

 前面讲到的所有算法都只能在一台计算机上运行。当算法设计的数据量很大，以至于任何算法都不足以用一台计算机搞定，此时可以用大规模机器学习的另一种方法，即映射减约。相比于梯度下降算法，映射减约能够处理更大规模的机器学习算法。

我们以样本数为 `400` 为例，实际样本数应该很大。假设现在有 `4` 台计算机，我们将样本集分为 `4` 份，分别交给不同的计算机。每台计算机计算方差和的一部分，最后统一交给中心运算服务器处理。上面的过程实际上就是将批量梯度下降的运算量分摊到多个计算机中。实际上，对于一台计算机，也可以映射到多核上进行处理。