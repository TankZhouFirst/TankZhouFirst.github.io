---
layout: post
title: "深度学习中的反向传播算法"
date: 2019-08-20 17:19:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式转载**

****

# 反向传播算法（BP, backpropagation algorithm）

## 核心思想

模型训练过程中，使用梯度下降算法进行参数更新。但是，问题的关键在于，如何计算损失相对于参数的梯度？

$$
w_{i,j}^` = w_{i,j} - \alpha \frac{\partial E}{\partial w_{i,j}}
$$

对于神经网络而言，之前层的参数对最后的总误差有一定贡献（用**误差率**表征某个神经元节点的输出对总输出误差的影响）。

>   **所以，`BP` 算法的核心思想是，通过链式求导法则，求总损失对所有参数的偏导。**

## BP 算法推导

以下面网络为例，进行反向传播算法的推导演示：

<div style="text-align:center">
<img src="/images/BP 算法推导.png" width="90%">
</div>

### 前向传播推导

前向传播计算如下所示，假设用一样的激活函数 `g` ：

<div style="text-align:center">
<img src="/images/前向传播.png" width="45%">
</div>

### 反向传播推导

首先假设通过损失函数 `f` 计算得到的输出 `y` 与 `target` 之间的损失为 `E`，通过链式求导法则，反向传播推导各参数的梯度如下：

<div style="text-align:center">
<img src="/images/反向传播推导.png" width="60%">
</div><br>

上面就是反向传播的推导过程，当然离具体的应用公式还有差距。仔细观察上面的推导，会发现有很多公共部分。

所以计算过程中，需要保存前向传播的部分结果，以及反向传播的部分结果，利用这些公共部分，可以极大地避免反复计算同一节点的值，极大地提高计算效率。

# 时序反向传播算法（BPTT, Back Propagation Through Time）

由于 `RNN`, `LSTM` 等网络需要沿着时间线进行反向传播，所以称之为：**Back Propagation Through Time**。

## RNN 的反向传播

### 设定

首先假设输入为 $$x = {\{x_1, x_2, ..., x_T\}} $$，输出为 $$y = \{ {y_1, y_2, ..., y_T} \}$$，`RNN` 要学习 $$ f : x \to y$$。

### 前向传播（暂时不考虑偏置）

<div style="text-align:center">
<img src="/images/RNN 前向传播.png" width="90%">
</div><br>

如上图所示，`RNN` 前向传播公式如下所示：

$$
h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_{t})  

z_t = softmax(W_{hz}h_t)
$$

其中：

-   $$W_{hh}$$ 表示的是旧隐含层信息对新隐含层信息的影响；
-   $$W_{xh}$$ 表示的是输入层信息对隐含层信息的影响；
-   $$W_{hz}$$ 表示的是隐含层信息对输出结果的影响。

### 反向传播

首先定义损失函数：

$$
E=\sum_{t=0}^{T}{E_{t}}
$$

上面的损失考虑了所有时刻的误差，基于 `E` 的反向传播算法也叫做 `BPTT` (`Back Propagation Through Time`)。

对两边求导，有：

$$
\frac{\partial{E}}{\partial{W}}=\sum_{t=0}^{T}{\frac{\partial{E_{t}}}{\partial{W}}}
$$

用 $$E^t$$ 对 $$W_{hh}, W_{xh}, W_{hz}$$ 求导，有：

<div style="text-align:center">
<img src="/images/LSTM 反向传播.png" width="95%">
</div><br>

上面公示的含义如下图所示，即：

1.  将时刻 `t` 的输出误差，沿时间链反向传播，得到其在每个时间步上对 `W` 的梯度，将这些梯度进行求和，即为误差 $$E_t$$ 对 `W` 的总梯度。
2.  最后 `W` 的总梯度为所有时间步，沿着时间链反向传播的所有的梯度之和。即：三条红色，两条蓝色，一条绿色方向上的梯度之和。

<div style="text-align:center">
<img src="/images/LSTM 反向传播 2.png" width="60%">
</div>

### 存在的问题

从上面推导可以看出，在时间链上，对 $$W_{hh}$$ 和 $$ W_{xh} $$ 求导，会产生连乘项，而这恰好就可能会**导致梯度消失和梯度爆炸**。

看连乘项中的每一个部分，即：$$\delta = tanh^{'} W^{hh}$$。其中，$$tanh^{'}$$ 的上界为 ·，所以：

1. 若 $$W^{hh} > 1$$，则会有 $$\delta > 1$$，所以时间链较长时，连乘就会发生梯度爆炸；
2. 若 $$W^{hh} < 1$$，则会有 $$\delta < 1$$，所以时间链较长时，连乘就会发生梯度消失；

关于第二点，实际上存在大量的"短距"连乘项，所以梯度不会为 `0`。但是：

> 在随机梯度下降的最开始，公式中的"短距"连乘项或许会产生一些梯度方向。随着参数的动态更新，这些"短距"连乘项构成的方向会引导 `Loss Function` 到局部最优的位置上去。
>
> 假如在这个过程中梯度爆炸没有发生，那么当 `Loss Function` 越接近于局部最优，**这些"短距"连乘项就会越接近于 0**。毕竟，对于 RNN 模型，在整个迭代过程中，如果不考虑"长距"连乘项，局部最优的地方就是梯度为 0 的地方。

> 可以假想，我们优化的是一个梯度不包含"长距"连乘项的特殊神经网络。当整个特殊神经网络已经优化的很好的时候，所有的"短距"连乘项的求和都接近于 `0`，而各个"长距"连乘项由于也接近于 0, 从而导致整个 `RNN` 的梯度趋于 0。梯度消失发生，模型的参数得不到进一步的更新，`Loss function` 的下降遭遇瓶颈！

## LSTM 的解决方式

### LSTM 传播路径

<div style="text-align:center">
<img src="/images/LSTM 传播路径.jpg" width="95%">
</div><br>


如上图所示，为 `LSTM` 网络的传播路径。

>   **`RNN` 依赖 h 来传播状态，与之不同，`LSTM` 依靠 C 来传播状态，而不是 h，这一点十分重要！！！**

### LSTM 反向传播粗略推导

由于 LSTM 依靠 $$C^t$$ 传输信息，所以为了验证方便，忽略 $$h_t-1$$ 的影响。则：

$$
C^t = \sigma(W^fX^{t} + b^f)C^{t-1} + \sigma(W^iX^{t} + b^i) tanh(W^cX^t+b^c)
$$

与 `RNN` 一样，将其沿着时间链进行推导，可以得到如下连乘项：

$$
\prod_{j=k+1}^{t} \frac{\partial C^t}{\partial C^{t-1}} = \prod_{j=k+1}^{t} \sigma(W^f + b^f)
$$

上面每一项在 `0~1` 之间，但是在实际参数更新中，可以通过控制 `bias` 比较大，使得该值接近于 `1`；在这种情况下，即使通过很多次连乘的操作，梯度也不会消失，仍然可以保留"长距"连乘项的存在。即：总可以通过选择合适的参数，在不发生梯度爆炸的情况下，找到合理的梯度方向来更新参数，而且这个方向可以充分地考虑远距离的隐含层信息的传播影响。

这种情况对比 `RNN` 就很难实现，因为 `RNN` 中远距离的隐层影响要么非常强，要么就非常弱，所以难以找到合适的梯度来优化这些远距离的信息效应。

另外需要强调的是，`LSTM` 搞的这么复杂，除了在结构上天然地克服了梯度消失的问题，更重要的是具有更多的参数来控制模型；通过四倍于 `RNN` 的参数量，可以更加精细地预测时间序列变量。

在预测过程中，`LSTM` 可以通过一套更复杂的结构来说明深度循环神经网络中，哪些历史信息应该记住，哪些历史信息应该遗忘，以及是否每个阶段都应该有有效的信息被输出！

**但是：** 在时间链上不会发生梯度消失或梯度爆炸，不意味着在每个时间步的网络层中不会（竖向）。

### LSTM 反向传播的详细推导

#### 前向传播的计算

LSTM 中前向传播计算方式如下所示，这里只看一个时间步的运算。如下图所示，为单时刻前向传播的计算方式：

<div style="text-align:center">
<img src="/images/LSTM 前向传播推理.jpg" width="95%">
</div><br>

<div style="text-align:center">
<img src="/images/LSTM 前向传播推理 2.png" width="70%">
</div><br>

#### 反向传播的推导

##### 反向传播路径

<div style="text-align:center">
<img src="/images/反向传播路径.jpg" width="95%">
</div><br>

##### 参数尺寸

设输入 $$x_t$$ 尺寸为 $$n \times 1$$，上一级输出 $$h_{t-1}$$ 尺寸为 $$m \times 1$$；隐藏层尺寸为 `p`，则其他变量尺寸如下所示：

$$
W_f, W_i, W_c, W_o : p \times (m + n)  

f_t, i_t, \tilde{C}_t, o_t, C_t, C_{t-1}, h_t : p \times 1
$$

可以发现，`p = m`，均为隐藏层的尺寸。

##### LSTM 反向传播推导

***一些符号定义***

- ⊙ 是 `element-wise` 乘，即按元素乘
- 为了表示向量和矩阵的元素，我们把时间写在上标。
- 用 $$\delta z^t$$ 表示 $$E^t$$ 对 $$z^t$$ 的偏导
- ⊗ 表示外积，即列向量乘以行向量

下面以 `LSTM` 单节反向传播为例进行推导，各变量参照[反向传播路径](#反向传播路径)。

***0 : $$L --> h_t$$***

设 $$E^t$$ 为 `t` 时刻的误差，首先求其对 `t` 时刻 `LSTM` 输出的偏导：

$$
\frac{\partial E^t}{\partial h^t} = \delta h^t \quad\quad\quad\quad  (1)
$$

***1. $$h^t --> o^t, C^t$$***

<div style="text-align:center">
<img src="/images/LSTM 反向推导 3.png" width="75%">
</div><br>

***2. $$\delta f_t, \delta i_t, \delta \tilde{C}_t, \delta C^{t-1}$$***

<div style="text-align:center">
<img src="/images/LSTM 反向推导 4.png" width="60%">
</div><br>

***3. $$\delta W_f, \delta W_i, \delta W_c, \delta W_o$$***

<div style="text-align:center">
<img src="/images/LSTM 反向推导 5.png" width="80%">
</div><br>

***4. $$\delta h_{t-1}$$***

<div style="text-align:center">
<img src="/images/LSTM 反向推导 6.png" width="90%">
</div><br>



<div style="text-align:center">
<img src="/images/LSTM 反向推导 7.png" width="70%">
</div><br>

***5. 总梯度***

$$
\frac{\partial E}{\partial W} = \sum_{t=0}^{T} \frac{\partial E^t}{\partial W}
$$