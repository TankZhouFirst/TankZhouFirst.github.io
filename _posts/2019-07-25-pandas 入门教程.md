---
layout: post
title:  "pandas 详细入门"
date:   2019-07-25 23:31:01 +0800
categories: 数据分析
tag: Python 第三方库
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**


****

**数据**： [豆瓣电影数据](https://share.weiyun.com/564O0wR)   **密码** : `x38u97`


****

# 引入

##　基本介绍

`Python Data Analysis Library` 是基于 `NumPy` 的一种工具，该工具是为了解决数据分析任务而创建的。 `Pandas` 纳入了大量库和些标准的数据模型，提供了高效地操作大型数据集所需的工具。 `Pandas` 提供了大量能使我们快速便捷地处理数据的函数和方法。

```python
import pandas as pd
```

## 显示设置

```python
#显示所有列
pd.set_option('display.max_columns', None)
#显示所有行
pd.set_option('display.max_rows', None)
#设置value的显示长度为100，默认为50
pd.set_option('max_colwidth',100)
```

## 基本数据结构

`Pandas` 中有两种基本结构：  

**Series**

> 一维数组，与  `Numpy` 中的一维 `array `类似。二者与 `Python` 中的  `list` 接近。`Series` 能保存不同的数据类型，字符串，`boolean` 值，数字等都能保存。

**DataFrame**

> 二维的表格型数据结构。很多功能与 R 中的 `data.frame` 类似。可以将 `DataFrame` 理解为 `Series` 的容器。以下内容以 `DataFrame` 为主。

### Series 类型

#### 创建

可以用一维列表进行初始化。默认情况下，`Series` 的下标均为数字（可以使用额外的参数指定），类型是统一的。

```Python
import pandas as pd
import numpy as np

s1 = pd.Series([1,3,5, np.nan, 6,8])
s1

# 0    1.0
# 1    3.0
# 2    5.0
# 3    NaN
# 4    6.0
# 5    8.0
# dtype: float64
```

如上面的结果所示，左边为索引（即上面的下标），右边为值。默认索引为从 0 开始的数字，可以通过如下方式更改：

```Python
s2 = pd.Series([1,3,5, np.nan, 6,8], index=['a','b','c','d','e','f'])
s2

# a    1.0
# b    3.0
# c    5.0
# d    NaN
# e    6.0
# f    8.0
# dtype: float64
```

#### 访问

##### 索引列表

```pyrthon
s1.index   # RangeIndex(start=0, stop=6, step=1)
```

##### 值列表

```pyrthon
s2.values  # array([ 1.,  3.,  5., nan,  6.,  8.])
```

##### 指定索引的值

```pyrthon
s2[0]    # 1.0
```

##### 切片操作

```python
s2[2:5]
s2[::2]
```

##### 给索引列表命名

```python
# 索引
# 0    1.0
# 1    3.0
# 2    5.0
# 3    NaN
# 4    6.0
# 5    8.0
# dtype: float64
```

##### 替换指定值

可以使用如下方式，将制定值进行替换：

```python
Seri = Seri.replace(origin_value, new_value)
```

上面代码，将会替换 `Seri` 中，所有值 `origin_value` 为 `new_value`。

##### 自定义操作

可以使用 apply 成员方法进行自定义操作(不改变原值)，如下面代码所示：

```python
a = pd.Series([1,3,5, np.nan, 6,8], index=['a','b','c','d','e','f'])
# a    1.0
# b    3.0
# c    5.0
# d    NaN
# e    6.0
# f    8.0
# dtype: float64

a.apply(lambda x:x+1)
# a    2.0
# b    4.0
# c    6.0
# d    NaN
# e    7.0
# f    9.0
# dtype: float64
```

### DataFrame 类型

#### 创建

##### 传入二维数组构造

```Python
# 构造时间序列，作为下标
date = pd.date_range('20180101', periods=6)
# DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
#                '2018-01-05', '2018-01-06'],
#               dtype='datetime64[ns]', freq='D')


# 创建 DataFrame 结构
# 未指定 index，默认为从 0 开始的连续数字
df = pd.DataFrame(np.random.randn(6,4))
#           0         1         2         3
# 0 -1.711927  1.215190 -0.880697  0.078906
# 1 -0.419565  1.009853  0.338155 -3.223133
# 2 -0.375881 -1.453980 -0.322463 -2.396469
# 3 -0.249740  0.438954 -0.313317 -0.003007
# 4 -0.887117 -0.396241  0.170407 -0.096708
# 5 -2.013753 -0.634808 -0.015089 -0.741981
```

##### 使用字典数据构造

字典中的每个 `key` 代表一列，其 `value` 可以是能够转换为 `Series` 的对象。与 `Series` 要求的所有数据的类型一致不同， `DataFrame` 只要求每一列数据的格式相同。

```pyhton
df2 = pd.DataFrame({
    'A':1.0, 
    'B':pd.Timestamp('20181001'),
    'C':pd.Series(1, index=list(range(4)), dtype=float), 
    'D':np.array([3] * 4, dtype=int), 
    'E':pd.Categorical(["test","train","test","train"]), 
    'F':"abc"
})

#      A          B    C  D      E    F
# 0  1.0 2018-10-01  1.0  3   test  abc
# 1  1.0 2018-10-01  1.0  3  train  abc
# 2  1.0 2018-10-01  1.0  3   test  abc
# 3  1.0 2018-10-01  1.0  3  train  abc
```

##### 给行/列指定索引

```python
# 创建 DataFrame 结构
# 使用指定的 index 作为索
df = pd.DataFrame(np.random.randn(6,4), index=date)
#                    0         1         2         3
# 2018-01-01  1.066076 -0.677561  0.154068 -1.390533
# 2018-01-02 -0.544129 -0.428739 -1.036881 -0.116188
# 2018-01-03 -3.253782  0.363127 -0.228255 -0.361673
# 2018-01-04 -0.311910  1.129741  2.149620  0.291126
# 2018-01-05 -1.025816  1.726248 -1.558656 -0.572937
# 2018-01-06 -0.911481 -2.511359  1.079168  0.842394

# 创建 DataFrame 结构
# 指定 index 以及 columns 形式
df = pd.DataFrame(np.random.randn(6,4), index=date, columns=list('ABCD'))
#                    A         B         C         D
# 2018-01-01  0.767461 -1.573637  1.054566  0.043007
# 2018-01-02  0.690353 -1.181627 -0.891142  0.233988
# 2018-01-03  0.006906  0.423511  0.057445  0.571189
# 2018-01-04 -0.311366 -0.098603 -0.068230 -1.102378
# 2018-01-05 -1.077342  1.049928 -0.473674 -1.284919
# 2018-01-06  1.370746 -1.867103  0.645494 -0.092669
```

#### 访问

##### 头尾数据

`head` 和 `tail` 方法可以分别查看前几行和末尾几行的数据（默认为 5）。

```Python
df.head()

df.tail()

# 看最后 3 行
df.tail(3)
```

##### 下标，列标，数据

```Python
# 下标使用 index 查看
df.index

# 列标使用 columns 查看
df.columns

# 数据值使用 values 查看
df.values
```

#### 与 Numpy 相互转换

**df to array**

```python
df_array = df.values
```

**array to df**

```python
df = pd.DataFrame(df)
```

## 基本数据操作

### 文件读写

#### 文件读取

```Python
# 读取 excel 文件
df = pd.read_excel("豆瓣电影数据.xlsx")
df.head(5)

# 读取 csv 文件
# df = pd.read_csv("nasdaq-listings.csv")
# df.head(3)
```

#### 文件写入

```Python
# 写入 excel
df.to_excel("movie_data.xlsx")

# 写入 csv
df.to_csv("movie_data.csv")
```

### 行列操作

#### 行操作

##### 查看指定行

```Python
# 查看指定行
df.iloc[0]

# 查看多行，不包括 index=5 的行
df.iloc[:5]

# 使用 loc，包括边界行
df.loc[:5]
```

##### 添加行

```Python
# 原始数据
dt = pd.Series({
    '名字':'复仇者联盟3',
    '投票人数':123456,
    '类型':'剧情/科幻',
    '产地':'美国',
    '上映时间':'2018-05-04 00:00:00',
    '时长':142,
    '年代':2018,
    '评分':8.7,
    '首映地点':'美国'
})
# pandas 数据
s = pd.Series(dt)

# 原始数据最后几行，最后一行为 38737
df.tail()

# 添加行
s.name = 38738
df = df.append(s)
df.tail()
```

##### 删除行

删除行时，传入的是行的 `index`，并返回新的 `DataFrame`，可以通过条件判断，加上 `.index` 指定要删除的行。

```Python
df = df.drop([38738])
df.tail()

# 删除 coid 为 0 的数据中的前两行
df = df.drop(df[df['coid'] == 0].index[:2])
```

#### 列操作

##### 查看列

```Python
# 查看列名称
df.columns

# 查看某一列
df['投票人数']

# 查看某一列的指定位置元素
df['投票人数'][1:5]

# 查看多列
df[['投票人数', '上映时间']][:5]
```

##### 增加一列

```Python
df['test'] = range(0, len(df))
df
```

##### 删除一列  df.drop

```Python
# axis 指定删除的方向
df = df.drop('test', axis=1)
df
```

##### 调整列的顺序

可以通过如下方式调整列的顺序，同时可以删除列（列表中不包含对应的列即可）。

```python
columns = ['Region', 'District', 'Garden', 'Layout', 'Floor', 'Year', 'Size', 'Elevator', 'Direction', 'Renovation', 'PerPrice', 'Price']
df = pd.DataFrame(df, columns = columns)
```

#### 选定指定数据

##### 索引选择  df.loc

通过这种方式进行的选择，返回时源数据的引用，也就是可以修改原数据。

```python
df.loc[Dataframe.index, [columns]]
```

```Python
# df.loc[[index], [column]]
df.loc[[5], ['上映时间']]

df.loc[[1,3,5,7,9], ['名字', '产地', '评分']]

# 修改指定行指定列的值
df.loc[df[df['coid'] == coid].index, 'high'] = tmp_list
```

##### 条件选择

这种方式返回的是原数据的拷贝，更改之不会改变原数据。

```Python
# 相等判断，得到每个位置的 bool 值
df['产地']=='美国'

# 取出为 True 的数据
df[df['产地']=='美国']

df[(df['产地']=='中国大陆') & (df['评分'] > 9.3)]

df[((df['产地']=='中国大陆') | (df['评分'] > 9.3)) & (df['投票人数'] > 500000)]
```

# 数据清洗

## 查看基本信息

### info

可以通过 `info` 查看基本信息，从而确定缺失值所属属性：

```Python
# 查看数据摘要
df.info()

# <class 'pandas.core.frame.DataFrame'>
# Int64Index: 38738 entries, 0 to 38737
# Data columns (total 9 columns):
# 名字      38178 non-null object
# 投票人数    38738 non-null float64
# 类型      38738 non-null object
# 产地      38738 non-null object
# 上映时间    38736 non-null object
# 时长      38738 non-null object
# 年代      38738 non-null object
# 评分      38738 non-null float64
# 首映地点    38737 non-null object
# dtypes: float64(2), object(7)
# memory usage: 3.0+ MB
```

### 查看描述型统计

`dataframe.describe()` ：对 `dataframe` 中的**数值型数据**进行描述型统计。所以首先需要将数据类型进行转换。

通过描述型统计，往往可以发现一些异常值。很多异常值需要逐步发现。

```Python
df.describe()

# 观察异常数据

# 清洗异常数据

# 查看异常数据
df[df['年代'] > 2018]
df[df['时长'] > 1000]

# 清洗异常数据
df.drop(df[df['年代'] > 2018].index, inplace=True)
df.drop(df[df['年代'] < 1850].index, inplace=True)
df.drop(df[df['时长'] > 1000].index, inplace=True)
df.drop(df[df['投票人数'] < 0].index, inplace=True)

# 重新查看
df.describe()
```

下图是未处理的 `describe` 显示信息，可以发现投票人数有问题。

<div style="text-align:center">
<img src="/images/describe.png" width="50%">
</div>

### 具体属性获取

#### 最值 min / max

```python
# 最值
df['投票人数'].max()
df['投票人数'].min()
```

#### 均值与中位数

```python
# 均值和中值
df['投票人数'].mean()
df['投票人数'].median()   # 更有参考价值
```

#### 方差与标准差

```Python
# 方差和标准差
df['评分'].var()
df['评分'].std()
```

#### 求和

```python
# 求和
df['投票人数'].sum()
```

#### 相关系数和协方差

```python
# 相关系数和协方差
df[['投票人数','评分']].corr()
df[['投票人数','评分']].cov()
```

#### 查看唯一值列表

```python
# 计数
len(df)  # 总数据
df['产地'].unique()  # 查看唯一值
```

### 排序

#### 默认排序

默认按照 `index` 进行排序。

#### 按照指定属性进行排序

```Python
# 默认升序排列
df.sort_values(by='评分')

# 降序排列
df.sort_values(by='评分', ascending=False).head()
```

#### 多值排序

按照次序进行排序，先第一个值，若相同，则第二个值。

```Python
df.sort_values(by=['评分', '投票人数'], ascending=False).head()
```

### 查看总量  value_counts()

```Python
# 查看每一年的电影量
df['年代'].value_counts()[:10]
# 2012    2042
# 2013    2001
# 2008    1963
# 2014    1887
# 2010    1886
# 2011    1866
# 2009    1862
# 2007    1711
# 2015    1592
# 2006    1515
# Name: 年代, dtype: int64

# 查看每个产地的电影数量
df['产地'].value_counts()

# 查看成片质量
result = df[df['评分'] > 8]['产地'].value_counts() / df['产地'].value_counts()
result.sort_values(ascending=False)
```

### 逐行累加 `pd.cumsum()`

通过流量得到存量，比如每天销售量的多少，得到今年的销售量总量。

```python
df = pd.DataFrame()
df['userId'] = np.random.randint(1,4,6)
df['times'] = [1,2,4,2,1,2]

#    userId  times
# 0       2      1
# 1       1      2
# 2       3      4
# 3       2      2
# 4       2      1
# 5       2      2

# 现在要统计各个用户的下载量
df['sumtimes'] = df['times'].groupby(df['userId']).cumsum()

#    userId  times  sumtimes
# 0       2      1         1
# 1       1      2         2
# 2       3      4         4
# 3       2      2         3
# 4       2      1         4
# 5       2      2         6
```

### 逐行累乘 `pd.cumprod()`

累乘是通过变化率来得到存量，比如有每天的数据变动趋势，通过累乘来得到当前的数据。

## 数据透视 (统计特性)

`Excel` 中数据透视表的使用非常广泛，其实 `Pandas` 也提供了一个类似的功能，名为 `pivot_table`。

`pivot_table` 非常有用，我们将重点解释 `pandas` 中的函数 `pivot_table`。

使用 `Pandas` 中的 `pivot_table` 的一个挑战是，你需要确保你理解你的数据，并清楚地知道你想通过透视表解决什么问题，虽然 `Pivot_table` 看起来只是一个简单的函数但是它能够快速地对数据进行强大的分析。

### 设置显示属性

```python
# 设置显示所有行
pd.set_option('max_columns', 100)   # 设置最大显示 100 列
pd.set_option('max_rows', 500)      # 设置最大显示 500 行
```

### 基础形式

首先指定 `df`，然后指定进行透视的列，默认计算均值。将会统计指定属性，并统计该属性每一个值下，其他数值型属性的统计均值。

```Python
pd.pivot_table(df, index=['年代']).head()
```

### 指定多个属性

```Python
# 依次按照年代产地，进行统计
pd.pivot_table(df, index=['年代','产地']).head()
```

<div style="text-align:center">
<img src="/images/数据透视.png" width="40%">
</div>

### 指定要统计的数据

上面是统计某属性下个分量对应的其他所有属性的统计均值，实际上可以指定只统计部分其他分量。

```Python
pd.pivot_table(df, index=['年代','产地'], values=['评分'])
```

### 自定义统计操作

```Python
# aggfunc 指定对数据进行操作的函数
pd.pivot_table(df, index=['年代','产地'], values=['投票人数'], aggfunc=np.sum)

# 通过将 '投票人数' 和 '评分' 列进行对应分组，对 '产地' 实现数据聚合和总结
pd.pivot_table(df, index=['产地'], values=['投票人数', '评分'], aggfunc=[np.sum, np.mean])

# 使用 fill_value 参数，将非数值(NaN) 进行处理
pd.pivot_table(df, index=['产地'], aggfunc=[np.sum, np.mean], fill_value=0)

# margins=True 设置在列表下方显示对所有数据的操作。
pd.pivot_table(df, index=['产地'], aggfunc=[np.sum, np.mean], fill_value=0, margins=True)
```

<div style="text-align:center">
<img src="/images/自定义统计操作.png" width="70%">
</div>

对不同值执行不同函数，可以向 `aggfunc` 传入一个字典，不过副作用就是，必须将标签做的更加简洁。比方说，投票人数的均值无意义，更期望的是投票人数的总和以及评分的均值。

```Python
pd.pivot_table(df, index=['产地'], values=['投票人数','评分'],aggfunc={'投票人数':np.sum, '评分':np.mean}, fill_value=0, margins=True)
```

实际上，对于部分缺失的值，可以设置填充值。

```Python
table = pd.pivot_table(df, index=['年代'], values=['投票人数','评分'], aggfunc={'投票人数':np.sum, '评分':np.mean}, fill_value=0)
```

## 缺失值处理

### 常用方法

| 方法    | 说明                                   |
| ------- | -------------------------------------- |
| dropna  | 根据标签中的缺失值进行过滤，删除缺失值 |
| fillna  | 对缺失值进行填充                       |
| isnull  | 返回布尔值对象，判断哪些值为缺失值     |
| notnull | isnull 的反                            |

### 判断缺失值

```Python
df.isnull()

# 查看 评价人数 缺失的数据
# 条件索引
df[df['名字'].isnull()][:10]
```

<div style="text-align:center">
<img src="/images/判断缺失值.png" width="９0%">
</div>

### 填充缺失值

```Python
# 用均值填充
# 如果不用 inplace 的话，则不会改变原 df
df['评分'].fillna(int(np.mean(df['评分'])), inplace=True)
df[-5:]

# 将会对所有空数据填充为 “未知电影”
df.fillna('未知电影')[-5:]
```

### 删除缺失值

**函数原型：**

```python
df.dropna()  参数：
```

**函数参数：**

> how='all'    # 删除全为空值的行或列    
> inplace=True # 覆盖之前的数据  
> axis=0       # 选择行或列，默认为行 

```Python
# 读取 excel 文件
df = pd.read_excel("豆瓣电影数据.xlsx")
print(len(df))      # 38738
df2 = df.dropna()   # 38175
print(len(df2))
```

### 重建索引

上面删除部分数据之后，对应的索引可能会不连续，需要重新建立索引。

```Python
df.index = range(len(df))
```

## 异常值处理

异常值，即在数据集中存在不合理的值，又称离群点。比如年龄为 `-1`，笔记本电脑重量为吨等，都属于异常值的范围。

对于异常值，一般来说数量都会很少，在不影响整体数据分布的情况下，我们直接删除就可以了。  其他属性的异常值处理，我们会在格式转换部分，进一步讨论。

### 显示异常值

```Python
# 读取 excel 文件
df = pd.read_excel("豆瓣电影数据.xlsx")

df[df.投票人数 < 0]
df[df.投票人数 % 1 != 0]
```

<div style="text-align:center">
<img src="/images/显示异常值.png" width="90%">
</div>

### 删除异常值

```Pyrthon
# 删除异常值。本质上就是保留非异常值
df = df[df['投票人数'] % 1 == 0]
df
```

## 数据类型转换

在做数据分析的时候，原始数据往往会因为各种原因产生各种各样的数据格式问题。而数据格式错误往往会导致很严重的后果。并且，很多异常值只有经过格式转换之后才能发现。所以，对规整数据，数据清洗非常重要。

### 数据类型查看与转换

与 `numpy` 兼容，使用一样的函数。

```Python
# 查看数据格式
df['投票人数'].dtype

# 转换格式，转换为 int
df['投票人数'] = df['投票人数'].astype('int')
df[:5]

df['产地'].dtype
df['产地'] = df['产地'].astype('str')
df
```

### 将年份转换为整数格式

```Python
df['年代'] = df['年代'].astype('int')
# 可能会报错，找出出错原因和数据

# 查找问题数据
df[df['年代'] == '2008\u200e']

df[df['年代'] == '2008\u200e']['年代'].values
# unicode 格式控制字符  \u200e

# 替换出错数据
df.loc[15205, '年代'] = 2008
df['年代'] = df['年代'].astype('int')
df.tail()
```

### 将电影时长转换为整数

```Python
df['时长'] = df['时长'].astype('int')

# 报错，清理

# 删除数据，并更新 df  inplace=True 表示替换原 df
df.drop([19689], inplace=True)

# 报错，清理

df['时长'] = df['时长'].astype('int')
```

## 数据替换

数据替换可用于替换指定值。

```Python
df['产地'].unique()
# array(['美国', '意大利', '中国大陆', '日本', '法国', '英国', '韩国', '中国香港', '阿根廷', '德国',
#        '印度', '其他', '加拿大', '波兰', '泰国', '澳大利亚', '西班牙', '俄罗斯', '中国台湾', '荷兰',
#        '丹麦', '比利时', 'USA', '苏联', '墨西哥', '巴西', '瑞典', '西德'], dtype=object)


df['产地'].replace('USA', '美国', inplace=True)
df['产地'].unique()  # 可以发现 USA 消失了

# 多替换
df['产地'].replace(['西德', '苏联'], ['德国', '俄罗斯'], inplace=True)
df['产地'].unique()  # 可以发现 USA 消失了
# array(['美国', '意大利', '中国大陆', '日本', '法国', '英国', '韩国', '中国香港', '阿根廷', '德国',
#        '印度', '其他', '加拿大', '波兰', '泰国', '澳大利亚', '西班牙', '俄罗斯', '中国台湾', '荷兰',
#        '丹麦', '比利时', '墨西哥', '巴西', '瑞典'], dtype=object)
```

## 相关矩阵

可以通过如下代码，获取相关变量的相关矩阵，以便后续创建热力图等。

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

allDf = pd.DataFrame({
    'x':[0,1,2,4,7,10],
    'y':[0,3,2,4,5,7],
    's':[0,1,2,3,4,5],
    'c':[5,4,3,2,1,0]
},index = ['p1','p2','p3','p4','p5','p6'])

# print(allDf) 

corr_matrix = allDf.corr()

```

## scatter_matrix

```python
# 画任意两列数值属性的散点图，最后画一个散点图的矩阵，对角线为分布直方图。

df = pd.DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])
pd.plotting.scatter_matrix(df, alpha=0.2)

plt.show()
```

<div style="text-align:center">
<img src="/images/scatter_matrix.png" width="80%">
</div>
# 数据重建

## 层次化索引

层次化索引是 `pandas` 的一项重要功能，它能使我们在一个轴上拥有多个索引。

### Series 层次化索引

#### 创建与索引

```Python
s = pd.Series(np.arange(11,20), index=[['a','a','a','b','b','c','c','d','d'],[1,2,3,1,2,3,1,2,3]])
# a  1    11
#    2    12
#    3    13
# b  1    14
#    2    15
# c  3    16
#    1    17
# d  2    18
#    3    19
# dtype: int64

# 包含两层索引
s.index
# MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],
#            labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 2]])

s['a']
s['a':'c']

s[:,1]
# 外层全部，内层取 1：a1,b1,c1 ，分别对应 11，14，17

s['c', 1]
```

#### 使用 unstack 将层次化的 Series 转换为 DataFrame

<div style="text-align:center">
<img src="/images/Series 层次化索引.png" width="90%"/>
</div><br>


```Python
s.unstack()
```

### DataFrame 层次化索引

#### 创建层次化索引

对于 `DataFrame` 而言，行和列均能进行层次化索引。

```Python
data = pd.DataFrame(np.arange(12).reshape(4,3))

data = pd.DataFrame(np.arange(12).reshape(4,3), index=[['a','a','b','b'],[1,2,1,2]])

data = pd.DataFrame(np.arange(12).reshape(4,3), index=[['a','a','b','b'],[1,2,1,2]], columns=[['A','A','B'],['Z','X','C']])
```

<div style="text-align:center">
<img src="/images/DF 创建层次化索引.png" width="25%"/>
</div> 

#### 更改索引名

```
data['A']

# 设置外层索引名称
data.index.names = ['row1', 'row2']
data.columns.names = ['col1', 'col2']
data
```

<div style="text-align:center">
<img src="/images/DF 更该索引名.png" width="30%"/>
</div> 

#### 交换索引顺序

```python
# 切换层级
data = data.swaplevel('row1', 'row2')
```

<div style="text-align:center">
<img src="/images/DF 交换索引层次.png" width="30%"/>
</div>

### stack 和 unstack

`dataframe` 也可以使用 `stack` 和 `unstack` ，转换为层次化索引的 `Series`，可能会产生缺失值。

```Python
data.stack()

data.stack().unstack()
```

### 实例：将电影数据处理层多层索引结构

```Python
df = pd.read_excel('movie_data2.xlsx')
df.head()
```

## 索引与列互换

`set_index` 可以把列变成索引。`reset_index` 可以把索引变成列。

```Python
df = df.set_index(['产地','年代'])
df

# 在上面这种情况下，每一个索引都是一个元组。
df.index[0]

# 获取所有美国电影：由于产地信息已经变为了索引，所以使用 .loc 方法。
df.loc['美国']

# 取消层次化索引
df = df.reset_index()
df
```

## 数据旋转

```Python
# 行列旋转
data = df.head().T
data
```

## 数据分组

`GroupBy` 技术：实现数据分组和分组运算，作用类似于数据透视表，只会对数值变量进行分组运算。如下图所示：

<div style="text-align:center">
<img src="/images/数据分组.png" width="70%"/>
</div><br>


```Python
df = pd.read_excel("豆瓣电影数据.xlsx")

# 按照电影产地进行分组
# 先定义一个分组变量
group = df.groupby(df['产地'])
type(group)

# 计算分组后的各个统计量
group.mean()
group.sum()

# 计算每年平均分数
df['评分'].groupby(df['年代']).mean()

df['年代'] = df['年代'].astype('str')
df.groupby(df['产地']).mean()

# 可以看到，年代为字符串，不参与运算

# 传入多个分组变量
# 将会计算每个产地对应的每年的各个量的均值
df.groupby(['产地','年代']).mean()


获取每个地区每一年的电影评分均值
# 由于这里只要求计算评分的均值，所以先提取评分部分，然后进行分组
means = df['评分'].groupby([df['产地'],df['年代']]).mean()

# 这得到的为 Series 数据
means
```

## 离散化处理

在实际的数据分析项目中，对有的数据属性，我们往往并不关注数据的绝对取值，只关注它所处的区间或者等级。  比如，我们可以把评分 9 分及以上的电影定义为 A，7 到 9 分定义为 B，5 到 7 分定义为 C，3 到 5 分定义为 D，小于分定义为 E。  

离散化也可称为分组、区间化。`Pandas` 为我们提供了方便的函数 `cut()` 和 `qcut()`:

### 等距分割 pd.cut()

等间距分割，即：每个部分的间距是相同的。

**函数原型：**  

```python
pd.cut(x, bins, right＝True, labels＝None, retains＝False,precision=3, include_lowest＝False)
```

**函数参数：**  

```
x:  需要离散化的数组, Series, DataFrameX对象  
bins:            分组的依据。可以是一个数字，此时表示划分成多少等间距的区间。也可以是一个序列，此时按照序列进行划分    
right:           指定是否包括右端点，默认包括  
include_lowest:  指定是否包括左端点，默认不包括  
......
```

**使用实例：**

```Python
# 将评分分到所划定的区域
# 区间为 [(0, 3] < (3, 5] < (5, 7] < (7, 9] < (9, 10]]
# 分别指定对应区间标记为 ''E', 'D', 'C', 'B', 'A'
df['评分等级'] = pd.cut(df['评分'], [0,3,5,7,9,10], labels=['E', 'D', 'C', 'B', 'A'])
df
```

假设投票越多的热门程度越高：

```Python
bins = np.percentile(df['投票人数'], [0,20,40,60,80,100])
bins

df['热门程度'] = pd.cut(df['投票人数'], bins, labels=['E', 'D', 'C', 'B', 'A'])
df

# 热门烂片
df[(df['评分等级'] == 'E') & (df['热门程度'] == 'A')]

# 冷门高分
df[(df['评分等级'] == 'A') & (df['热门程度'] == 'E')]
```

<div style="text-align:center">
<img src="/images/等距分割.png" width="90%"/>
</div> 

### 等频分割 qcut()

使用 `qcut()` 可以实现等频分割：根据频率均匀分割，即：各部分数量相等。

```python
factors = np.random.randn(9)
pd.qcut(factors, 3)  #返回每个数对应的区间

# [(1.525, 2.154], (-0.158, 1.525], (1.525, 2.154], (-2.113, -0.158], (-2.113, -0.158], (1.525, 2.154], (-2.113, -0.158], (-0.158, 1.525], (-0.158, 1.525]]
```

## 合并数据集

### 行方向拼接 append

```Python
# 创建用于合并的数据
df_usa = df[df['产地'] == '美国']
df_china = df[df['产地'] == '中国大陆']

# 上下的拼接
df_china.append(df_usa)
```

<div style="text-align:center">
<img src="/images/列方向拼接.png" width="90%"/>
</div> 

### 列方向属性拼接 merge

用的较多，进行横向的拼接（列方向）

**函数原型：**  

```Python
pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False)
```

**函数参数：**  

> left  : 对象  
> right : 另一个对象  
> on    :  要加入的列的名称，必须同时在两个数据中都存在。  
> left_on  : 若想连接左右不同的键，则 left_on 指定左边的列名  
> right_on : 若想连接左右不同的键，则 right_on 指定右边的列名  
> left_index  :   
> right_index :  
> how  : 指定以什么样的方式进行连接。可默认为 'inner'，即取两者交集；  
> sort : 通过连接键按字典顺序对结果进行排序。  
> suffixes :   
> copy :   
> indicator:  

**使用实例：**

```Python
# 选取 6 部热门电影
df1 = df.loc[:5]
df1

df2 = df.loc[:5][['名字','产地']]
df2['票房'] = [1234,56464,6546,6121,1566,68489]
df2

# 打乱数据
df2 = df2.sample(frac=1)
df2.index = range(len(df2))
df2

# 合并 df1 和 df2，index 已经不匹配
pd.merge(df1,df2, how='inner', on='名字')

# 由于两个数据集均存在产地，因此合并后会有两个产地信息
# 分别为 产地_x 和 产地_y
```

### 批量行合并 concat

可以将多个数据集进行批量合并

```Python
df1 = df[:10]
df2 = df[100:110]
df3 = df[200:210]

pd.concat([df1,df2,df3])
```