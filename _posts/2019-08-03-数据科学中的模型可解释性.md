---
layout: post
title:  "数据科学中的模型可解释性"
date:   2019-08-04 21:37:01 +0800
categories: 数据分析
tag: 数据分析
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

1. [Interpretable Machine Learning](https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b)
2. [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)

****

[TOC]

****

## 模型可解释性

> **理解（interpret）表示用可被认知（understandable）的说法去解释（explain）或呈现（present）。在机器学习的场景中，可解释性（interpretability）就表示模型能够使用人类可认知的说法进行解释和呈现数据和预测。**

<div style="text-align:center">
<img src="/images/可解释性.png" width="70%">
</div><br>

### 洞悉模型

许多人认为机器学习模型是黑盒，因为其可以做出准确地预测，但是无法理解预测背后的逻辑。对于大多数数据科学而言，这是对的，因为我们无法从模型中抽象洞察（`insights`）。

本文将带您学习如何使用一些工具，来从复杂的机器学习模型中，抽象出如下洞察：

1. 模型认为数据中的哪些特征是重要的？
2. 在每次模型预测中，数据中的每个特征是如何影响具体的预测的？
3. 从统计学角度讲，在大量预测中，每一个特征对预测结果的典型影响？

### 模型可解释性的好处

对模型的这些洞察力，具有诸多好处：

1. 易于调试
2. 启发特征工程进行
3. 指导未来数据收集
4. 启发人为决策
5. 建立信任

#### 调试

在实际项目中，通常会出现大量的不可靠的，不规整的垃圾数据。在进行相关数据预处理时，需要思考潜在的错误源。要考虑到数据泄露（`target leakage`）的可能性，因为在某些数据实践中，某些点存在问题是很常见的事！

鉴于数据科学中，`bug` 经常出现，且后果是灾难性的，因此，调试是数据科学中最具价值的技能之一。

理解模型所匹配的模式，将帮助我们确定，什么时候这些模式与现实世界是有所区别的，这通常是追溯 `bug` 的第一步。

#### 启发特征工程

特征工程通常是改善模型精度的最有效的方式。特征工程就是不断利用原始特征以及之前合成的特征，通过某种转换，来创建新的特征。

有时候，我们只需要通过对问题的直觉，就可以完成这一过程。但是，若原始特征较多，或者缺乏背景知识时，还是需要某些指引的。

#### 指导未来数据收集

我们无法掌控从网上下载的数据集。但是许多使用数据科学的商业或组织有能力去扩充他们想要的数据类型。

收集新类型的数据代价昂贵且极不方便，因此他们只想收集有价值的数据。基于模型的洞察可以让我们更好的理解当前拥有的特征的价值，这将帮助我们解释哪些新的数据最有价值。

####启发人为决策

一些决策通常由模型自动给出。比方说访问 `Amazon` 时，并无人工决定要对你展示什么样的商品。

但是许多重要的决策需要有人类来给出。因此，洞察往往比预测更有价值。

#### 建立信任

在没有进行一些验证的情况下，在做重要决策的时候，许多人并不会相信模型。因此，展示符合人们对于问题的一般认知的洞察，将帮助建立人们对模型的信任，甚至是对于对数据科学并无深刻了解的人。

## permutation importance

### 介绍

特征对模型预测结果的影响程度，我们称之为特征重要性（`feature importance`）。有多种方式可以求解特征重要性，本节介绍其中的一种：`permutation importance`。其具有以下特性：

1. 计算较快
2. 广泛使用，易于理解
3. 与我们想要的特征重要性度量所需具备的特性一致

### 工作原理

假设我们现在要建立模型，预测一个人在 `20` 岁时候的身高，但是只能使用 `10` 时候的数据。数据中可能包含一些有用的特征，比如说 `10` 岁时候到的身高，以及一些无用特征，比如说股票持有数，还有一些不知影响的特征。

我们现在想知道，哪些特征对模型的预测有较大影响？

`permutation importance` 工作方式如下：

1. 准备一个针对当前数据训练好的模型，以及一些验证数据。
2. 对验证数据的（一行表示一个样本）某一列（某特征）进行 `shuffle`，然后以此作为模型的输入，得到预测结果。如果与原始值相比，精度变化较多，则说明该特征对模型结果影响加大；若精度变化很小，则该特征对模型结果影响较小。
3. 还原数据，对每个特征，重复进行步骤 `2`，依次得到每个特征的重要性

### 代码实现

以下代码，以 `sklearn` 框架为基础，进行的实验，因为其抽象层次较高，可直接调用 `API` 即可。不同框架下，实现源码不同，但核心思想不变。模型在 `kaggle` 上提供的 `kernel` 进行的，没有本地数据。

本实验将使用一个模型，基于球队的统计信息，来预测该球队内谁会获得足球先生的称号，模型直接使用随机森林模型。

我们使用 `ELI5` 库可以进行 `Permutation Importance` 的计算。`ELI5` 是一个可以对各类机器学习模型进行可视化和调试 `Python` 库，并且针对各类模型都有统一的调用接口。`ELI5` 中原生支持了多种机器学习框架，并且也提供了解释黑盒模型的方式。

```python
# 定义并训练模型
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data_path = '../input/FIFA 2018 Statistics.csv'

data = pd.read_csv(data_path)
data.head()

y = (data['Man of the Match'] == 'Yes')
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)

# 进行 permutation importance 分析
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X,val_y)
eli5.show_weights(perm, feature_names=val_X.columns.tolist())
```

### 结果分析

上面的代码，结果如下所示：

<div style="text-align:center">
<img src="/images/permutation importance.png" width="60%">
<p>permutation importance</p>
</div><br>

- 靠近上方的绿色特征，表示对模型预测较为重要的特征
- 为了排除随机性，每一次 `shuffle` 都会进行多次，然后取结果的均值和标准差
- 部分特征出现负值，表示其 `shuffle` 之后，对精度反而有所提升。这通常出现在特征不那么重要的时候。当数据集较小的时候，这种情况更为常见。

## Partial Dependence Plots

### 介绍

前面通过 `permutation importance` 展示了，什么特征对预测精度影响最大，而本节将介绍 `partial dependence plots` (`PDD` 或 `PD`)，其表示一个特征如何影响预测。

`PDP` 会展示一个或两个特征对于模型预测的边际效益。`PDP` 可以展示一个特征是如何影响预测的。与此同时，我们可以通过绘制特征和预测目标之间的一维关系图或二维关系图来了解特征与目标之间的关系。

### 工作原理

与 `permutation importance` 相似，`Partial Dependence Plots` 也是在训练好的模型上进行的。

仍以球队的数据为例，各个球员之间可能存在各种差异，比如传球数目，射门次数，进球次数等。乍看之下，似乎不能决断哪个特征影响更大。

我们以一个球员的数据为例，通过不断改变某一个特征的值，来看预测结果随之变化的情况。为了避免随机性，我们会对多个样本进行重复试验。最后绘制出该特征对模型预测的影响的平均情况。

### 实例代码

下面仍旧用 `eli5` 库，并以决策树模型为例，查看特征 `Goal Scored` 是如何影响模型的预测的。

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier


data = pd.read_csv('../input/FIFA 2018 Statistics.csv')

y = (data['Man of the Match'] == 'Yes')
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]

train_X, val_X, train_y, val_y = train_test_split(X,y, random_state=1)
tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)


from matplotlib import pyplot as plt
from pdpbox import pdp, get_dataset, info_plots

pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')

pdp.pdp_plot(pdp_goals, 'Goal Scored')
plt.show()
```

### 结果分析

结果如下所示：

<div style="text-align:center">
<img src="/images/Partial Dependence Plots.png" width="95%">
<p>Partial Dependence Plots</p>
</div><br>

如上图所示：

1. `y` 轴表示预测相较于基准值或最左侧值的增量
2. 蓝色区域表示置信区间
3. 从上图对进球数的 `PDP` 分析看，随着进球数的增多，球员获得足球先生的概率会逐步增加。但是，到达一定阈值之后，该特征的影响就饱和了

### 与模型的关系

现在来看另一个特征的影响。

下面使用的是决策树模型，进行绘制 `PDP` 曲线。

```python
feature_to_plot = 'Distance Covered (Kms)'
pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)

pdp.pdp_plot(pdp_dist, feature_to_plot)
plt.show()
```

结果如下所示：

<div style="text-align:center">
<img src="/images/Distance Covered (Kms) for decisionTree.png" width="95%">
<p>Distance Covered (Kms) for decisionTree</p>
</div><br>

如上图所示，结果似乎较为简单，这是因为模型本身就如此简单，因此无法表现出数据本该有的性质。如下换成随机森林模型，再进行尝试。

```python
# Build Random Forest model
rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)

pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)

pdp.pdp_plot(pdp_dist, feature_to_plot)
plt.show()
```

结果如下所示：

<div style="text-align:center">
<img src="/images/Distance Covered (Kms) for randomForest.png" width="95%">
<p>Distance Covered (Kms) for randomForest</p>
</div><br>

如上所示，随机森林模型就能较好地反映数据的的特性了。从图中可以发现，球员跑动距离达到 `100km` 时，赢得足球先生的概率较大，而更长的距离，则反而会降低获胜概率。

### 二维 PDP

除了一维的 `PDP` 之外，还可以使用二维的 `PDP` 来表示特征之间的影响。

下面仍以决策树模型为例，绘制特征 `Goal Scored` 和 `Distance Covered (Kms)` 之间的相互影响。

```python
eatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']
inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)
plt.show()
```

结果如下所示：

<div style="text-align:center">
<img src="/images/二维 PDP.png" width="95%">
<p>二维 PDP</p>
</div><br>

从上图可以看出，当一个球员至少得一分，并且跑动距离接近 `100km` 时，获胜的概率最大。但是若得分为 `0`，则跑动总距离将变得无关紧要。

## shap-values

上面的两节，都是介绍如何分辨哪些特征较为重要，以及特征是如何影响模型的预测的。下面，我们将详解，如何对模型的推理过程会进行拆解，使之不再那么黑盒化。

### 介绍

`SHAP`（`SHapley Additive exPlanation`）有助于拆解预测过程，从而显示每个特征的影响。其基于 `Shapley values` ，这是一种博弈论的技术。该值用于确定协作游戏中，每个玩家促成游戏成功的贡献值。

通常情况下，在准确性和可解释性之间取得正确的权衡可能是一个困难的平衡行为，但 `SHAP` 值可以同时提供这两者。

`SHAP` 可以拆解每一次预测，**来展示每个特征的影响**。比如如下情况：

1. 一个模型告知银行，不应该贷款给某人，为该银行解释每一次拒绝贷款的原因（依据什么特征做出的决策）
2. 一个医护人员想要判定，是什么因素导致病人患病的风险，从而他们可以直接给出有针对性的健康建议，来解决这些风险因素

### 工作原理

详细原理参考论文 [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)。其基本原理为：

> 计算一个特征加入到模型时的边际贡献，然后考虑到该特征在所有的特征序列的情况下不同的边际贡献，取均值，即某该特征的 `SHAP baseline value`。

`SHAP` 值解释了，对于一个指定特征，相较于使用一些基准值，给其一个指定值时，对预测结果的影响。

仍以足球先生为例。假设我们想知道，某球员取得 `3` 个得分时，而不是一些基准值时，对模型预测结果的影响。对于其他特征的处理过程，也是如此。

`SHAP` 以一种相当优雅的的方式，处理这一过程，并具有一些不错的特性。例如，可以用如下公式，来分解每一侧预测：
$$
sum(\text{SHAP values for all features}) = \text{pred_for_team} - \text{pred_for_baseline_values}
$$
也就是说，所有特征的 `SHAP` 值进行汇总，来解释为什么我的预测与基准值不同。

> **其实不太明白具体原理，没看懂，英文渣渣。但是下文的代码可以看出结果合作用。**

### 源码实现

要实现 `SHAP` 相关功能。我们使用强大的 `Shap` 库来计算 `SHAP` 值。具体代码如下所示，相关讲解见注释部分。

```python
# -------------------------- 训练模型 --------------------------
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/FIFA 2018 Statistics.csv')

y = (data['Man of the Match'] == 'Yes')
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)


# -------------------------- 进行 SHAP 分析 --------------------------
# 首先查看原始输出
row_to_show = 5
data_for_prediction = val_X.iloc[row_to_show]# use 1 row of data here. Could use multiple rows if desired
print(data_for_prediction.shape)
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)
my_model.predict_proba(data_for_prediction_array) # [0.3, 0.7] 有 70% 的概率获胜

# 查看 SHAP 分析
import shap

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# Calculate Shap values
# 其中，shap_values 对象是一个有两个数组元素的列表。
# 第一个数组为一个负输出（loss the game）的 SHAP 值列表，
# 另一个是正输出（win the game）的 SHAP 列表。
# 两者完全一样，只不过对应元素符号恰好相反
shap_values = explainer.shap_values(data_for_prediction)

shap.initjs()
# 我们通常以 positive 输出的形式来考虑预测值，因此我们提取正输出的那一个列表。
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)
```

### 结果分析

上面代码结果如下所示:

<div style="text-align:center">
<img src="/images/SHAP 分析.png" width="98%">
<p>SHAP 值分析</p>
</div><br>

如上图所示，我们的基准值为 `0.4979`，而预测值为 `0.7`。上面预测值的增量为红色，预测值的衰减量为蓝色。从图中可以看出，对模型推理结果增加最多的特征为 `Goal Scored`，而 `Ball Possession` 这一特征，反而会对模型的准确度造成不好的影响。

根据这一分析，我们就可以看出，哪些特征对模型具有怎么样的影响，将模型预测从基准值的结果，推向最终结果。

此外，`SHAP` 可用于任何模型。

## Advanced uses of shap values

### 介绍

`Permutation importance` 很强大，因为它创造了简单的数值度量来查看哪些特征对模型影响较大。这将帮助我们很容易的在特征之间做出比较，并且可以将比较结果展示给非技术人员。

为了了解模型中哪些特性最重要，我们可以为每个示例绘制每个特性的 `SHAP values`。摘要图说明哪些特性是最重要的，以及它们对数据集的影响范围。  

但是，其并不会告诉你，每个特征对结果如何产生影响。比方说，一个特征具有中等的 `permutation importance`，这意味着其：
1. 对于某些预测有较大影响，但是并非所有（或者）
2. 对所有的预测都具有中等影响

此时，`SHAP summary plot` 就派上用场了，它将对每个特征的重要性，创建俯瞰图。

### 代码实现

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import  train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/FIFA 2018 Statistics.csv')

y = (data['Man of the Match'] == "Yes")
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)

import shap

explainer = shap.TreeExplainer(my_model)
shap_values = explainer.shap_values(val_X)
shap.summary_plot(shap_values[1], val_X)
```

### 结果分析

<div style="text-align:center">
<img src="/images/Advanced uses of shap values.png" width="98%">
<p>Advanced uses of shap values</p>
</div><br>

如上图所示，对于每个点：

1. 垂直位置显示了它所描述的特征
2. 颜色显示数据集中这一行的特征值是高还是低
3. 水平位置显示该值的影响是导致较高的预测还是较低的预测

例如，最左上角蓝色的点，表示进球数较少，这将导致该球员的获胜概率降低 `0.25`。此外，还有：

1. 这个模型几乎忽略了 `Red and Yellow` 和 `Red` 这两个特征
2. `Yellow Card` 这个特征，通常不会有大的影响，但是也有几个极端的例子大幅降低了预测值
3. `Goal Scored` 这个特征，大体上和预测值有正相关性

需要注意的是，`SHAP` 计算过程较慢，尤其是对具有一定规模的数据集而言时，需要格外小心。但是，对于 `XGboost` 模型而言，`SHAP` 对其进行了一定的定制优化。

###  SHAP Dependence Contribution Plots

#### 介绍

使用 `Partial Dependence Plots` 来查看单一特征如何影响预测，但是其无法展示所有信息。例如：影响的分布如何？特征具有一个特定值的影响，是较为固定的，还是说会随着其他特征的值而有较大变化？`SHAP` 提供了一个与 `PDP` 相似的 `insight`，但是加入了大量的更多的细节。

<div style="text-align:center">
<img src="/images/Ball Possession 分布.png" width="80%">
<p>Ball Possession 影响分布</p>
</div><br>

如上图所示，从大体图像上的斜坡形状可以看出控球率越高就会给预测值带来越多正效益。我们再来看下图中标记出来的两个点，有几乎相同的控球率，但是 `SHAP value` 却差距很大。这里看不出有什么不同，可能是因为其他特征的影响。

而下面这个图中，可以看到，尽管控球率很高，但是由于进球数不够，所以也是预测值会降低，说明当一个球员长期持球而不进球，那么也是属于表现不好。

<div style="text-align:center">
<img src="/images/Ball Possession 分布 2.png" width="80%">
<p>Ball Possession 影响分布 2</p>
</div><br>

#### 实现源码

```python
import shap

explainer = shap.TreeExplainer(my_model)
shap_values = explainer.shap_values(X)
shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index="Goal Scored")
```

如果不提供参数 `interaction_index`，`Shapley` 将使用一些逻辑，来采样可能感兴趣的点。