---
layout: post
title:  "CNN 基础网路层"
date:   2020-05-07 08:29:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}

****

**参考**

- [刷脸背后，卷积神经网络的数学原理原来是这样的](<https://mp.weixin.qq.com/s/CYJZ7KGOFyRbjIFkqtAFJQ>)
- [如何通俗地理解卷积](<https://www.matongxue.com/madocs/32.html>)
- [CNN 中卷积层的计算细节](<https://zhuanlan.zhihu.com/p/29119239>)
- [Depthwise 卷积与 Pointwise 卷积](<https://blog.csdn.net/tintinetmilou/article/details/81607721>)

****

# LeNet-5

>   **卷积神经网络的核心思想是：局部感受野，权值共享以及时间或空间亚采样这三种思想结合起来，获得了某种程度的位移、尺度、形变不变性。**

如下所示，是一个典型的神经网络 `LetNet-5`，用于手写数字识别。

<div style="text-align:center">
<img src="/images/LeNet-5.png" width="98%">
</div><br>

# 全连接层

## 全连接层的定义

全连接指的是每个神经元与所有输入节点之间均有连接。

<div style="text-align:center">
<img src="/images/全连接层.png" width="60%">
</div><br>

## 全连接层的缺点

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

### 参数数量太多

考虑一个输入 `1000 *1000` 像素的图片(一百万像素，现在已经不能算大图了)，输入层有 `1000 * 1000 = 100 万` 节点。假设第一个隐藏层有 `100` 个节点(这个数量并不多)，那么仅这一层就有 `(1000 * 1000 + 1 ) * 100 = 1 亿` 参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。

### 没有利用像素之间的位置信息

对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。

当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。

### 网络层数限制

我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过 `3` 层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

# 卷积层 — 提取特征

## 卷积运算过程

### 数学原理

二维卷积的运算公式如下所示：

$$
G[m, n]=(f * h)[m, n]=\sum_{j} \sum_{k} h[j, k] f[m-j, n-k]
$$


### 二维卷积

如下图所示，为二维卷积的运算过程：利用卷积核（二维），分别对每层输入特征的同一感受野进行卷积，然后所有输入通道求和，得到输出通道对应位置的值。

<div style="text-align:center">
<img src="/images/卷积运算过程.gif" width="70%">
</div>


### 三维卷积

实际使用中，可能存在三维卷积，即：卷积核为三维的。如下所示：

<div style="text-align:center">
<img src="/images/多输入通道卷积.png" width="70%">
</div><br>

在三维卷积中，运算方式与二维卷积别无二致：元素分别相乘相加，得到对应位置的输出值。只不过，在多出的维度上，可能也要进行滑动，最后每一个卷积核的输出为 `3` 维的特征图。

### 计算细节

下面是二维卷积的运算细节。

<div style="text-align:center">
<img src="/images/卷积运算细节.png" width="90%">
</div><br>

各部分的尺寸如下所示：

- **输入矩阵**：$$[N, H_{in}, W_{in}, C_{in}]$$
- **输出矩阵**：$$[N, H_{out}, W_{out}, C_{out}]$$
- **权重矩阵(卷积核)**：$$[C_{in}, C_{out}, H_{K}, H_{k}]$$

### 卷积实例

如下所示，是利用卷积核提取图像的边缘特征的实例：

<div style="text-align:center">
<img src="/images/用卷积核寻找边缘.gif" width="70%">
</div><br>

## 卷积层的核心思想

<div style="text-align:center">
<img src="/images/全连接 VS 卷积层.png" width="80%">
</div>

### 局部连接  Sparse Connectivity

#### 图像的局部相关性

由于图像具有**空间相关性**，局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因此，神经元没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。

#### 局部连接

由于图像的局部相关性，因此，神经元只需要关注图像的局部区域即可，这个区域就是**感受野**（`local field`）。

如上图所示，每个光束即为一个感受野。通过局部连接，可以大幅度减少连接数目，即：参数数目。因此，局部连接又称为**稀疏连接**。

### 权值共享  Shared Weights

#### 定义

虽然采用局部连接可以减少每个神经元的连接数，但是仍然比较庞大。此时可以采用权值共享：一组连接可以共享同一组权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。

#### 特征提取

把每个卷积核 （`kernel`） 看成是一种提取特征的方式，该方式与位置无关。也就是说，图像上的内容不受位置影响。因为图像的一部分的统计特性与其他部分是一样的。

这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。比如说边缘特征。

但是，一组权值参数只能提取一种特征，因此，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为 `Feature Map`。

## Padding

### 纯卷积的不足

用上面的卷积运算取提取特征时，有两个明显的缺陷。

1. 每次提取特征后，图像就会减小。因此，当提取较多特征时（深层网络中），可能图像就变得不能使用了。
2. 图像的边缘像素只参与过依次卷积运算，而非边缘的特征会多次参与运算，因此，这样可能会丢失边缘信息。

### Padding 的含义及作用

因此，弥补的措施就是，在原图周围围上一层 `0` 值，这样经过卷积运算后，图像仍保持原来大小，并且边缘信息也得到了充分提取。

## 步长 stride

带步长的卷积在卷积神经网络中很常用。在上面的卷积运算中，每次卷积核移动一格，步长为 `1`。实际应用中，可以一次移动两格等，加快运算。

设置步长是一种调整输入张量维数的方法。降维可以减少所需的计算量，并可以避免创建一个完全重叠的感受野。

## Separable Convolution

**Depthwise(DW)** 卷积与 **Pointwise(PW)** 卷积，合起来被称作 **Depthwise Separable Convolution** (参见 `Google` 的 [**Xception**](<http://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf>))，该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些**轻量级网络**中会碰到这种结构如 [**MobileNet**](<https://arxiv.org/pdf/1704.04861.pdf>)。

### 常规卷积运算

上面讲到的卷积运算都是常规卷积运算，其卷积核的通道数与输入特征的通道数一致，对应通道分别卷积并求和。如下所示：

<div style="text-align:center">
<img src="/images/常规卷积运算.png" width="70%">
</div><br>

### Depthwise Separable Convolution

`Depthwise Separable Convolution` 是将一个完整的卷积运算分解为两步进行，即 `Depthwise Convolution` 与 `Pointwise Convolution`。

#### Depthwise Convolution

在 **Depthwise Convolution** 中，每个卷积核只有一层通道，负责输入特征的一个通道的卷积运算，因此输出通道数与输入通道数一致。如下所示：

<div style="text-align:center">
<img src="/images/Depthwise Convolution.png" width="70%">
</div><br>

这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的 `feature` 信息。因此需要 **Pointwise Convolution** 来将这些 **Feature map** 进行组合生成新的 **Feature map**。

#### Pointwise Convolution

**Pointwise Convolution** 的运算与常规卷积运算非常相似，它的卷积核的尺寸为 $$1×1×M$$，$$M$$ 为上一层的通道数。所以这里的卷积运算会将上一步的 `map` 在深度方向上进行加权组合，生成新的 `Feature map`。有几个卷积核就有几个输出 `Feature map`。如下图所示：

<div style="text-align:center">
<img src="/images/Pointwise Convolution.png" width="70%">
</div><br>

### 参数对比

常规运算方式下，总参数数目为：

$$
4 * 3 * 3 * 3 = 108
$$

`DS` 方式下，参数总数为：

$$
3 * 3 * 3 + 1 * 1 * 3 = 39
$$

相同的输入，同样是得到 `4` 张 `Feature map`，**Separable Convolution** 的参数个数是常规卷积的约 $$\frac{1}{3}$$。因此，在参数量相同的前提下，采用 **Separable Convolution** 的神经网络层数可以做的更深。

# 池化层 — 特征抽象

除了用卷积层提取特征外，卷积神经网络还经常用池化层（`pooling layers`）来减少展示量，从而提高运算速度，并增强了某些特征的检测功能。

池化层可以减少过拟合，并减小输入的尺寸来提高性能，提高泛化能力。除此之外，池化层还可以提升感受野尺寸，增加平移不变性。

与卷积针对所有输入通道不同，池化是分别对每一个输入通道进行的。但是，现在大多使用步长大于 `1` 的卷积层来代替池化。

关于池化层的作用，这个[视频](https://www.zhihu.com/question/36686900/answer/483713218)可以看一下。

## 最大池化 max pooling

最大池化运算方式如下所示：

<div style="text-align:center">
<img src="/images/max pooling.png" width="70%">
</div><br>

最大池化法的机制就是，如果检测到了特征，就保留特征的最大值，如果没有检测到特征，则该区域所有值都相对较小。池化层中有两个超参数，但是没有要学习的参数。

## 均值池化 Average pooling

除了最大池化外，还有均值池化。但是最大池化最常用。均值池化的计算方式如其字面含义。

## 池化层的反向传播

<div style="text-align:center">
<img src="/images/池化层的反向传播.gif" width="70%">
</div><br>

