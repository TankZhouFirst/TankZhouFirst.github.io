---
layout: post
title:  "PReLU"
date:   2019-06-30 14:44:01 +0800
categories: 人工智能
tag: 图像分类
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**


****

**参考 :**

- **Paper :** [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)

****

# 引入

## 行业背景

1. 行业内，主要从两个方面提升模型的性能：更强大的模型，以及更高效的抗过拟合策略
2. `ReLU` 是主流的激活函数
3. 使用 `Xavier` 初始化的模型较难收敛

## 主要贡献

1. 提出 `Parametric Rectified Linear Unit`，即 `PReLU`，其对 `ReLU` 进行了改进推广。在几乎不增加计算量的前提下，有效的改善了模型的过拟合问题。收敛更快，误差更低。
2. 提出一种更加稳健的初始化方式，其充分考虑到了整流单元的非线性。这种方法使得我们可以直接从零开始训练更深的网络
3. 成就：基于 `PReLU` 的网络（`PReLU-nets`），在 `ImageNet-2012` 分类数据集上取得了 `4.94%` 的 `top-5` 误差。首次在计算机视觉识别任务上，超越人类水平！

# 主要内容

## PReLU

### 定义

`PReLU` 的图像如下所示：

<div style="text-align:center">
<img src="/images/ReLU vs. PReLU.png" width="65%"/>
<p>图 1：ReLU vs. PReLU</p>
</div><br>


`PReLU` 的计算方式如下所示：

$$f\left(y_{i}\right)=\left\{\begin{array}{ll}{y_{i},} & {\text { if } y_{i}>0} \\ {a_{i} y_{i},} & {\text { if } y_{i} \leq 0}\end{array}\right. \quad\quad\text{(1)}$$

其中，$$y_i$$ 为非线性激活函数在第 $$i$$ 个通道的输入；$$a_i$$ 为激活函数负向的斜率。

> 对于每一个通道，都有一个可学习参数来控制斜率。

上面的计算又可以表示为：

$$f\left(y_{i}\right)=\max \left(0, y_{i}\right)+a_{i} \min \left(0, y_{i}\right)$$

### 参数更新

反向传播阶段，某一层中，`loss` 对 $$a_i$$ 的梯度为：

$$\frac{\partial \mathcal{E}}{\partial a_{i}}=\sum_{y_{i}} \frac{\partial \mathcal{E}}{\partial f\left(y_{i}\right)} \frac{\partial f\left(y_{i}\right)}{\partial a_{i}}   \quad\quad\text{(2)}$$

其中，$$\mathcal{E}$$ 为目标函数，$$\frac{\partial \mathcal{E}}{\partial f\left(y_{i}\right)}$$ 为后续层传回的梯度，而：

$$\frac{\partial f\left(y_{i}\right)}{\partial a_{i}}=\left\{\begin{array}{ll}{0,} & {\text { if } y_{i}>0} \\ {y_{i},} & {\text { if } y_{i} \leq 0}\end{array}\right.   \quad\quad\text{(3)}$$

其中，$$\sum_{y_i}$$ 是在当前层的第 $$i$$ 个通道的特征图中，所有像素点上进行求和的。

在更新参数 $$a_i$$ 时，我们使用动量的方法：

$$\Delta a_{i} :=\mu \Delta a_{i}+\epsilon \frac{\partial \mathcal{E}}{\partial a_{i}}   \quad\quad\text{(4)}$$

其中，$$\mu$$ 为动量系数，$$\epsilon$$ 为学习速率。我们并未使用权值衰减（`L2` 正则），因为其将使得 $$a_i$$ 趋于 `0`。此外，我们设置初始时刻所有的 $$a_i = 0.25$$。

### 对比实验

我们使用一个 `14` 层的网络进行试验，网络结构参数如下表所示：

<div style="text-align:center">
<img src="/images/一个小而深的网络.png" width="60%"/>
<p>表 1：一个小而深的14 层网络</p>
</div><br>

如上表所示，有两个有意思的地方：

1. 第一层卷积层对应的 `PReLU` 学习后的参数远大于 `0`，但之后都接近于 `0.1`。由于第一层卷积层作为图像边缘特征提取器，这就表明，**网络同时兼顾了正负响应，有效的提取了低层次信息**
2. 对于 `channel-wise` 的版本，网络的深层通常参数越小。这表明，网络更倾向于在早些层保存更多信息，而在更深层更加可辨别

在上面的网络中，所有卷积层和前两层全连接层上加入 `ReLU`，就得到了基准网络。将 `ReLU` 换成 `PReLU`，即为我们的网络。实验对比结果如下：

<div style="text-align:center">
<img src="/images/对比实验结果.png" width="60%"/>
<p>表 2：不同激活函数实验结果对比</p>
</div><br>


## 为 PReLU 初始化权值参数

### 前人的努力

尽管使用 `PReLU` 更易于训练，但是较差的初始化仍旧不利于高度非线性系统的训练。针对这一问题，我们提出了一种更加稳健的初始化方法，来强化模型的训练。

目前大多数的网络将参数初始化为服从固定标准差（通常为 `0.01`）和零均值的高斯分布的值。对于深度网络，这么做可能会较难收敛。

也有人提出用一个合适范围的均匀分布来初始化参数，即 `Xavier`。但是这种初始化是建立在激活函数为线性的假设上，而对于 `ReLU` 和 `PReLU` 而言，显然不成立。

基于此，我们在初始化时考虑到 `ReLU` 和 `PReLU`，提出新的初始化方式，从而可以训练极深的网络。

### 前向传播

> 核心思想是考虑每一层所有响应的方差。

对于卷积层，每个卷积窗口的响应为：

$$\mathbf{y}_{l}=\mathbf{W}_{l} \mathbf{x}_{l}+\mathbf{b}_{l}  \quad\quad\text{(5)}$$

其中，输入通道数为 $$c$$，卷积核尺寸为 $$k \times k$$，因此每个卷积核的参数数量为 $$n = k^2c$$，输出通道数为 $$d$$，$$y_l$$ 表示第 $$l$$ 个输出特征图。若忽略 `b`，则有：

$$Var([y_i]) = \sum_{i = 1}^{k^2c}w_il_i$$

我们假设参数 $$ W_l $$ 内的内各元素相互独立，且服从相同分布。$$ \mathbf{x_l} $$ 亦是如此。$$\mathbf{W_l}$$ 和 $$\mathbf{x_l} $$ 之间相互独立。因此有：

$$\operatorname{Var}\left[y_{l}\right]=n_{l} \operatorname{Var}\left[w_{l} x_{l}\right]   \quad\quad\text{(6)}$$

其中，$$y_l, x_l, w_l$$ 分别表示 $$\mathbf{y_l}, \mathbf{W_l}, \mathbf{x_l}$$ 内的每个元素的随机变量。我们使得 $$w_l$$ 均值为 `0`。因此：

$$\begin{aligned}
Var[y_l] &= n_l(E([(w_l x_l)^2]) - E[w_lx_l]^2) \\
&= n_l(E[w_l^2]E[x_l^2] - E[x_l]^2E[w_l]^2) \\
&= n_l(E[w_l^2]E[x_l^2]) \\
&= n_l((E[w_l^2] - E[w_l]^2)E[x_l^2])
\end{aligned}$$

所以有：

$$\operatorname{Var}\left[y_{l}\right]=n_{l} \operatorname{Var}\left[w_{l}\right] E\left[x_{l}^{2}\right]   \quad\quad\text{(7)}$$

其中，$$ E\left[x_{l}^{2}\right] \neq Var[x_l]$$，除非 $$x_l$$ 均值为 0。对于 `ReLU` 激活函数，$$x_{l}=\max \left(0, y_{l-1}\right)$$，因此其均值不为 0。

如果让 $$w_{l-1}$$ 在 `0` 附近对称分布，且 $$b_{l - 1} = 0$$，那么 $$y_{l - 1}$$ 均值为 `0`，且在 `0` 附近对称分布（$$E(XY) = E(X)E(Y)$$）。

在使用 `ReLU` 时，由于负向值为 `0`，因此：$$E\left[x_{l}^{2}\right]=\frac{1}{2} \operatorname{Var}\left[y_{l-1}\right]$$。结合上式，可得：

$$\operatorname{Var}\left[y_{l}\right]=\frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right] \operatorname{Var}\left[y_{l-1}\right]   \quad\quad\text{(8)}$$

则从第一层传递到第 `L` 层，有：

$$\operatorname{Var}\left[y_{L}\right]=\operatorname{Var}\left[y_{1}\right]\left(\prod_{l=2}^{L} \frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right]\right)   \quad\quad\text{(9)}$$

其乘积是初始化设计的关键。一个合适的初始化方法，应该避免以指数形式改变输入信号。因此，我们期望上面的乘积能够得到一个合适的标量，比如说 `1`。因此，令每一层下：

$$\frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right]=1, \quad \forall l   \quad\quad\text{(10)}$$

这将导致一个 `0` 均值的高斯分布，其标准差为 $$\sqrt{2/n_l}$$，这也正是我们对参数进行初始化的方式。我们同样将偏置值初始化为 `0`。

对于第一层，令 $$n_{1} \operatorname{Var}\left[w_{1}\right]=1$$，因为输入信号上，不存在 `ReLU` 的作用。由于只有一层，所以系数 $$1/2$$ 无关紧要。

### 反向传播阶段

在反向传播阶段，卷积层的梯度为：

$$\Delta \mathbf{x}_{l}=\hat{\mathbf{W}}_{l} \Delta \mathbf{y}_{l}   \quad\quad\text{(11)}$$

其中，我们用 $$\Delta \mathbf{x}$$ 和 $$\Delta \mathbf{y}$$ 表示梯度：$$ \frac{\partial \mathcal{E}}{\partial \mathbf{x}} $$ 和 $$\frac{\partial \mathcal{E}}{\partial \mathbf{y}}$$。$$ \Delta \mathbf{y} $$ 表示一个 $$k \times k$$ 像素，$$d$$ 通道的矩阵。每个卷积核内，参数总数为 $$ \hat{n}=k^{2} d $$，其中，$$\hat{n} \neq n = k^2c$$。$$\hat{W}$$ 是一个 $$c \times \hat{n}$$ 的矩阵，其以反向传播的方式进行了重新排列。

综上，我们假设 $$w_l$$ 和 $$\Delta{y_l}$$ 相互独立，那么当 $$w_l$$ 初始化为 `0` 附近的对称分布时，对于所有的 $$l$$， $$\Delta{x_l}$$ 具有 `0` 均值。

在反向传播阶段，我们同时有 $$ \Delta y_{l}=f^{\prime}\left(y_{l}\right) \Delta x_{l+1} $$，其中 $$f^{'}$$ 为 $$f$$ 的微分。在 `ReLU` 的情况下， $$f^{\prime}(y_{l})$$ 只能为 `0` 或 `1`，且两者概率相等。我们假设 $$ f^{\prime}\left(y_{l}\right) $$ 和 $$\Delta x_{l+1}$$ 相互独立。因此可得，$$ E\left[\Delta y_{l}\right]=E\left[\Delta x_{l+1}\right] / 2=0 $$ 以及 $$ E\left[\left(\Delta y_{l}\right)^{2}\right]=\operatorname{Var}\left[\Delta y_{l}\right]=\frac{1}{2} \operatorname{Var}\left[\Delta x_{l+1}\right] $$ 。对于 `PReLU`，将变为 $$\frac{1 + a^2}{2}$$。因此，可得反向传播的方差为：

$$\begin{aligned} \operatorname{Var}\left[\Delta x_{l}\right] &=\hat{n}_{l} \operatorname{Var}\left[w_{l}\right] \operatorname{Var}\left[\Delta y_{l}\right] \\ &=\frac{1}{2} \hat{n}_{l} \operatorname{Var}\left[w_{l}\right] \operatorname{Var}\left[\Delta x_{l+1}\right] \end{aligned}   \quad\quad\text{(12)}$$

从第 `L` 层往前反向传播，可得：

$$\operatorname{Var}\left[\Delta x_{2}\right]=\operatorname{Var}\left[\Delta x_{L+1}\right]\left(\prod_{l=2}^{L} \frac{1}{2} \hat{n}_{l} \operatorname{Var}\left[w_{l}\right]\right)   \quad\quad\text{(13)}$$


我们考虑到一个充分的条件，即：梯度不是指数级的大或小：

$$\frac{1}{2} \hat{n}_{l} \operatorname{Var}\left[w_{l}\right]=1, \quad \forall l   \quad\quad\text{(14)}$$

该公式与公式 `10` 的唯一区别在于， $$\hat{n}_{l}=k_{l}^{2} d_{l}$$ 而 $$n_{l}=k_{l}^{2} c_{l}=k_{l}^{2} d_{l-1}$$。上式结果将得到一个 `0` 均值的高斯分布，其标准差为 $$\sqrt{2/\hat{n}_l}$$。


对于第一层（$$l = 1$$），我们无需计算 $$\Delta{x_1}$$，因为其输入为图像。但是我们仍可将上式用于第一层，与前向传播中一样，单层影响不大。

使用公式 `10` 或 `14` 计算权值参数的分布，进行初始化均可。

### 讨论

假设每层的前向传播 / 反向传播信号通过因子 $$\beta$$ 进行了不合理的缩放，则在 `L` 层之后，最后的反向传播信号将会被缩放 $$\beta^{L}$$ 倍，因此很容易发生梯度消失或梯度爆炸，从而导致模型不收敛。

我们同时做了实验，解释为什么常量的标准差，比如 `0.01` 会导致深度网络训练停滞。我们以一个 `VGG` 网络（`model B`）为例，其有 `10` 层卷积层，每一层的尺寸均为 $$3 \times 3$$。前两层的滤波器数量为 `64`，三四层为 `128`，五六层为 `256`，其余的为 `512`。标准差计算：$$\sqrt{2/\hat{n}_l}$$ 分别为 `0.059`， `0.042`， `0.029` 和 `0.021`。

而如果直接初始化为 `0.01`，则从 `conv10` 到 `conv2` 的反向传播梯度为 $$ 1/(5.9 + 4.2^2 + 2.9^2 + 2.1^2)  = 1/(1.7 \times 10^4) $$。这就解释了，为什么实验过程中会发现梯度衰减的现象。

同样要注意，输入信号的方差可大致的从第一层保持到最后一层。当输入信号未进行标准化时（例如取值范围为 `[-128, 128]`），其影响较大，将使得 `softmax` 上溢。可以通过对输入进行标准化解决，但是这可能会影响其他超参数。另一种解决方式是在部分或全部层的权值上，引入一个小的因子，例如 $$\sqrt[L]{1 / 128}$$ 在 `L` 层上。

实际上，我们对前两层全连接层使用 `0.01` 的标准差，对最后一层使用 `0.001` 的标准差，比其理论值要小（$$\sqrt{2/4096}$$），并将解决归一化问题。

对于 `PReLU` 情况下的初始化，公式 `10` 将变为：

$$\frac{1}{2}\left(1+a^{2}\right) n_{l} \operatorname{Var}\left[w_{l}\right]=1, \quad \forall l   \quad\quad\text{(15)}$$

其中，$$a$$ 为系数的初始值。

### 与 `Xavier` 初始化的对比

与 `Xavier` 初始化相比，我们的区别在于解决了整流器的非线性问题。`Xavier` 只考虑到了线性情况，其结果由 $$n_{l} \operatorname{Var}\left[w_{l}\right]=1$$ 指定（前向传播），而这可以实现为 `0` 均值的高斯分布，其标准差为 $$\sqrt{1/n_l}$$。当存在 `L` 层时，其为我们标准差的 $$1 / \sqrt{2}^{L}$$。该值不足以小到使我们的模型（如下表所示）收敛停滞为止。

<div style="text-align:center">
<img src="/images/大模型的架构.png" width="90%"/>
<p>表 3：22 层较大网络的架构</p>
</div><br>

如下图所示，为一个 `22` 层模型的收敛对比。均使用 `ReLU` 激活函数。`Xavier` 均匀分布，我们的为正态分布。两种方式都能收敛，但是我们的方式收敛更快。

<div style="text-align:center">
<img src="/images/22 层模型的收敛.png" width="70%"/>
<p>图 2：22 层较大网络的收敛速度</p>
</div><br>

同时我们也研究了其对精度的影响，如表 `2` 所示，`Xavier` 结合 `ReLU` 的精度并未明显低于我们的方法。

接着，我们在一个更深的 `30` 层（`27` 层 `Conv`，`3` 层 `FC`）网络上，比较了两种方法。我们在表 `1` 的基础上添加了 `16` 层卷积层，每层包含 `256` 个卷积核，尺寸为 $$2 \times 2$$。下图显示了其收敛情况。

<div style="text-align:center">
<img src="/images/30 层较小模型的收敛情况.png" width="70%"/>
<p>图 3：30 层较大模型的收敛情况</p>
</div><br>

如上图所示，我们的初始化方法可以使得较深的模型收敛，而 `Xavier` 初始化下，模型的训练则完全停滞。

这些研究表明，尽管较深的网络表现并不一定好（由于一些未知原因），但是可以证明，我们的初始化方式对于深度网络的训练是可行的。

## 网络架构

以上的研究为我们的网络架构设计提供了基本设计准则。

我们的 `baseline` 为一个 `19` 层的模型（`model A`），如表 `3` 所示。其相对于 `VGG` ，有如下更改：

1. 第一层，用 $$7 \times 7$$ 的卷积层，`stride = 2`
2. 将两个最大特征图尺寸上 $$(224, 112)$$ 的另 `3` 个卷积层，转移至较小的特征图 $$(56, 28, 14)$$ 上。时间复杂度几乎不变
3. 在第一层 `FC` 之前，我们使用空间金字塔池化（`spatial pyramid pooling`，`SPP`）。其有 `4` 层，池化窗口尺寸分别为 $$7 \times 7, 3 \times 3, 2 \times 2, 1 \times 1$$，共 `63` 个输出值。

<div style="text-align:center">
<img src="/images/SPP.jpg" width="80%"/>
<p>SPP 原理</p>
</div><br>

`model A` 的精度并不一定比 `VGG-19` 好，但是其速度更快。

在表 `3` 中，模型 `B` 相较于 `A` 更深，其添加了额外的 `3` 层卷积层。模型 `C` 相较于 `B` 则更宽（更多的卷积核）。

# 实验细节

## 训练

首先将图像进行 `resize`，使其短边长度为 `s`，然后随机 `crop` 尺寸为 $$224 \times 224$$ 的图像，将其在每个像素上，减去像素的均值，之后作为输入。同时，`crop` 之前，将图像以 `0.5` 的概率进行随机水平翻转，并进行随机颜色抖动。

此外，我们利用我们的初始化方式，从零开始训练模型，这使得我们的模型精度有所提升，因为其避免了模型在与训练数据集上可能发生的局部最优。

其他的超参数为：权值衰减系数为 `0.0005`，动量系数为 `0.9`。`Dropout` 系数为 `0.5`，且只在前两层 `FC` 层使用。`batch size` 固定为 `128`。学习速率设置为 `1e-2`， `1e-3` 和 `1e-4`，每次当误差不再降低时进行切换。每个模型的总的 `epoch` 为 `80`。

## 测试

我们采用 `multi-view` 的策略，并使用 `dense sliding window` 方法进行优化。

我们首先对 `resize` 后的整图应用卷积层，得到最后的卷积层特征图。在这些特征图中，对每个 $$14 \times 14$$ 的窗口使用 `SPP` 池化。然后用 `FC` 层来计算得分。同时，我们对图像的水平镜像也做此处理。所有 `dense sliding window` 的得分将进行平均。我们进一步结合多尺度的结果。

# 在 ImageNet 上的实验

我们在 `ImageNet-2012` 数据集上进行实验，共 `1000` 类。

## ReLU 和 PReLU 的比较

如下表所示，为在较大模型 `model A` 上使用不同激活函数的结果。

<div style="text-align:center">
<img src="/images/model A 上的精度对比.png" width="70%"/>
<p>表 4：model A 上不同激活函数的表现</p>
</div><br>


结合表 `2` 和表 `4` 可知，在较小和较大模型上，使用 `PReLU` 相较于 `ReLU`，其精度均有所提升，且几乎不引入任何额外的计算。

## 比较单模型结果

接下来，我们比较单模型上的表现。如下表所示，为 `10-view` 的测试结果：

<div style="text-align:center">
<img src="/images/单模型上 10-view 结果.png" width="50%"/>
<p>表 5：ImageNet 2012 验证集上单模型上 10-view 结果</p>
</div><br>

如下表所示，为 `ImageNet 2012` 验证集上的单模型对比结果，都是在 `multi-scale 和 multi-view` 测试下的结果。其中，我们的模型用 `MSRA` 表示。

<div style="text-align:center">
<img src="/images/ImageNet 2012 验证集上单模型结果.png" width="80%"/>
<p>表 6：ImageNet 2012 验证集上单模型结果</p>
</div><br>

<div style="text-align:center">
<img src="/images/ImageNet 2012 测试集上多模型结果.png" width="80%"/>
<p>表 7：ImageNet 2012 测试集上多模型结果</p>
</div><br>