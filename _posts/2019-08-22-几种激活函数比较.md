---
layout: post
title:  "几种激活函数的比较"
date:   2019-08-22 14:18:01 +0800
categories: 人工智能
tag: 深度学习基础
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

# 激活函数的定义及作用

如下图所示，在神经元线性变换之后，引入的非线性函数即为激活函数(`Activation Function`)。

<div style="text-align:center">
<img src="/images/激活函数定义.png" width="70%">
</div><br>
激活函数用于引入非线性。神经网络的多层架构，理论上可以逼近任何非线性函数。若无激活函数，再多层也只是线性变换，即：逻辑回归。

> **激活函数用于对结果进行平滑处理，其目标是引入非线性。因为非线性能够表示更分为复杂的特征**。

此外，由于神经网络的运行依赖于反向传播，而反向传播又依赖于链式求导，所以，激活函数必须是**可微的**。由于模型最优化需要将问题转换为凸优化，所以需要保持激活函数为**单调的**。

# 常用的激活函数

## sigmoid 函数

**函数公式**

$$
f(z) = \frac{1}{1 + exp(-z)}
$$

**函数图像**

函数及其导数图像如下所示：

<div style="text-align:center">
<img src="/images/sigmoid 函数.png" width="80%">
</div><br>

**优点**

`sigmoid` 函数的取值范围为 `0 ~ 1`，它可以将一个实数映射到 `(0,1)` 的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。

**缺点**

- **容易出现梯度消失**（`Gradient Vanishing`）

    在反向传播过程中，`sigmoid` 容易导致梯度消失。从上面的图像可以看出，其导数最大值为 `0.25`，且非零区域很小。当数据分布于两侧时，导数会非常小，更易发生梯度消失。
    
    详细证明，见反向传播部分的笔记。

- 输出是 `Not-zero-centered`

    `Sigmoid` 函数的输出值恒大于 `0`，这会导致模型训练的收敛速度变慢。详细见笔记《谈谈激活函数以零为中心的问题》。
    
    深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用 `zero-centered` 数据 (可以经过数据预处理实现) 和 `zero-centered` 输出。

- 幂运算相对耗时

    相比于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在 `ReLU` 函数中，需要做的仅仅是一个 `thresholding`，相对于幂运算来讲会快很多。

## Tanh 函数

**函数公式**

$$
f(z) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**函数图像**

函数及其导数图像如下所示：

<div style="text-align:center">
<img src="/images/tanh 激活函数图像.png" width="80%">
</div><br>

**特点**

`tanh` `具有 sigmoid` 的优点，同时解决了非 `Not-zero-centered` 的问题。但同时，仍具备 `sigmoid` 的缺点。

## ReLU 函数

`ReLU` 函数即 `Rectified Linear Unit`，表示线性整流函数。是目前最流行的激活函数。

**函数公式**

$$
\phi(x) = max(0, x)
$$

**函数图像**

函数及其导数图像如下所示：

<div style="text-align:center">
<img src="/images/ReLU 激活函数.png" width="90%">
</div>

**优点**

`ReLU` 并非全区间可导，因此可以取子梯度。主要有如下优点：

- 解决了 `gradient vanishing` 问题 (至少 `x` 在正区间内，神经元不会饱和)
- 由于 `ReLU` 线性、非饱和的形式，在 `SGD` 中能够**更快速收敛**
- **计算速度要快很多**。`ReLU` 函数只有线性关系，不需要指数计算


**缺点**

- `ReLU` 的输出不是“零为中心”(`Notzero-centered output`)
- 随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。这种神经元的死亡是不可逆转的死亡

**神经元死亡**

若某个神经元输入小于 `0`，则其对应的输出应该为 `0`。回到反向传播部分，`ReLU` 在 `< 0` 的部分，其导数始终为 `0`，也即参数无法得到更新。所以无论训练多少次，输出仍为 `0`。所以，神经元不在起作用，即：死亡！

## Leaky ReLU 函数

`Leaky ReLU` 是对 `ReLU` 的一种改进，可以解决假死问题。在小于 `0` 的部分，函数值及其梯度值不为 `0`。这样，即修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。

**函数公式**

$$
y_{i}=\left\{\begin{array}{ll}{x_{i}} & {\text { if } x_{i} \geq 0} \\ {\frac{x_{i}}{a_{i}}} & {\text { if } x_{i}<0}\end{array}\right.
$$

**函数图像**

<div style="text-align:center">
<img src="/images/leaky ReLU 图像.png" width="90%">
</div>

**优点**

- 神经元不会出现死亡的情况。
- 对于所有的输入，不管是大于等于 `0` 还是小于 `0`，神经元不会饱和。
- 由于 `Leaky ReLU` 线性、非饱和的形式，在 `SGD` 中能够快速收敛。
- 计算速度要快很多。`Leaky ReLU` 函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比 `sigmoid` 和 `tanh` 快。

**缺点**

`Leaky ReLU` 函数中的 α，需要通过先验知识人工赋值。

## maxout 函数

`Maxout` 的拟合能力非常强，它可以拟合任意的凸函数。`Goodfellow` 在论文中从数学的角度上也证明了这个结论，只需要 `2` 个 `Maxout` 节点就可以拟合任意的凸函数，前提是“隐含层”节点的个数足够多。

**函数方程**

$$
f_i(x) = max_{j \in [1, k]} z_{ij}, \quad\quad \text{其中：}

z_{ij} = x^TW_{...ij} + b_{ij}
$$

**优点**

- `Maxout` 具有 `ReLU` 的所有优点，线性、不饱和性。
- 同时没有 `ReLU` 的一些缺点。如：神经元的死亡。

**缺点**

从这个激活函数的公式中可以看出，每个 `neuron` 将有两组 `w`，那么参数就增加了一倍。这就导致了整体参数的数量激增。


# 激活函数以零为中心

以二元神经元为例，$$f(X;W,b) = f(w_0x_0 + w_1x_1 + b)$$。现在假设，对应的最优解 $$w_0^*, w_1^*$$ 满足条件：

$$
\begin{cases}
w_0 < w_0^* \\
w_1 > w_1^* 
\end{cases}
$$

这也就是说，我们希望 $$w_0$$ 适当增大，但希望 $$w_1$$ 适当减小。根据反向传播，两参数的梯度应该有公共部分，这必然要求输入 $$x_0$$ 和 $$x_1$$ 符号相反。

以 `sigmoid` 函数为例，输出恒为正。也就是说，如果上一级神经元采用 `Sigmoid` 函数作为激活函数，那么我们无法做到 $$x_0$$ 和 $$x1$$ 符号相反，只能同时增大或缩小。

此时，模型为了收敛，不得不向逆风前行的风助力帆船一样，走 `Z` 字形逼近最优解。如下图所示：

<div style="text-align:center">
<img src="/images/激活函数非零中心.png" width="50%">
</div><br>

如上图所示，模型参数走绿色箭头能够最快收敛，但由于输入值的符号总是为正，所以模型参数可能走类似红色折线的箭头。如此一来，使用 `Sigmoid` 函数作为激活函数的神经网络，收敛速度就会慢上不少了。