---
layout: post
title:  "Dropout 网络层"
date:   2019-08-22 14:08:01 +0800
categories: 人工智能
tag: 深度学习基础
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**
> 昨天面试的时候，遇到的问题。以前是看吴恩达的课程里面学习的，时间较长，加上没有认真总结过，所以面试的时候没有答全，这里记录一下。
> 当前记录也不一定完整，如果以后有新的想法或收获，再进行补充。

****

**参考**

-   [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
-   [神经网络 Dropout 层中为什么 dropout 后还需要进行 rescale？](https://www.zhihu.com/question/61751133)
-   [Implementing dropout from scratch](https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch)

****

## 原理动机

一开始提出 `dropout` 是为了解决全连接网络的过拟合问题，旨在缓解神经元之间的**隐形协同效应**，希望用尽可能少的神经元，完成以前多个神经元共同完成的工作，从而达到降低模型复杂度的目的。

>   **一般用于全连接层。**

## 基本流程和原理

`dropout` 之在训练阶段进行，在推理阶段不进行。在训练阶段的**每次**迭代中，**随机**对神经网络的部分（通过概率值控制）神经元进行失活。其中，被失活的神经元在当前迭代中，参数值为 `0`，从而不参与前向和反向传播，达到降低网络结构复杂度的作用。

### 训练阶段

在训练之前，需要通过如下方式设置模型：

```python
model.train()
```

在训练阶段，对每一层的神经元，以概率 `(1 -  p)` 进行随机失活，失活的神经元参数值为 `0`。

由于上一层的输入来源不确定，因此，下一层的值不敢太依赖任何的输入点。因此，权值会给每个连接较小的权值。通过这种方式泛化权值，有利于压缩权值的范数，防止过拟合。

<div style="text-align:center">
<img src="/images/dropout.png" width="90%">
</div><br>

由于只有 `p` 比例的神经元参与训练，而测试阶段全部神经元都需要参与，为了保证训练和测试同等的输出规模，需要使用 `inverted dropout`：在每次 `dropout` 后的权重，扩大为当前值的 $$\frac{1}{p}$$ 倍，从而保持总规模不变。

$$
\begin{aligned} r_{j}^{(l)} & \sim \operatorname{Bernoulli}(p) \\ \widetilde{\mathbf{y}}^{(l)} &=\mathbf{r}^{(l)} * \mathbf{y}^{(l)} \\ z_{i}^{(l+1)} &=\mathbf{w}_{i}^{(l+1)} \widetilde{\mathbf{y}}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &=f\left(z_{i}^{(l+1)}\right) \end{aligned}
$$

### 测试阶段

在测试之前，需要通过如下方式对模型进行设置：

```python
model.eval()
```

在测试阶段，不能使用 `dropout`。其余同正常网络，无需变动。

$$
\begin{aligned} z_{i}^{(l+1)} &=\mathbf{w}_{i}^{(l+1)} \mathbf{y}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &=f\left(z_{i}^{(l+1)}\right) \end{aligned}
$$

## 作用效果

1.  使用 `dropout` 进行随机失活，可以简化网络结构，从而起到正则化的效果
2.  由于每次迭代的随机失活，使得网络的神经元之间的隐形协同效应不再明显（不能保证两个神经元每次都处于激活态），神经元之间依赖程度降低，迫使网络学习更加鲁棒的特征，同时模型参数数值相对较小，不会过分依赖与某些神经元，起到了 `L2` 范数的效果，压缩了权重，起到了正则化的作用
3.  由于每次迭代中，`dropout` 网络的结构都是随机的。因此，可以将使用 `dropout` 层训练的网络视作**多模型融合**

## Numpy 实现

```python
class Dropout(nn.Module):
    def __init__(self, p=0.5, inplace=False):
#         print(p)
        super(Dropout, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p
        self.inplace = inplace

    def forward(self, input):
        print(list(input.shape))
        return np.random.binomial([np.ones((len(input),np.array(list(input.shape))))],1-dropout_percent)[0] * (1.0/(1-self.p))

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
            + 'p=' + str(self.p) \
            + inplace_str + ')'

class MyLinear(nn.Linear):
    def __init__(self, in_feats, out_feats, drop_p, bias=True):
        super(MyLinear, self).__init__(in_feats, out_feats, bias=bias)
        self.custom_dropout = Dropout(p=drop_p)

    def forward(self, input):
        dropout_value = self.custom_dropout(self.weight)
        return F.linear(input, dropout_value, self.bias)
```

