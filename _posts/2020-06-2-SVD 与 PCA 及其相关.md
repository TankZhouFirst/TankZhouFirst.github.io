---
layout: post
title:  "SVD 与 PCA 及其相关"
date:   2020-06-02 12:28:01 +0800
categories: 人工智能
tag: 机器学习
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- [线性代数基础梳理](https://blog.csdn.net/a727911438/article/details/77531973)
- [矩阵特征值](https://baike.baidu.com/item/矩阵特征值/8309765?fr=aladdin)
- [矩阵奇异值分解 (SVD) 与主成份分析 (PCA) 详解](https://blog.csdn.net/u014475479/article/details/78520737)
- [奇异值分解 (SVD) 原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)
- [主成分分析（PCA）原理总结](https://www.cnblogs.com/pinard/p/6239403.html)
- [如何证明奇异值分解 SVD？](https://www.zhihu.com/question/23546309?sort=created)

****

# 线代基础

## 特征值与特征向量

### 基本定义

设 `A` 是 **`n` 阶**矩阵，如果存在一个数 `λ` 及**非零**的 `n` 维**列向量** `α` ，使得：

$$
A\alpha = \lambda\alpha
$$

成立，则称 `λ` 是矩阵 `A` 的一个**特征值**，称非零向量 `α` 是矩阵 `A` 属于特征值 `λ` 的一个**特征向量**。

### 求解方式

可以将上式转换为如下形式：

$$
(\lambda E - A)\alpha = 0
$$

这是一个 `n` 阶其次线性方程组，其存在非零解的充要条件为：系数行列式 $$|A - \lambda E| = 0$$。因此，求解上式，即可求得矩阵 `A` 的特征值和特征向量。

### 计算实例

求解如下矩阵的特征值和特征向量。

$$
\begin{equation}
A=\left[\begin{array}{lll}{1} & {-3} & {3} \\ {3} & {-5} & {3} \\ {6} & {-6} & {4}\end{array}\right]
\end{equation}
$$

求解过程如下：

**首先求特征值：**

$$
\begin{equation}
\begin{array}{l}{\operatorname{det}(\lambda E-A)=\left|\begin{array}{ccc}{\lambda-1} & {3} & {-3} \\ {-3} & {\lambda+5} & {-3} \\ {-6} & {6} & {\lambda-4}\end{array}\right|=(\lambda+2)\left|\begin{array}{ccc}{1} & {3} & {-3} \\ {1} & {\lambda+5} & {-3} \\ {0} & {6} & {\lambda-4}\end{array}\right|} \\ {=(\lambda+2)^{2}(\lambda-4)=0}\end{array}
\end{equation}
$$

解得特征值为：$$\lambda_1 = \lambda_2 = -2, \lambda_3 = 4$$。

**分别求特征值对应的特征向量：**

对于特征值 `-2`，代入可得方程组 $$(-2E-A)x = 0$$：

$$
\begin{equation}
-2 \mathbf{E}-\mathbf{A}=\left[\begin{array}{rrr}{-3} & {3} & {-3} \\ {-3} & {3} & {-3} \\ {-6} & {6} & {-6}\end{array}\right] \rightarrow\left[\begin{array}{rrr}{1} & {-1} & {1} \\ {0} & {0} & {0} \\ {0} & {0} & {0}\end{array}\right]
\end{equation}
$$

可得同解方程组：$$x_1 - x_2 + x_3 = 0$$，解得：$$x_1 = x_2 - x_3$$。其中，$$x_2, x_3$$ 为未知向量。分别设定自由未知量：

$$
\begin{equation}
\left[\begin{array}{l}{\mathbf{x}_{2}} \\ {\mathbf{x}_{3}}\end{array}\right]=\left[\begin{array}{l}{\mathbf{1}} \\ {\mathbf{0}}\end{array}\right]\left[\begin{array}{l}{\mathbf{x}_{2}} \\ {\mathbf{x}_{3}}\end{array}\right]=\left[\begin{array}{l}{\mathbf{0}} \\ {\mathbf{1}}\end{array}\right]
\end{equation}
$$

解得基础解系为：

$$
\begin{equation}
\xi_{1}=\left[\begin{array}{l}{1} \\ {1} \\ {0}\end{array}\right] \quad \xi_{2}=\left[\begin{array}{c}{-1} \\ {0} \\ {1}\end{array}\right]
\end{equation}
$$

因此，特征值 $$\lambda = -2$$ 对应的全部特征向量为：$$\begin{equation} \mathrm{x}=\mathrm{k}_{1} \xi_{1}+\mathrm{k}_{2} \xi_{2}
\end{equation}$$ （$$k_1, k_2$$ 不全为 `0`）。

同理可得，$$\lambda = 4$$ 对应的基础解系为：$$\begin{equation} \xi_{3}=[1,1,2]^{T} \end{equation}$$，特征向量为：$$\begin{equation}x=k_{3} \xi_{3}\end{equation}$$。

### 物理含义

1. 一个列向量在左乘一个矩阵后，会经过一系列的线性变换，最终向量的长度会变成原来的 `λ` 倍。
2. **从向量空间的角度来看，因为不同特征值对应的特征向量线性无关，把每个特征向量看做是一个坐标轴，特征值是对应坐标轴（即特征向量）的坐标值。**简单来说，就是用特征值（坐标）与特征向量（坐标轴）来表示原矩阵。（空间坐标系转换）

> **更形象的解释，参考：https://www.zhihu.com/question/20507061**

## 特征值分解

特征值分解，就是将矩阵分解为由其特征值和特征向量表示的矩阵之积。需要注意：**只有对可进行对角化的矩阵（所有特征向量线性无关），才可以进行特征分解**。

若矩阵 `A` 为一个 `N` 阶矩阵，且其存在 `N` 个**线性无关的特征向量** `q`，则 `A` 可被分解为：

$$
\begin{equation}
\mathbf{A}=\mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}=\mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{T}
\end{equation}
$$

其中，`Q` 为 `N` 阶方阵，其第 `i` 列为 `A` 的特征向量。$$\begin{equation} \boldsymbol{\Lambda}\end{equation}$$ 为对角矩阵，对角线上元素为对应的特征值。注意：特征值与特征向量之间的位置要对应。

> **详细运算过程，参考：[矩阵特征值和特征向量求解——特征值分解](https://blog.csdn.net/qq_14959801/article/details/69803254)**

## 正交矩阵与正交变换

### 正交矩阵

若 $$AA^T = E$$，则称方阵 `A` 为正交矩阵。其满足如下条件和性质：

1. 矩阵 `A` 的各行均为单位向量，且两两正交
2. 矩阵 `A` 的各列均为单位向量，且两两正交 

### 正交变换

若矩阵 `P` 为正交矩阵，则线性变换 $$y = Px$$ 称为正交变换。正交变换具有如下优秀的特质：**不改变向量的长度和内积，即不改变图形的几何形状**。

证明如下：

若 $$y = Px$$ 为正交变换，且 $$y_1 = Px_1, y_2 = Px_2$$，则：

$$
\begin{equation}
\begin{array}{l}{\left\|y_{1}\right\|=\sqrt{y_{1}^{\top} y_{1}}=\sqrt{\left(P x_{1}\right)^{\top}\left(P x_{1}\right)}=\sqrt{x_{1}^{\top} P^{\top} P x_{1}}=\sqrt{x_{1}^{\top} x_{1}}=\left\|x_{1}\right\|} \\ {\left[y_{1}, y_{2}\right]=y_{1}^{\top} y_{2}=\left(P x_{1}\right)^{\top}\left(P x_{2}\right)=x_{1}^{\top}\left(P^{\top} P\right) x_{2}=x_{1}^{\top} x_{2}=\left[x_{1}, x_{2}\right]}\end{array}
\end{equation}
$$

# 主成分分析 PCA

主成分分析（`Principal components analysis`，以下简称 `PCA`）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。

## 基本思想

`PCA` 的主要思想是，用数据最主要的方面来替换原始数据。具体来讲，假设我们的数据集是 `n` 维的，共有 `m` 个数据，我们希望将其降维到 $$n^{'}$$ 维，且损失尽可能小。

如下图所示，将二维将为一维，我们更倾向于保留 $$u_1$$ 维度上。

<div style='text-align:center'>
    <img src='/images/PCA 降维.png' width='80%' height='80%'>
</div>

原因如下：

1. 样本点到这个直线的距离足够近
2. 样本点在这个直线上的投影能尽可能的分开

## PCA 推导

> **待补充，参考：https://www.cnblogs.com/pinard/p/6239403.html**

## PCA 算法流程

样本 $$x^{(i)}$$ 的 $$n^{'}$$ 维的主成分，其实就是求样本集的协方差矩阵 $$XX^T$$ 的前 $$n^{'}$$ 个特征值对应的特征向量矩阵 `W`，然后对于每个样本 $$x^{(i)}$$ 做如下变换 $$z^{(i)} = W^Tx^{(i)}$$，即达到降维的 `PCA` 目的。

具体流程如下：

1. 对所有输入样本进行**中心化**：$$\begin{equation}
    x^{(i)}=x^{(i)}-\frac{1}{m} \sum_{j=1}^{m} x^{(j)}
    \end{equation}$$
2. 计算样本的**协防差**矩阵 $$XX^T$$
3. 对协防差矩阵 $$XX^T$$ 进行**特征值分解**
4. 取出最大的 $$n^{'}$$ 个特征值对应的特征向量，将其标准化，得到特征矩阵 `W`
5. 对样本集中的每一个样本 $$x^{(i)}$$，转换为新样本：$$z^{(i)} = W^Tx^{(i)}$$
6. 得到输出数据集

> **有时候可以不指定 $$n^{'}$$，而是指定主成分累计比重的阈值。**

## 总结

作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩、去噪。因此在实际场景应用很广泛。

主要优点有：

1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响
2. 各主成分之间正交，可消除原始数据成分间的相互影响的因素
3. 计算方法简单，主要运算是特征值分解，易于实现

主要缺点有：

1. 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强
2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响

# 奇异值分解 SVD

奇异值分解 (`Singular Value Decomposition`) 是一种重要的矩阵分解方法，其广泛应用于降维算法、推荐系统、自然语言处理等领域。

## SVD 定义

前面的特征值分解中，要求矩阵为方阵，而在 `SVD` 中，不做此要求。对于实矩阵 $$A_{m \times n}$$，我们定义其 `SVD` 为：

$$
A_{m * n}=U_{m * m} \Sigma_{m * n} V_{n * n}^{T}
$$

其中，$$\Sigma$$ 矩阵z中，除了对角线元素外，其余元素均为 `0`，对角元素称为奇异值。`U` 和 `V` 均为**正交矩阵**。

<div style='text-align:center'>
    <img src='/images/SVD 分解.png' width='90%' height='90%'>
</div>

## SVD 求解

取 $$AA^T$$，得到一个 `m` 阶对称矩阵，可以进行奇异值分解（特征值分解）。将分解得到的所有的特征向量组成一个 `n` 阶**正交矩阵** `V`，这就是上面的 `V` 矩阵。我们将 `V` 中的每个特征向量称为 `A` 的右奇异向量。

证明如下式所示：

$$
\begin{equation}
A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma^{T} U^{T} \Rightarrow A^{T} A=V \Sigma^{T} U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}
\end{equation}
$$

类似的，根据 $$A^TA$$ 得到的特征矩阵即为 `U`。我们将 `U` 中的每个特征向量称为 `A` 的左奇异向量。证明同上。

根据上面的证明，亦可发现矩阵 $$\Sigma$$ 的求法，即为 $$AA^T$$ 的特征值的开方。

> **注意：奇异值与奇异变量的顺序要对应。**

## SVD 证明

首先，对于任意实矩阵矩阵 $$A_{m \times n}$$ ，其表示从空间 $$R^n$$ 到 $$R^m$$ 的线性映射，如下：

$$
A_{m \times n}c_{n \times 1} = s_{m \times 1}
$$

$$A^TA$$ 为一个 `n` 阶对称方阵，对其进行特征值的分解，得到：$$A^TA = VDV^T$$，特征向量 $$(v_1, v_2,...,v_n)$$ 构成空间 $$R^m$$ 的一组标准正交基。

我们使用下式，进行空间 $$R^N \to R^m$$ 的线性映射：

$$
\begin{equation}
A\left(v_{1}, v_{2}, \ldots v_{n}\right)=\left(A v_{1}, A v_{2}, \ldots A v_{n}\right) \in R^{m}
\end{equation}
$$

设定 $$A^TA$$ 的秩为 `r`，则 $$A v_{1}, A v_{2}, \ldots A v_{n}$$ 中有 `r` 个向量，可以构成 $$R^m$$ 的一部分基，其余的对应于零向量。

对 $$A v_{1}, A v_{2}, \ldots A v_{n}$$ 两两之间做内积，由正交性，只有 $$i=j \quad (i,j \leq r)$$ 时，有：

$$
\begin{equation}
\left|A v_{i}\right|^{2}=\left(A v_{i}, A v_{i}\right)=\lambda_{i} v_{i}^{T} v_{i}=\lambda_{i}
\end{equation}
$$

对 $$Av_i$$ 进行标准化，得到：

$$
\begin{equation}
u_{i}=\frac{A v_{i}}{\left|A v_{i}\right|}=\frac{1}{\sqrt{\left(\lambda_{i}\right)}} A v_{i}
\end{equation}
$$

将 $$u_1,u_2,...,u_r$$ 扩充至空间 $$R^m$$ 中的一组标准正交基 $${\{u_1,u_2,...,u_r,u_{r+1},...,u_m}\}$$。在这组基下，有：

$$
\begin{equation}
A V=\left(\delta_{1} u_{1}, \delta_{2} u_{2}, \ldots \delta_{r} u_{r}, 0, \ldots 0\right)=U \Sigma
\end{equation}
$$

其中，$$\begin{equation} \Sigma=\operatorname{diag}\left(\delta_{1}, \delta_{2}, \ldots \delta_{r}, 0, \ldots, 0\right) \end{equation}$$ 是由奇异值构成的对角矩阵，称为奇异值矩阵；$$U^* = (u_1,u_2,...,u_m)$$ 。

将上式两边同时乘以 $$V^{-1}$$，可得：$$A = U \Sigma V^T$$。

从上面的证明可以发现：

1. 所谓奇异值分解，就是揭示了一般意义上两个不同维度的线性空间之间进行线性映射前后的基之间的变换关系。其中，`V` 表示原始域的标准正交基；`U` 表示变换后的标准正交基；$$\Sigma$$ 表示了 `V` 与 `U` 之间的映射关系
2. 第 $$i$$ 个奇异值 $$\delta_i$$ 还反映了变换后第 $$i$$ 个维度上的分量对数据总信息量的贡献程度。

## SVD 性质

在上面的求解中，将奇异值按照降序排列，一般奇异值衰减速度很大。通常前 `10%` 甚至 `1%` 的奇异值占据全部奇异值之和的绝大部分。

因此，我们可以用最大的 `k` 个奇异值及其对应的左右奇异向量来近似描述原始矩阵。基于这一点，我们可以只用这一部分向量来表征原始数据，实现数据将维，做数据压缩和降噪。

当然，`SVD` 的缺点是分解出的矩阵**解释性往往不强**，有点黑盒子的味道，不过这不影响它的使用。

还有一个问题就是，提取的属性之间没有联系。

## SVD 应用

### 图像压缩

参考博客：[利用矩阵奇异值分解对图像进行压缩](https://blog.csdn.net/fllubo/article/details/8273493)

主要思路如下：

$$
\begin{equation}
A=\left[\begin{array}{llll}{u_{1}} & {u_{2}} & {\dots} & {u_{m}}\end{array}\right]_{m}\left[\begin{array}{cc}{D} & {0} \\ {0} & {0}\end{array}\right]\left[\begin{array}{c}{v_{1}^{T}} \\ {v_{2}^{T}} \\ {\dots} \\ {v_{m}^{T}}\end{array}\right] \\

A=\sigma_{1} u_{1} v^{T}_{1}+\sigma_{2} u_{2} v^{T}_{2}+\ldots+\sigma_{1} u_{r} v_r^{T}
\end{equation}
$$


## SVD 用于 PCA

使用 `PCA` 降维，需要找到样本协方差矩阵 $$XX^T$$ 的最大的 `d` 个特征向量，对应到 `SVD` 中，即为右奇异矩阵。而部分 `SVD` 算法是不需要使用 $$AA^T$$ 即可求出 `V`。因此，可以大幅度减少计算量。

> 左奇异矩阵可以用于行数的压缩。
>
> 相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的 PCA 降维。