---
layout: post
title:  集成学习的基本概念"
date:   2020-06-02 12:28:01 +0800
categories: 人工智能
tag: 机器学习
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- [菊安酱的机器学习 第 6 期 Adaboost 算法](https://www.bilibili.com/video/av37887744/)

****

## 集成学习的定义

集成学习（`ensemble learning`）是机器学习中的一类方法，它对多个机器学习模型进行组合形成一个精度更高的模型，参与组合的模型称为弱学习器（`weak learner`）。

集成学习可以看成一种训练框架，其弱分类器可以是任何的模型。

## 集成学习的框架

集成学习的实现框架有如下两种：

### bagging

`Bagging` 框架的流程如下所示：

<div style="text-align:center">
<img src="/images/bagging 流程.png" width="95%"/>
</div><br>

其核心思想是：根据均匀概率分布从数据集中重复抽样（有放回的），得到一定容量的采样集。

一般来讲，采样集容量与原数据集一致，且可能会有部分重复样本，部分原数据集中样本并未采样。

然后根据当前采样的数据集，训练一个弱分类器。最后多个弱分类器通过一定方式进行组合，得到强分类器。

`bagging` 的代表有随机森林。

### boosting

`boosting` 的流程如下所示：

<div style="text-align:center">
<img src="/images/boosting 流程.png" width="95%"/>
</div><br>

其核心思想是：给每个样本一个权重，根据上一个弱分类器的表现，自适应的调整所有样本的权重，并用更新分布后的样本，训练下一个弱分类器。

`boosting` 算法的代表有 `Adaboost`，`GBDT`，`XGBoost` 等算法。

## 组合策略

学习完多个弱分类器之后，需要使用一定的组合策略来得到强分类器。组合策略主要有以下几种。

### 平均法

平均法常用于数值型的回归预测问题。假设我们最终得到的 n 个弱分类器为 $$ \left\{h_{1}, h_{2}, \dots, h_{n}\right\} $$。

最简单的就是算数平均：

$$
H(x)=\frac{1}{n} \sum_{1}^{n} h_{i}(x)
$$

如果弱分类器有一个权重，则最终预测的是：

$$
{H(x)=\frac{1}{n} \sum_{1}^{n} w_{i} h_{i}(x)} \\ 

{\text { s.t. } w_{i} \geq 0, \sum_{1}^{n} w_{i}=1}
$$


### 投票法

对于分类问题，常用的是投票法。

**相对多数投票法**：

选择 $$n 个弱分类器对 $$x$$ 的预测结果中，数量最多的类别为最终的预测类别。如果有多个最高票，则随机选择一个。

**绝对多数投票法**：

也就是常说的票数过半。这种推测方式下，不光要获得最高票，还要求票数过半，否则会拒绝预测。

**加权投票法**：

和平均投票法一样，每个弱分类器的分类票数乘以一个权值，最终的各个类别的加权票数总和，最大值者作为最终类别。

### 学习法

前两种方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法。

对于学习法，代表方法是 `stacking`，当使用 `stacking` 的结合策略时，我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本再用次级学习器预测一次，得到最终的预测结果。