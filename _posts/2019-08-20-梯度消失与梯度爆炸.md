---
layout: post
title:  "梯度消失与梯度爆炸"
date:   2019-08-20 16:15:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式转载**

****


## 梯度消失与梯度爆炸的概念

梯度消失与梯度爆炸的概念类似，因为其产生的原因是一样的。其直观表现如下：

在网络训练过程中：
- 如果靠近输入层的部分比靠近输出层的部分变化更快，则为**梯度消失**(`vanishing gradient`)。具体表现就是，发生梯度消失的时候，只有靠近输出层的部分才会进行学习，而其他部分则一层不变，最后网络表现并不好。
- 若靠近输入层的比靠近输出层部分变化更慢，则为**梯度爆炸**(`exploding gradient`)。具体表现就是，网络由输入层部分主导，训练的 `loss` 可能会 `NaN`。

**由于梯度消失出现的概率远大于梯度爆炸出现的概率，除非特别说明，下面以梯度消失为例**

## 产生梯度消失和梯度爆炸的原因

这两种非正常现象产生的根本原因在于，反向传播过程中，梯度的**连乘项太小或太大**导致的（`参照反向传播的笔记`）。即：网络层深度（结构不合适），权值初始化不合理（初始化过大），激活函数不合理（导数项不合理）等因素引起。

### 网络结构角度

网络结构设计不合理时，不同层的学习速度会相差很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢（反向传播的导数很小），有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。

因此，调整网络结构，减少直接的深度，是一种不错的方法。

### 激活函数角度

回看反向传播的笔记，可以发现，连乘项的值，与激活函数的导数有关。比如 `sigmoid` 和 `tanh` 函数及其导数图像分别如下所示：

<div style="text-align:center">
<img src="/images/激活函数角度.png" width="80%">
</div><br>

<div style="text-align:center">
<img src="/images/激活函数角度 2.png" width="80%">
</div><br>

`sigmoid` 函数的导数不大于 `0.25`，极其容易发生梯度消失；而 `tanh` 的导数也小于 `1`，并且只有很窄的区间才不为 `0`，也容易发生梯度消失。

## 梯度消失和梯度爆炸的解决方法

可以通过如下手段解决梯度消失和梯度爆炸的问题：

- 参数修正
    - 选择合适的参数初始化（治本）
    - 梯度裁剪（解决梯度爆炸，效果不太大）
    - `gradient scaling`（解决梯度消失，可以但没必要）
    - 使用 `L2` 正则化（可以的）
    - 批量归一化（治本）

- 更换激活函数（治本）
- 更改网络结构
    - 引入残差网络（`CNN`，治本，解决先天不足）
    - 引入 `LSTM`（`RNN`，治本，解决先天不足）

### 参数修正

#### 随机初始化

主要用到 [Xavier Initialization(Glorot Initialization)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 进行随机初始化，使每一层的输入输出的方差相等，而且正反向传播的梯度也相等。

#### 梯度裁剪（Gradient Clipping）

主要是针对梯度爆炸，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

实际上，梯度裁剪的效果可能并不好，因为发生梯度爆炸时，主要问题是，前面的层梯度较大，占据主导，因此，即使梯度裁裁剪，做法也是很粗糙的。

#### gradient scaling

很多人都在过度解释，认为 `gradient scaling` 从原理上讲不通。但是其实 `gradient scaling` 是可以正常工作的。实践中没人用 `gradient scaling` 的原因就是大家太懒，而且没有必要（因为有了更好的解决方案）。

#### 正则化

另外一种解决梯度爆炸的手段是采用权重正则化（`weithts regularization`）。一般用到 `L2` 正则化更多，因为 `L2` 正则化可以限制参数取值尽可能小，且不为 `0`。

#### 批量归一化（Batch Normalization, BN）

`Batchnorm` 是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果。

`BN` 可以很好的解决梯度爆炸和消失的问题,使得网络对权重初始化不再那么敏感可以使用更大的学习率，提高训练速度，具有正则化的效果，可以减少 `dropout` 等其他正则化技术的使用。但同时，`BN` 操作增加了模型的复杂度，预测时间将不可避免地增加。

`Batchnorm` 本质上是解决反向传播过程中的梯度问题。`batchnorm` 全名是 `batch normalization`，简称 `BN`，即批规范化，通过规范化操作将输出信号规范化保证网络的稳定性。

其基本思想为：  
- 由于反向传播中，连乘项需要乘以权值参数，所以参数的取值范围直接影响梯度的大小。  

- `batchnorm` 就是通过对每一层的输出规范为均值和方差一致的方法，消除了 `w` 带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为 `BN` 将输出从饱和区拉倒了非饱和区。

### 更换激活函数

`sigmoid` 和 `tanh` 激活函数容易导致梯度消失，可以替换成 `ReLU` 函数，`ReLU` 及其导数图像如下所示：

<div style="text-align:center">
<img src="/images/更换激活函数.png" width="90%">
</div><br>

可以很容易看出，`relu` 函数的导数在正数部分是恒等于 `1` 的，因此在深层网络中使用 `relu` 激活函数就不会导致梯度消失和爆炸的问题。

除此之外，`ReLU` 函数更容易计算，可以加速网络的训练。但同时容易导致网络失活（`< 0 ` 的部分）。还可以用 `leakrelu, elu, maxout` 等等。

### 更新网络结构

#### 引入残差结构

残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（`shortcut`）部分，如下所示，详细见 `ResNet` 的笔记。

<div style="text-align:center">
<img src="/images/残差网络.jpeg" width="60%">
</div><br>

残差结构的连乘项为(非严密推导)：

$$
\frac{\partial loss}{\partial x_l} = \frac{\partial loss}{\partial x_L} \frac{\partial x_L}{\partial x_l} = \frac{\partial loss}{\partial x_L}(1 + \frac{\partial}{\partial x_l} \sum_{i=l}^{L}F(x_i, W_i))
$$

如上面所示，始终有一个 `1`，所以不会出现梯度消失的情况。

#### 使用 LSTM 网络

对于时序网络，还有一种  `LSTM` 网络，可以先天性的避免梯度消失，详见反向传播推导部分。

