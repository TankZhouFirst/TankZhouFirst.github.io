---
layout: post
title:  "各种范数"
date:   2019-08-20 17:41:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**参考**：

- [机器学习中的规则化范数( L0, L1, L2 核范数)](https://www.cnblogs.com/TenosDoIt/p/3708996.html)

****

# L0 和 L1 范数

## L0 范数

### 定义

$$ L_0 $$ 范数指的是向量中非零元素的数目。公式如下所示：

$$
||x||_0 = \sum_{i=0}^n x_i^0
$$

### 作用

如果用 $$L0$$ 范数来正则化参数矩阵 `W`，就是希望其大部分元素接近 `0`，实现稀疏。$$L_0$$ 范数的这个特性，很适用于机器学习中的稀疏编码。在特征选择过程中，通过最小化 $$L_0$$ 范数来寻找最小最优的稀疏特征项。

## L1 范数

### 定义

可以理解为，向量中各元素的绝对值之和，即：

$$
||x||_1 = \sum_{i=0}^n |x_i|
$$

### 作用

由于 `L1` 范数会导致参数稀疏化，所以也称之为 “**稀疏规则算子**”（`Lasso regularization`）。实际上，任何的规则化算子，如果他在 $$ w_i = 0$$ 的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。

### 稀疏化的好处

#### 特征选择(Feature Selection)

稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为 `0`。

#### 可解释性(Interpretability)

由于大部分参数为 `0`，所以只考虑模型中起决定性作用的参数，可以更好的解释模型。

### 为什么一般用 L1 而非 L0

因为 `L0` 范数的优化求解是 `NP` 难问题，而 `L1` 又是其最优凸近似。所以一般使用 `L1` 范数，如下所示：

<div style="text-align:center">
<img src="/images/L0 和 L1 的关系.png" width="40%"/>
</div> 

# L2 范数

## 定义

`L2` 范数即欧几里得( `Eulid` )范数，常用于计算向量长度。即：

$$
||x||_2 = \sqrt {\sum_{i=0}^n x_i^2}
$$

## 作用

`L2` 范数又称为“岭回归”（`Ridge Regression`），也叫“权值衰减”（`weight decay`）。`L2` 范数主要用于解决过拟合问题。  

>   **L1 范数只是让所有参数的绝对值之和最小化，可能会出现某些参数很大，而某些参数很小，参数不均匀。**
>
>   **而 L2 最小化则要求所有参数都很小，尽可能接近而不等于 0。越小的参数说明模型越简单，越不易于过拟合。所以 L2 范数可以防止过拟合，提高泛化能力。**

# Smooth L1

## 定义

`Smooth L1` 定义如下所示：

$$
\operatorname{smooth}_{L 1}(x)=\left\{\begin{array}{ll}{\frac{1}{2} x^{2}} & {\text { if }|x|<1} \\ {|x|-\frac{1}{2}} & {\text { otherwise }}\end{array}\right.
$$

## 作用

`L1 Loss` 存在一个问题，就是数据中的异常值可能会对 `loss` 造成很大的干扰。比如说传感器由于信号干扰，导致出现一个远高于正常值的噪点，值为 `100`，其绝对值会对求和结果产生很大影响，比如说梯度爆炸等。  

# L1 和 L2 对比

从 `L1` 和 `L2` 的图像可以看出，`L1` 正则化产生稀疏的权值，`L2` 正则化产生平滑的权值。  

除此之外，`L2` 的收敛速度比 `L1` 快得多。

## 梯度角度分析

$$
\begin{aligned}
&L1 : \frac{\partial L_1}{\partial w_i} = 1 \quad or \quad -1  \\

&L2 : \frac{\partial L_2}{\partial w_i} = w_i
\end{aligned}
$$

也就是说，`L2` 的范数为常数，`L2` 的范数为非零实数。假设学习速率为 $$\alpha = 0.5$$，则：

$$
\begin{aligned}
&L1 : w_i = w_i - 0.5 * 1  \\

&L2 : w_i = w_i - 0.5 * w_i
\end{aligned}
$$

也就是说，`L1` 的梯度会一直减少一个固定值，直到为 `0`；而 `L2` 的梯度每次只会减少原来的 `0.5` 倍，但始终不为 `0`。如对应的函数图像所示。

## 图像对比

`L1` 范数的图像如下所示：

<div style="text-align:center">
<img src="/images/L1 范数图像.png" width="40%"/>
</div> 

`L2` 范数函数图像如下所示：

<div style="text-align:center">
<img src="/images/L2 范数图像.png" width="60%"/>
</div>


综上所述：

1. `L1` 能产生等于 `0` 的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果
2. `L2` 可以得迅速得到比较小的权值，但是难以收敛到 `0`，所以产生的不是稀疏而是平滑的效果。

<div style="text-align:center">
<img src="/images/l1-l2-sl.jpg" width="60%"/>
</div>


## 贝叶斯角度分析

从贝叶斯分析的角度讲，加入正则项相当于加入一种先验。即当训练一个模型时，仅依靠当前的训练数据集是不够的，为了实现更好的泛化能力，往往需要加入先验项。  

`L1` 范数相当于加入了一个 `Laplacean` 先验；`L2` 范数相当于加入了一个 `Gaussian` 先验。

<div style="text-align:center">
<img src="/images/贝叶斯分析 L1 和 L2.png" width="70%"/>
</div>