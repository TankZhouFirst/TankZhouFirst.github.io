---
layout: post
title:  "Ch2 k-近邻算法"
date:   2020-06-02 07:50:01 +0800
categories: 人工智能
tag: 机器学习实战
---

* content
{:toc}

****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- 《机器学习实战 第二章》

****

# 原理概述

> **k-近邻算法通过度量不同特征值之间的距离来进行分类。不足是计算量大，且无法给出数据的内在含义。**

## 基本原理

1. 数据集包含**训练数据**（已知标签）和**测试数据**（未知标签），每个数据均由若干特征构成
2. 对于一个测试数据，计算其与训练数据之间的**特征的欧氏距离**（距离的平方和的开方）
3. 取距离最小的 **k** 个训练数据，出现频次最高的标签，即为测试数据的分类

## 基本特点

- **优点**：精度高，对异常值不敏感
- **缺点**：**计算复杂度高、空间复杂度高**
- **适用**：数值型

## 一般流程

1. **收集数据**：任何手段
2. **数据预处理**：距离计算需要数值，最好是结构化数据
3. **分析数据**：可以使用任何方法
4. **训练算法**：k-邻近算法不包含训练步骤
5. **测试算法**：计算错误率
6. **算法实用**：k-邻近每次进行分类时，都需要计算测试样本与每个训练样本的距离，然后判定类别，并进行后续处理

# 实例讲解

## 问题描述

海伦希望通过算法帮自己删选约会对象属于哪种类型：

- 不喜欢的人
- 魅力一般的人
- 极具魅力的人

删选主要通过如下三个特征进行：

- 每年获得的飞行常客里程数
- 玩视频游戏所消耗时间百分比
- 每周消费的冰淇淋公升数

## 数据准备

首先需要准备数据，将数据解析为数值格式，变为 `numpy` 矩阵。

```python
def file2matrix(self, filename):
    f = open(filename)
    lines = f.readlines()
    shuffle(lines)

    matrix = []
    for line in lines:
        values = line.strip().split('\t')
        try:
            values = [float(v.strip()) for v in values]
        except:
            continue
        matrix.append(values)

return np.array(matrix)[:, :-1], np.array(matrix)[:, -1]
```

## 数据预处理

实际应用中，数据可能存在各种问题，例如：

- 特征值范围不匹配：有的特征值数值较大，有的较小，对结果的影响不同
- 样本分布不匹配：例如样本各类别数目不匹配等

这里，只考虑特征值数值范围的不匹配，需要将其进行归一化，详细代码如下：

```python
def autoNorm(self, features):
    maxVals = features.max(0)
    minVals = features.min(0)
    features = (features - minVals) / (maxVals - minVals)
    return features, minVals, maxVals
```

## k-近邻算法

下面是单个样本的分类代码：

```python
def classify_single_sample(self, sample, features, labels, k):
    distances = ((features - sample) ** 2).sum(1) ** 0.5  # 计算距离
    sorted_indexs = distances.argsort()                   # 排序，并返回索引
    selected_labels = labels[sorted_indexs[:k]]           # 根据索引获取对应的 label

    class_count = {}            # 统计 label
    for label in selected_labels:
        # 若存在，则 + 1；否则设为 0
        class_count[label] = class_count.get(label, 0) + 1

    max_count = -1
    classified_class = 0
    for k, v in class_count.items():
        if v > max_count:
            max_count = v
            classified_class = k
    
    return classified_class
```

## 测试在数据集上的表现

```python
def datingClassifyTest(self, features, labels, test_ratio, k=10):
    split_index = int(len(labels) * test_ratio)

    test_features, train_features = features[:split_index], features[split_index:]
    test_labels, train_labels = labels[:split_index], labels[split_index:]

    test_error = 0
    for test_sample, test_class in zip(test_features, test_labels):
        if self.classify_single_sample(test_sample, train_features, train_labels, k) != test_class:
            test_error += 1

    return test_error / split_index
```

## 完整代码

```python
import numpy as np
from random import shuffle

class knn(object):
    def __init__(self):
        pass


    def file2matrix(self, filename):
        f = open(filename)
        lines = f.readlines()
        shuffle(lines)

        matrix = []

        for line in lines:
            values = line.strip().split('\t')
            try:
                values = [float(v.strip()) for v in values]
            except:
                continue
            matrix.append(values)

        return np.array(matrix)[:, :-1], np.array(matrix)[:, -1]


    def autoNorm(self, features):
        maxVals = features.max(0)
        minVals = features.min(0)
        features = (features - minVals) / (maxVals - minVals)
        return features, minVals, maxVals


    def classify_single_sample(self, sample, features, labels, k):
        distances = ((features - sample) ** 2).sum(1) ** 0.5  # 计算距离
        sorted_indexs = distances.argsort()                   # 排序，并返回索引
        selected_labels = labels[sorted_indexs[:k]]           # 根据索引获取对应的 label

        class_count = {}            # 统计 label
        for label in selected_labels:
            # 若存在，则 + 1；否则设为 0
            class_count[label] = class_count.get(label, 0) + 1

        max_count = -1
        classified_class = 0
        for k, v in class_count.items():
            if v > max_count:
                max_count = v
                classified_class = k

        return classified_class
        

    def datingClassifyTest(self, features, labels, test_ratio, k=10):
        split_index = int(len(labels) * test_ratio)

        test_features, train_features = features[:split_index], features[split_index:]
        test_labels, train_labels = labels[:split_index], labels[split_index:]

        test_error = 0
        for test_sample, test_class in zip(test_features, test_labels):
            if self.classify_single_sample(test_sample, train_features, train_labels, k) != test_class:
                test_error += 1
        
        return test_error / split_index

if __name__=="__main__":
    knn_ins = knn()
    features, labels = knn_ins.file2matrix("datingTestSet2.txt")
    features, minVals, maxVals = knn_ins.autoNorm(features)

    test = np.array([44952, 3.424649, 1.004504])
    test = (test - minVals) / (maxVals - minVals)
    
    knn_ins.classify_single_sample(test, features, labels, 10)

    err_rat = knn_ins.datingClassifyTest(features, labels, 0.1)
    print(err_rat)
```