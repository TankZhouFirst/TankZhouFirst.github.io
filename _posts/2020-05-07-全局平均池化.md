---
layout: post
title:  "全局平均池化"
date:   2020-05-07 08:21:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

## Traditional Pooling Methods

> **池化层的作用在于降采样：保留显著特征、降低特征维度，增大 `kernel` 的感受野，便于提取语义信息**。

深度网络越往后面越能捕捉到物体的语义信息，这种语义信息是建立在较大的感受野基础上。

以盲人摸象为例，每个盲人只能触及到大象的一部分，即 `local response`，基于这些 `local response`，盲人们很难猜对他们到底在摸什么。

也就是说，局部信息很难提供更高层的语义信息，因此对 `feature map` 降维，进而增大后面各层 `kernel` 的感受野是一件很重要的事情。  

另外一点值得注意：`pooling` 也可以提供一些旋转不变性。

## Fully Connected layer

一般来讲，全连接层后面接一个激活函数用于分类（通常为 `softmax` ）等，其中全连接层的作用是将最后一层卷积层得到的 `feature map stretch` 成向量。  

全连接层很重要，稍有不慎，可能造成过拟合，此时可用 `dropout` 等来解决。  

## Global Average Pooling

但是全连接层参数数量过大，降低了训练的速度，且很容易过拟合。由于全连接层需要将 `feature map` 展开成向量，然后进行分类，所以可以直接用 `GAP` 将这两步进行统一。如下图所示：

<div style="text-align:center">
<img src="/images/Global Average Pooling.png" width="80%"/>
</div><br>

如上图所示，`GAP` 的真正意义在于，**对整个网络在结构上做正则化，防止过拟合**。**其直接剔除了全连接层中黑箱的特征，直接赋予了每个 `channel` 实际的内别意义。**

实践证明其效果还是比较可观的，同时 `GAP` 可以实现任意图像大小的输入。但是值得我们注意的是，使用 `gap` **可能会造成收敛速度减慢**。