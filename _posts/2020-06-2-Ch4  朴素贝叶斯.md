---
layout: post
title:  "Ch4  朴素贝叶斯"
date:   2020-06-02 07:50:01 +0800
categories: 人工智能
tag: 机器学习实战
---

* content
{:toc}

****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- 《机器学习实战 第四章》

****

# 原理概述

## 基本原理

利用贝叶斯定理（条件概率），以及强假设（特征独立），朴素贝叶斯可以根据数据集的统计规律，判定新数据的类别，并计算其各类别对应的概率。

## 基本特点

- **优点**：在数据较少的情况下依然有效，可以处理多分类问题
- **缺点**：由于根据数据进行统计，所以对于输入数据的准备方式较为敏感

## 两个假设

1. **特征之间相互独立：统计意义上的独立**
2. **每个特征同等重要**

尽管不能完全满足上面假设，朴素贝叶斯的效果已然足够好。

# 实例讲解

下面以使用朴素贝叶斯进行文档分类为例，进行讲解。文档又可以由词汇组成，可以将词汇的出现与否，当成文档的特征。

## 准备数据：从文本构建词向量

主要包含如下几步：

1. 加载文本集和文本对应的类别，每条文本为一个 `list`
2. 创建词汇表
3. 创建每篇文章对应的词向量

```python
def getDataSet(self, rootdir):
    dataset, labels, keymap = self._loadDataSet(rootdir)
    vocabList = self._createVocabList(dataset)

    VecList = []

    for data in dataset:
        VecList.append(self._Sentence2Onehot(vocabList, data))

    return np.array(VecList), np.array(labels), keymap, vocabList


def _loadDataSet(self, rootdir):
    dirs = os.listdir(rootdir)   # 各类别路径

    dataset = []   # 数据集
    labels = []    # 标签列表
    keymap = {}    # 键值索引

    for index, dirname in enumerate(dirs):
        keymap[index] = dirname

        files = os.listdir(os.path.join(rootdir, dirname))

        for filename in files:
            try:
                content = open(os.path.join(rootdir, dirname, filename), 'r').read()

                # 文本解析，这一部分需要根据文本特性详细处理
                regEx = re.compile('\\W')       # 匹配非（字母数字以及下划线）
                content = regEx.split(content)  # 语句分割
                # 过滤长度小于 3，或数值类型，以及 '_' 的 token
                content = [token.lower() for token in content if ((len(token) > 2)\
                             and (token != "_") and (not str.isdigit(token)))]

                dataset.append(content)
                labels.append(index)
            except:  # 部分文本无法读取，直接跳过
                print("Cannot open file : ", os.path.join(rootdir, dirname, filename))

    indexList = list(range(len(labels)))
    shuffle(indexList)

    dataset = [dataset[i] for i in indexList]
    labels = [labels[i] for i in indexList]

    return dataset, labels, keymap


def _createVocabList(self, dataset):
    vocabSet = []
    for post in dataset:
        vocabSet.extend(post)
    return list(set(vocabSet))

def _Sentence2Onehot(self, vocabList, sentence):
```

## 训练算法：从词向量计算概率

根据贝叶斯定理，有：

$$
p(c_1 | w) = \frac{p(w | c_1)p(c_1)}{p(w)}
$$

其中，$$w$$ 表示待分类的文章，$$c_1$$ 表示类别 $$1$$，$$p(c_1 | w)$$ 表示文章属于类别 $$c_1$$ 的概率。**根据假设特征之间相互独立**，可以将 $$w$$ 分拆为一个个词汇。因此 $$p(c_1|w)$$ 和 $$p(w)$$ 均可通过概率累乘计算得到。 

因此，可以通过文本库，计算 $$p(w_{word} | c1)$$，$$p(c_1)$$ 以及 $$p(w_{word})$$ 的概率。对于新出现的文本，利用贝叶斯定理，拆分成单个词汇，然后计算该文本的类别 及其概率。

> **在数据量很大的时候，根据中心极限定理，频率是等于概率的，这里只是一个例子，所以我就进行统计即可。**

> 需要注意以下两点：
>
> 1. **未现词汇 0 概率**：如果某个词汇出现的概率为 `0`，则会导致条件概率结果为 `0`，因此初始化的时候，可以将每个词汇出现次数默认为 `1`，对应的分母初始化为 `2`。
> 2. **累乘结果下溢**：由于每个词汇对应的概率均较小，累乘可能导致结果为 `0`，因此可取对数，变为累加

```python
def getExperience(self, dataset, labels):
    # 计算 p(c_i)
    totalDocs = len(labels)
    pClass = []
    for class_index in range(len(set(labels))):
        pClass.append(sum(labels == class_index) / totalDocs)
    pClass = np.log(pClass)

    # 统计词频
    pWordsDict = {}
    word_count = [1] * len(dataset[0])
    word_sum = 2
    for doc_index, docs in enumerate(dataset):
        if labels[doc_index] not in pWordsDict.keys():
            pWordsDict[labels[doc_index]] = {}
        pWordsDict[labels[doc_index]]['single_word_count'] = pWordsDict[labels[\
            doc_index]].get('single_word_count', [1] * len(dataset[0])) + docs
        pWordsDict[labels[doc_index]]['word_total'] = pWordsDict[labels[\
                       doc_index]].get('word_total', 2) + sum(docs)
        word_count += docs
        word_sum += sum(docs)

    # 计算 p(w_{word} | c_i) 和 p(w_{word})
    p_words_class = [np.log(pWordsDict[ci]['single_word_count'] / pWordsDict[ci]\
                           ['word_total']) for ci in range(len(set(labels)))]
    p_word = np.log(word_count / word_sum)

    return pClass, p_words_class, p_word
```

## 用于新数据

```python
def classify(self, testVec, pClass, p_words_class, p_word):
    resultList = []
    for class_index in range(len(pClass)):
        p_w_c = sum(testVec * p_words_class[class_index])
        p_c = pClass[class_index]
        resultList.append(p_w_c + p_c - sum(testVec * p_word))
    return resultList.index(max(resultList))
```

## 完整代码

```python
import os
import re
from random import shuffle
import numpy as np

class bayes(object):
    def __init__(self):
        pass

    def getDataSet(self, rootdir):
        dataset, labels, keymap = self._loadDataSet(rootdir)
        vocabList = self._createVocabList(dataset)
        VecList = []
        for data in dataset:
            VecList.append(self._Sentence2Onehot(vocabList, data))
        return np.array(VecList), np.array(labels), keymap, vocabList


    def _loadDataSet(self, rootdir):
        dirs = os.listdir(rootdir)   # 各类别路径
        dataset = []   # 数据集
        labels = []    # 标签列表
        keymap = {}    # 键值索引
        for index, dirname in enumerate(dirs):
            keymap[index] = dirname
            files = os.listdir(os.path.join(rootdir, dirname))
            for filename in files:
                try:
                    content = open(os.path.join(rootdir, dirname, filename),\
                                   'r').read()  # 读取内容
                    # 文本解析，这一部分需要根据文本特性详细处理
                    regEx = re.compile('\\W')       # 匹配非（字母数字以及下划线）
                    content = regEx.split(content)  # 语句分割
                    # 过滤长度小于 3，或数值类型，以及 '_' 的 token
                    content = [token.lower() for token in content if ((len(token) > \
                             2) and (token != "_") and (not str.isdigit(token)))]
                    dataset.append(content)
                    labels.append(index)
                except:  # 部分文本无法读取，直接跳过
                    print("Cannot open file : ", os.path.join(rootdir, dirname, \
                                         filename))
        indexList = list(range(len(labels)))
        shuffle(indexList)
        dataset = [dataset[i] for i in indexList]
        labels = [labels[i] for i in indexList]
        return dataset, labels, keymap

    def _createVocabList(self, dataset):
        vocabSet = []
        for post in dataset:
            vocabSet.extend(post)
        return list(set(vocabSet))

    def _Sentence2Onehot(self, vocabList, sentence):
        returnVec = [0] * len(vocabList)  # 创建空列表
        for word in sentence:
            if word in vocabList:
                returnVec[vocabList.index(word)] = 1
            else:
                print("ignored unrecognized word : ".format(word))
        return returnVec
    
    def getExperience(self, dataset, labels):
        # 计算 p(c_i)
        totalDocs = len(labels)
        pClass = []
        for class_index in range(len(set(labels))):
            pClass.append(sum(labels == class_index) / totalDocs)
        pClass = np.log(pClass)
        # 统计词频
        pWordsDict = {}
        word_count = [1] * len(dataset[0])
        word_sum = 2
        for doc_index, docs in enumerate(dataset):
            if labels[doc_index] not in pWordsDict.keys():
                pWordsDict[labels[doc_index]] = {}
            pWordsDict[labels[doc_index]]['single_word_count'] = pWordsDict[labels[\                   doc_index]].get('single_word_count', [1] * len(dataset[0])) + docs
            pWordsDict[labels[doc_index]]['word_total'] = pWordsDict[labels[\
                  doc_index]].get('word_total', 2) + sum(docs)
            word_count += docs
            word_sum += sum(docs)

        # 计算 p(w_{word} | c_i) 和 p(w_{word})
        p_words_class = [np.log(pWordsDict[ci]['single_word_count'] / pWordsDict[ci]\
                                ['word_total']) for ci in range(len(set(labels)))]
        p_word = np.log(word_count / word_sum)
        return pClass, p_words_class, p_word

    def classify(self, testVec, pClass, p_words_class, p_word):
        resultList = []
        for class_index in range(len(pClass)):
            p_w_c = sum(testVec * p_words_class[class_index])
            p_c = pClass[class_index]
            resultList.append(p_w_c + p_c - sum(testVec * p_word))
        return resultList.index(max(resultList))

if __name__=='__main__':
    rootdir = "AI/Machine Learning/Machine Learning in Action/Ch4  朴素贝叶斯/email"
    beiyesi = bayes()
    dataset, labels, keymap, vocabList = beiyesi.getDataSet(rootdir)
    # 划分数据集
    split_index = int(0.8 * len(labels))
    trainData = dataset[:split_index]
    trainLabels = labels[:split_index]
    testData = dataset[split_index:]
    testLabels = labels[split_index:]
    pClass, p_words_class, p_word = beiyesi.getExperience(trainData, trainLabels)
    error_num = 0
    for index in range(len(testLabels)):
        classified_result = beiyesi.classify(testData[index], pClass, \
                                             p_words_class, p_word)
        if classified_result != testLabels[index]:
            error_num += 1
    print(error_num / len(testLabels))
```