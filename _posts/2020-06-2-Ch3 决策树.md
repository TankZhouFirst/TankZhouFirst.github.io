---
layout: post
title:  "Ch3 决策树"
date:   2020-06-02 07:50:01 +0800
categories: 人工智能
tag: 机器学习实战
---

* content
{:toc}

****

> **未经许可，严禁任何形式的转载！**

****

**参考**

- 《机器学习实战 第三章》

****

# 原理概述

> **决策树的主要优势在于，可以理解数据中蕴含的信息。同时，推理阶段，运算速度快。**

## 基本原理

**每次以一个特征为基准，逐层划分目标，直至达到目标类别。**如下面例子所示：

<div style="text-align:center">
<img src="/images/决策树图示.jpg" width="80%">
<p></p>
</div><br>

## 基本特点

- **优点**：计算复杂度低；输出结果易于理解；对中间值的缺失不敏感；可以处理不相关的特征数据
- **缺点**：可能产生过度匹配的问题
- **适用范围**：数值型

> **备注**：若决策树分支过多，可能会导致过拟合的问题。此时可以通过裁剪决策树来去掉不必要（信息增益不大）的叶子节点。

## 一般流程

### 创建决策树

```python
检测当前数据子集中每个子项是否属于同一类：

if so return 类标签
else:
    寻找划分当前数据子集的最好特征
    划分数据集，并创建分支节点
        for 每个划分的子集:
            递归调用自己，并增加返回结果到分支节点中
    return 分支节点
```

### 使用决策树

根据决策树的特征顺序，逐层比对测试向量对应特征的特征值，从而决定走向，并获得最终的类别。

# 实例讲解

## 信息增益

>  **划分数据集的一大原则是：让无序的数据变的更有序。**可以用**信息论。**度量数据包含的信息。在划分数据集前后，信息发生的变化称之为**信息增益**。而使得信息增益最高的特征划分即为最佳划分选择。

> **集合信息的度量方式称为香农熵，简称熵（entropy），其定义为信息的期望值。**也就是所有类别所有可能值包含的信息期望值：

$$
H = -\sum^n_{i=1}{p(x_i)log_2p(x_i)}
$$

如下代码用于计算数据集的熵：

```python
def calcShannonEnt(self, dataset):
    '''
    计算数据集的熵
    '''
    sample_num = len(dataset)  # 样本总数
    # 统计各类别样本数
    class_count = {}
    for sample in dataset:
        label = sample[-1]
        class_count[label] = class_count.get(label, 0) + 1
    # 计算数据集的熵
    shannonEnt = 0.0
    for k, v in class_count.items():
        p = float(v / sample_num)
        shannonEnt -= p * math.log(p, 2)
    return shannonEnt
```

> **熵越高，表明混合的数据越多。**

## 划分数据集

> **划分数据集的主要目的，是寻找最佳的划分特征，使得划分前后数据集的信息增益最大，从而逐层构建决策树。**

```python
def getBestFeature(self, dataset):
    '''
    获取用于划分数据集的最佳的特征
    '''
    # 数据集划分前的熵
    baseEntropy = self.calcShannonEnt(dataset)

    best_feature_axis = -1   # 最佳特征索引
    best_ingo_gain = 0       # 最佳信息增益
    # 遍历所有特征，分别计算信息增益，获取信息增益最大的特征
    for fea_axi in range(len(dataset[0]) - 1):
        feature_vList = set(dataset[:, fea_axi])  # 获取当前特征的所有可能值
        
        # 按照当前特征的取值划分数据集，构成字典
        datasetDict = {}
        for v in feature_vList:
            # 删除指定列
            datasetDict[v] = np.array(np.delete(dataset[dataset[:, fea_axi] == v], \
                                                fea_axi, axis=1))
        
        # 计算划分后的信息熵
        new_Entroy = 0.0
        for _, subDataset in datasetDict.items():
            p = len(subDataset) / len(dataset)   # 子集的比例
            new_Entroy += p * self.calcShannonEnt(subDataset)   # 子集的熵
        
        # 比较信息熵
        infoGain = baseEntropy - new_Entroy
        if infoGain > best_ingo_gain:
            best_ingo_gain = infoGain
            best_feature_axis = fea_axi
    return best_feature_axis
```

## 创建决策树

> 上面我们只是针对一个特征进行划分，实际情况有多个特征，所以需要**逐级求解最佳划分特征，递归创建决策树。**

> **递归结束的条件为：遍历完所有的属性，或者子集中的数据均为同一类别。**

> **如果属性遍历完毕，但是子集不属于同一类别，则选定数目多的那一类作为该分支的类别。**

```python
def get_majority_class(self, class_list):
    '''
    获取出现数目最多的类别，class_list 为该分支下的样本的类别列表
    '''
    classSet = set(class_list)  # 所有可能的类别

    # 类别计数
    class_cnt = {}
    for c in classSet:
        class_cnt[c] = class_cnt.get(c, 0) + 1
    
    # 挑选最终类别
    fin_class = -1
    fin_class_cnt = 0
    for c in classSet:
        if class_cnt[c] > fin_class_cnt:
            fin_class = c
            fin_class_cnt = class_cnt[c]
    return fin_class


def createTree(self, sub_dataset, feature_labels):
    '''
    递归创建决策树，但是注意，特征以索引形式处理，实用阶段需要对应到具体的标签
    - feature_labels 为特征标签列表
    '''
    # 终止条件 1：子集属于同一类，则返回对应类别
    if len(set(sub_dataset[:, -1])) == 1:
        return sub_dataset[0][-1]
    # 终止条件 2：遍历完所有特征，返回数目最多的类别
    if len(sub_dataset[0]) == 1:
        return self.get_majority_class(sub_dataset)

    # 继续迭代：选择最佳划分特征，并递归构造决策树
    best_feature = self.getBestFeature(sub_dataset)  # 最佳特征
    fea_label = feature_labels[best_feature]         # 最佳特征对应的标签
    # 删除标签（因为子集中已经不包含该属性了）
    feature_labels = np.delete(feature_labels, best_feature)

    feature_vList = set(sub_dataset[:, best_feature])  # 获取当前特征的所有可能值
    # 按照当前特征的取值划分数据集，构成字典
    datasetDict = {}
    for v in feature_vList:
        # 删除指定列
        datasetDict[v] = np.array(np.delete(sub_dataset[sub_dataset[:, best_feature] \
                                                        == v], best_feature, axis=1))

    myTree = {fea_label : {}}
    # 对每个子集递归构建
    for v in feature_vList:
        myTree[fea_label][v] = self.createTree(datasetDict[v], feature_labels)
    
    return myTree
```

## 使用决策树

> 使用决策树时，从上至下，逐步根据特征走到叶子节点，从而判定最终的类别。

> 这里没有使用递归的方式，而是用迭代的方式：根据决策树的特性，根节点一定只有一个特征标签，所以可以层层剥离进行判定。

```python
def classify(self, decisionTree, test_Vec, feature_labels):
    '''
    根据构建好的决策树，检测测试样本向量的类别
    '''
    sub_tree = decisionTree # 存储子树

    while True:
        cur_feature = list(sub_tree.keys())[0]   # 当前特征标签
        index = list(feature_labels).index(cur_feature)  # 获取当前特征标签对应的索引
        cur_val = str(test_Vec[index])              # 获取当前特征值对应的子树
        sub_tree = sub_tree[cur_feature][cur_val]   # 获取子树

        if not isinstance(sub_tree, dict):  # 到达叶子结点
            return sub_tree
```

## 完整代码

```python
import numpy as np
import math


class DecisionTree(object):
    def __init__(self):
        pass

    def calcShannonEnt(self, dataset):
        '''
        计算数据集的熵
        '''
        sample_num = len(dataset)  # 样本总数
        # 统计各类别样本数
        class_count = {}
        for sample in dataset:
            label = sample[-1]
            class_count[label] = class_count.get(label, 0) + 1
        # 计算数据集的熵
        shannonEnt = 0.0
        for k, v in class_count.items():
            p = float(v / sample_num)
            shannonEnt -= p * math.log(p, 2)
        return shannonEnt

    
    def getBestFeature(self, dataset):
        '''
        获取用于划分数据集的最佳的特征
        '''
        # 数据集划分前的熵
        baseEntropy = self.calcShannonEnt(dataset)

        best_feature_axis = -1   # 最佳特征索引
        best_ingo_gain = 0       # 最佳信息增益
        # 遍历所有特征，分别计算信息增益，获取信息增益最大的特征
        for fea_axi in range(len(dataset[0]) - 1):
            feature_vList = set(dataset[:, fea_axi])  # 获取当前特征的所有可能值
            
            # 按照当前特征的取值划分数据集，构成字典
            datasetDict = {}
            for v in feature_vList:
                # 删除指定列
                datasetDict[v] = np.array(np.delete(dataset[dataset[:, fea_axi] == \
                                                            v], fea_axi, axis=1))
            
            # 计算划分后的信息熵
            new_Entroy = 0.0
            for _, subDataset in datasetDict.items():
                p = len(subDataset) / len(dataset)   # 子集的比例
                new_Entroy += p * self.calcShannonEnt(subDataset)   # 子集的熵
           
            # 比较信息熵
            infoGain = baseEntropy - new_Entroy
            if infoGain > best_ingo_gain:
                best_ingo_gain = infoGain
                best_feature_axis = fea_axi
        return best_feature_axis


    def get_majority_class(self, class_list):
        '''
        获取出现数目最多的类别，class_list 为该分支下的样本的类别列表
        '''
        classSet = set(class_list)  # 所有可能的类别

        # 类别计数
        class_cnt = {}
        for c in classSet:
            class_cnt[c] = class_cnt.get(c, 0) + 1
        
        # 挑选最终类别
        fin_class = -1
        fin_class_cnt = 0
        for c in classSet:
            if class_cnt[c] > fin_class_cnt:
                fin_class = c
                fin_class_cnt = class_cnt[c]
        return fin_class


    def createTree(self, sub_dataset, feature_labels):
        '''
        递归创建决策树，但是注意，特征以索引形式处理，实用阶段需要对应到具体的标签
        - feature_labels 为特征标签列表
        '''
        # 终止条件 1：子集属于同一类，则返回对应类别
        if len(set(sub_dataset[:, -1])) == 1:
            return sub_dataset[0][-1]
        # 终止条件 2：遍历完所有特征，返回数目最多的类别
        if len(sub_dataset[0]) == 1:
            return self.get_majority_class(sub_dataset)

        # 继续迭代：选择最佳划分特征，并递归构造决策树
        best_feature = self.getBestFeature(sub_dataset)  # 最佳特征
        fea_label = feature_labels[best_feature]         # 最佳特征对应的标签
        # 删除标签（因为子集中已经不包含该属性了）
        feature_labels = np.delete(feature_labels, best_feature)

        feature_vList = set(sub_dataset[:, best_feature])  # 获取当前特征的所有可能值
        # 按照当前特征的取值划分数据集，构成字典
        datasetDict = {}
        for v in feature_vList:
            # 删除指定列
            datasetDict[v] = np.array(np.delete(sub_dataset[sub_dataset[:, \
                                   best_feature] == v], best_feature, axis=1))

        myTree = {fea_label : {}}
        # 对每个子集递归构建
        for v in feature_vList:
            myTree[fea_label][v] = self.createTree(datasetDict[v], feature_labels)
        
        return myTree


    def createDataset(self):
        dataset = [
            [1, 1, 'yes'],
            [1, 1, 'yes'],
            [1, 0, 'no'],
            [0, 1, 'no'],
            [0, 1, 'no']]
        labels = ['no surfacing', 'flippers']
        return np.array(dataset), np.array(labels)


    def classify(self, decisionTree, test_Vec, feature_labels):
        '''
        根据构建好的决策树，检测测试样本向量的类别
        '''
        sub_tree = decisionTree # 存储子树

        while True:
            cur_feature = list(sub_tree.keys())[0]   # 当前特征标签
            index = list(feature_labels).index(cur_feature)  # 获取当前特征标签对应的索引
            cur_val = str(test_Vec[index])              # 获取当前特征值对应的子树
            sub_tree = sub_tree[cur_feature][cur_val]   # 获取子树

            if not isinstance(sub_tree, dict):  # 到达叶子结点
                return sub_tree


if __name__=='__main__':
    dt = DecisionTree()
    dataset, labels = dt.createDataset()  # 创建数据集
    decisionTree = dt.createTree(dataset, labels) # 创建决策树
    print(decisionTree)
    print(dt.classify(decisionTree, [1, 0], labels))
    print(dt.classify(decisionTree, [1, 1], labels))
```