---
layout: post
title:  "Yolo V1 论文笔记"
date:   2019-08-07 13:32:01 +0800
categories: 人工智能
tag: 目标检测
---


* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****
**参考**

1. **Paper** : [You Only Look Once:Unified, Real-Time Object Detection](<https://arxiv.org/abs/1506.02640>)
2. **主页** : [HomePage](https://pjreddie.com/darknet/yolo/)
3. **简书博客** : [YOLO](<https://www.jianshu.com/p/d535a3825905>)

****

# 引入

## 行业现状

目前大多数的目标检测算法，都是基于 `classifier` 实现的。为了进行目标检测，系统需要使用 `classifier` 对测试图像的不同部分（`ROI`）和尺度分别进行类别推理（重复推理，效率极低）。

在 `DPM (deformable parts models)` 系统中，先在整图中使用滑窗，然后在每个滑窗内执行 `classifier`，来判别是否存在目标。

`R-CNN` 提出的候选区方案 `(Region Proposals)`，首先从图片中搜索出一些可能存在对象的候选区，大概 `2000` 个左右，然后通过 `classifier` 对每个候选区进行选择性搜索（`Selective Search`），判定是否存在目标。最后，使用后续步骤，对 `bounding box` 进行修正，消除重复检测框。

详细可参考笔记《目标检测算法之 `Yolo` 系列》。

各个不同架构的效率如下图所示：

<div style="text-align:center">
<img src="/images/R-CNN 系列速度.jpg" width="80%">
</div><br>

> 这些复杂的流水线式模型，运行缓慢，难于优化，因为其分段式设计，必须对每个组件分别训练。
>
> 分段式实现精度较高，但是效率很低，完全达不到实时的要求。同时，网络需要分开训练，较难收敛。

## 本文主要贡献

本文针对这些目标检测的行业痛点，做出了以下几点改进：

1. 本文重新定义了目标检测任务，将其定义为回归问题（`regression problem`），来独立学习目标的定位和类别的识别。
2. 使用一体式神经网络，只需要对图像进行一趟推理，就可以完成目标的定位，以及类别的判断。速度很快，易于优化。
3. 巧妙的定义了 `anchor box` 和网络的输出形式，以及精巧的 `loss`的计算方式，便于优化和收敛

# Yolo 的优势和不足

## Yolo 的优点

`Yolo` 系统流程如下所示：

<div style="text-align:center">
<img src="/images/Yolo 概览.png" width="90%">
</div><br>

如上图所示，`Yolo` 首先将输入图像 `resize` 到 $$448 \times 448$$；然后对图片进行一次推理，得到输出；最后，进行 `NMS` 等操作过滤多余的检测值。

这种一体式设计，有如下优点：

### 速度极快，易优化

由于是回归问题设计，所以可以进行一体式设计，因此 `YOLO` 能够只读取一次图像，就完成 `bounding box` 和类别的推理，所以速度更快。

这种一体式设计使得网络能够进行端对端优化（`optimized end-to-end`），在保证高精度的同时，速度极快！

在 `Titan X GPU` 上，不进行批量处理的前提下，帧率可达 45，而 `fast yolo` 版本可达  `150 fps`。这意味着，可直接处理视频流，[视频流实例](<https://pjreddie.com/darknet/yolo/>)。

### 背景误判率低

由于 `Yolo` 是对完整的输入图像进行卷积的，因此其综合考虑了图像的上下文，能够更好的挖掘目标的类别和外在表现。而基于 `sliding window` 和 `region proposal` 的架构，其只能聚焦于局部。

因此，`Yolo` 的 `background errors` 只有 `Fast R-CNN` 等结构的一半不到。

### 泛化性能好

也正是由于综合考虑到了图像的全局上下文，所以 `Yolo` 能够更好地学习数据集的本质表达，因而相较于 `DPM` 和 `R-CNN` 等，其泛化性能较好。

### 识别精度高

除了以上优点之外，`Yolo` 的识别精度也很高。

## Yolo 的不足

`Yolo` 也存在不足，其中典型的不足就是，某些目标的定位精度不够，尤其是小目标。

### 密集目标识别效果差

由于每个格点单元仅预测 2 个 `bounding box`，并且只能预测一类目标。因此，`Yolo` 具有较强的空间局限性，即：

> 相互靠近的目标的预测效果不好。特别是多类别目标密集时，只会识别出其中某一种。

### 小目标识别不好

此外，模型对成群的小目标，预测效果并不好。

### 对异常宽长比的目标识别较差

由于模型适应于训练数据中目标的宽长比，因此，对于新目标或者非正常宽长比的目标的识别效果不太好。

### 定位误差大

在 `loss` 中，对 `large bounding box` 和 `small bounding box` 的 `errors` 同等对待。但实际上，同一 `loss` 对于不同尺寸的 `bounding box` 的 `IOU` 的影响是不同的。

因此，`Yolo` 的主要不足源自定位的错误，但是其背景误识别率较低。

# 网络架构设计

## Target 的设计

`Yolo` 将目标检测定义为回归问题，因此，输出应该包含预测框以及类别信息。

### Anchor box 的设计

`R-CNN` 虽然会找到一些候选区，但毕竟只是候选，等真正识别出其中的对象以后，还要对候选区进行微调，使之更接近 `ground truth`。这个过程就是边框回归：将候选区 `bounding box` 调整到更接近真实的 `bounding box`。

`Yolo` 并未真正去掉候选区(`R-CNN` 中的)，而是采用预先设定好的 `Anchor box`。

在 `Yolo` 中，首先将每张图片划分为 $$ S \times S $$ 个格点单元，每个格点单元负责预测 B 个 `bounding box`，共 $$ S \times S \times B $$ 个。可以将其理解为  个 $$ S \times S \times B $$ 候选区，它们很粗略的覆盖了图片的整个区域。

### 目标的归属

设定好了 `Anchor box` 之后，就需要确定目标的归属了，即：用哪一个 `Anchor box` 来锁定目标。这里规定，如果目标的中心落在某个格点单元内，则表明，该格点单元负责预测该目标。如下图所示：

<div style="text-align:center">
<img src="/images/目标归属.jpg" width="71%">
</div><br>

### 预测输出格式的设计

上面从宏观上，讲到了 `Anchor box` 的概念。现在从细节上来探讨每个 `bounding box` （`Anchor box` 对应的实际输出）对应的输出格式。

#### 置信度得分

首先需要确定每个 `box` 是否包含目标，对应的值为置信度的分 (`confidence score`)。

> 置信度的分表征了该 `box` 是否包含目标，以及定位的精度（用与 `ground truth box` 之间的 `IOU` 表示）。


$$
\operatorname{Pr}(Object) * \mathrm{IOU}_{\mathrm{pred}}^{\mathrm{truth}}
$$

在创建 `ground truth` 时，如果格点单元内不包含目标，则 `confidence score` 应该设定为 0；否则应该为 1。(`Label` 对应的 `box` 应该与 `ground truth` 完全重合)。

实际预测输出时，直接预测为一个 `0~1` 的数，表示包含目标且定位精确的可能性。

#### 坐标输出

除了置信度的分，每个 `bounding box` 还应该包含表示位置的坐标，`Yolo` 中用中心坐标和尺寸表示目标的位置，即：$$x, y, w, h$$。

其中，$$ x, y $$ 表示的是目标中心点与所属格点单元边界之间的坐标，数值相对于格点单元进行了归一化。$$ w, h $$ 为 `bounding box` 的尺寸，也是相对于原图进行了归一化的尺寸。如下图所示：

<div style="text-align:center">
<img src="/images/Yolo V1 坐标变换.jpg" width="91%">
</div><br>

其中，中心坐标的计算方式如下所示：

$$
x = S * \frac{x_c}{width_{image}}  - col = \frac{x_c}{width_{grid}}  - col  \\

y = S * \frac{y_c}{height_{image}}  - row = \frac{y_c}{height_{grid}}  - row
$$

#### 类别预测的输出

由于并不是所有的 `bounding box` 均包含目标，所以类别的概率也是一个条件概率，条件即格点单元是否包含目标，因此，概率为：

$$
\operatorname{Pr}\left(Class_{i} |Object \right)
$$

> 本文有一个设定，就是：每个格点单元只负责预测一个目标，而不是 `B` 个。所以，每个格点单元只输出一组类别的概率。

> 这一设定导致 `Yolo` 对于密集目标的识别效果并不好。

结合上面的部分，可以得到，一个 `bounding box` 是否包含某类目标的置信度为：

$$
\operatorname{Pr}\left(Class_{i} |Object \right) * \operatorname{Pr}(Object) * \operatorname{IOU}_{\text { pred }}^{\text { truth }}=\operatorname{Pr}\left(Class_{i}\right) * IOU pred
$$

这一得分同时包含了 `predicted box` 中包含某类目标的概率，以及 `predicted box` 对目标的定位精度。

#### 非极大值抑制  NMS

非极大抑制（`Non-Maximum Suppression`，`NMS`） 的核心思想是：选择置信度得分最高的作为输出，去掉与该输出重叠较高的预测框，不断重复这一过程直到处理完所有备选框（共 $$ S \times S \times B $$ 个）。

具体步骤如下所示：

1. 过滤掉 `confidence score` 低于阈值的 `bounding box`
2. 遍历每一个类别
   1. 找到置信度最高的 `bounding box`，将其移动到输出列表
   2. 对每个 `Score` 不为 `0` 的候选对象，计算其与上面输出对象的 `bounding box` 的 `IOU`
   3. 根据预先设置的 `IOU` 阈值，所有高于该阈值（重叠度较高）的候选对象排除掉
   4. 当剩余列表为 `Null` 时， 则表示该类别删选完毕，继续下一个类别的 `NMS`
3. 输出列表即为预测的对象

<div style="text-align:center">
<img src="/images/Yolo V1 推理过程.png" width="91%">
</div><br>

### 总结

综上所述，`Yolo` 模型的预测输出尺寸为：$$ S \times S \times (B * 5 + C) $$。格式如下所示：

<div style="text-align:center">
<img src="/images/Yolo V1 预测值.jpg" width="90%">
</div><br>

在 `Pascal VOC` 数据集上，评估 `Yolo` 时，$$ S = 7, B=2$$，由于 `VOC` 有 20 个类别，所以 $$C=20$$ 。因此，最终对于每张图片的预测输出尺寸为：$$ 7 \times 7 \times 30 $$。

## Loss 的设计

上面讲到了输出的格式，下面讲 `loss` 的计算。

最后以输出的求和平方误差（ `sum-squared error` ）进行优化。之所以用 `sum-squared error`，是因为其易于优化。但是与我们最大化平均精度的目标并不完全契合。因为它对定位误差与分类误差之间的权衡可能并不理想。

### 存在的问题及解决方式

对于每张图片，其中大多数格点单元不包含目标，其对应的置信度得分为 0。这种目标存在与否的失衡，将会影响最后 `loss` 的计算，从而影响包含目标的格点单元的梯度，导致模型不稳定，训练容易过早收敛。

因此，我们增加 `bounding box` 坐标对应的 `loss`，同时对于不包含目标的 `box`，降低其置信度对应的 `loss`。我们用 $$\lambda_{coord}$$ 和 $$\lambda_{noobj}$$ 来实现这一功能，且：$$ \lambda_{coord}=5,  \lambda_{noobj}=0.5$$ 。

同时，`sum-squared error` 还会同等看待 `large boxes` 和 `small boxes` 的 `loss` 。而同等的 `loss` 对于 `large boxes` 和 `small boxes` 的影响是不同的。

为了减缓这种空间上的不均衡，我们选择预测 `w` 和 `h` 的平方根，可以降低这种敏感度的差异，使得较大的对象和较小的对象在尺寸误差上有相似的权重。

### loss 解析

`Yolo` 的完整 `loss` 如下所示：

$$
\begin{aligned}
loss = & \lambda_{\text { coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text { obj }}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right] \\

+ & \lambda_{\text { coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text { obj }}\left[\left(\sqrt{w_{i}}-\sqrt{\hat{w}_{i}}\right)^{2}+\left(\sqrt{h_{i}}-\sqrt{\hat{h}_{i}}\right)^{2}\right] \\

+ & \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{obj}}\left(C_{i}-\hat{C}_{i}\right)^{2} \\ 

+ & \lambda_{\text { noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{noobj}}\left(C_{i}-\hat{C}_{i}\right)^{2} \\

+ & \sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{\mathrm{obj}} \sum_{c \in \mathrm{clases}}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}

\end{aligned}
$$

其中（以 `ground truth` 为判定依据）：

- $$ \mathbb{1}_{i}^{\mathrm{obj}}$$ 表示是否格点单元 $$ i $$ 中包含目标；
- $$ \mathbb{1}_{i j}^{\mathrm{obj}} $$ 表示格点单元 $$ i $$ 中，第 $$ j $$ 个预测的 `bounding box` 包含目标
- $$1_{ij}^{noobj}$$  意思是网格 i 的第 j 个 `bounding box` 中不存在对象

因此，上面的 `loss` 中：

- 第一行表示：当第 i 个格点中第 j 个 `box` 中存在目标 (`IOU` 比较大的 `bounding box`) 时，其坐标误差
- 公示的第二行表示：第 i 个格点中第 j 个 `box` 中存在目标时，其尺寸误差
- 公示的第三行表示：第 i 个格点中第 j 个 `box` 中存在目标时，其置信度误差
- 公示的第四行表示：第 i 个格点中第 j 个 `box` 中不存在目标时，其置信度误差
- 公示的第五行表示：第 i 个格点中存在目标时，其类别判定误差

## 神经网络的架构设计

`Yolo` 的前半部分卷积网络用于特征提取，后续的全连接层用于预测输出概率和坐标。

`Yolo` 灵感来自于 `GoogLeNet`，`Yolo` 包含 `24` 层卷积层，以及 `2` 层全连接层。与 `GoogLeNet` 不同，在 $$ 3 \times 3 $$ 的卷积层之后，我们只使用 $$ 1 \times 1 $$ 的 `reduction` 层。完整的网络结构如下图所示：

<div style="text-align:center">
<img src="/images/Yolo v1 网络结构.png" width="93%">
</div><br>

此外，`Fast yolo` 版本中，只使用 `9` 层卷积层，以及更少的滤波器。其他部分与 `Yolo` 一模一样。

对于卷积网络部分，先在 `ImageNet` 分类数据集上进行预训练，输入尺寸为 $$ 224 \times 224 $$，然后输入尺寸加倍，用于目标检测任务。

# 训练与推理

## 模型训练

首先，在 `ImageNet 1000-class` 比赛数据集上，对网络的前 `20` 层（外加一层均值池化层，以及一个全连接层），进行预训练，输入尺寸为 $$ 224 \times 224 $$。训练约一周后，在 `ImageNet-2012` 验证集上的`top-5` 精度为 `88%`，与 `GoogLeNet` 模型相当。训练和推理，全程使用 `darknet` 框架。

接着，将模型用于目标检测的训练。先保留预训练的卷积层部分，然后补全网络（这部分使用随机初始化），输入尺寸变为 $$ 448 \times 448 $$。

除输出层外，每一层使用 `Leaky ReLU` 作为激活函数。

$$
\phi(x)=\left\{\begin{array}{ll}{x,} & {\text { if } x>0} \\ {0.1 x,} & {\text { otherwise }}\end{array}\right.
$$

我们将网络在 `Pascal VOC 2007` 和 `Pascal VOC 2012` 的训练集和验证集上，训练了 `135` 个 `epoch`。当在 `2012` 的测试集上进行测试时，也将 `2007` 的测试集用于训练。

整个训练进程中，`batch_size=64`，`momentum=0.9`，`decay=0.0005`。学习速率变化如下：首先缓慢将学习速率从 $$ 10^{-3} $$ 升至 $$ 10^{-2} $$，因为一开始用较大的学习速率，由于梯度不稳定，可能会导致训练发散。接着，用 $$ 10^{-2} $$ 训练 75 个 `epochs`，以及 $$ 10^{-3} $$ 训练 30 个 `epochs`，最后用 $$ 10^{-4} $$ 训练 30 个 `epochs`。

为了避免过拟合，我们使用了 `dropout` （系数为 `0.5`）以及数据增强。其中，数据增强包括：随机缩放，在图像的 $$ HSV $$ 颜色空间中，以 `1.5` 的因子，随机调整曝光和饱和度。

## 模型推理

在 `Pascal VOC` 数据集上，对于每张图片，预测 $$ 7 \times 7 \times 2 =  98 $$ 个 `bounding box`。对每个 `bounding box`，预测一组 `class probabilities`。

这种格点设计，增强了 `bounding box` 预测的空间多样性。通常可以清楚地预测出目标坐落于哪个格点单元，对于每个目标，网络只预测一个 `bounding box`。但是，一些较大的目标，或者临近多格点边界的目标，可能会被预测属于多个格点单元。此时可以使用 `Non-maximal supression` 来修正多预测输出。

# 实验结果分析

## 与其他 Real-Time 系统的对比

<div style="text-align:center">
<img src="/images/Yolo V1 result1.png" width="70%">
</div><br>


如上表所示，为各种 `Real-Time` 系统在 `Pascal VOC 2007` 上的表现。

## VOC 2007 Error Analysis

判定标准：

> 1. Correct :  correct class and IOU > 0.5
> 2. Localization :  correct class, 0.1 < IOU < 0.5
> 3. Similar :  class is similar, IOU > 0.1
> 4. Other: class is wrong,  IOU > 0.1
> 5. Background: IOU < 0.1 for any object

<div style="text-align:center">
<img src="/images/Error Analysis.png" width="80%">
</div><br>


## Combining Fast R-CNN and YOLO

由于 `Yolo` 的背景误差较小，因此可以使用 `Yolo` 消除 `Fast R-CNN` 中的背景，使得性能有较大提升。如下所示：

<div style="text-align:center">
<img src="/images/yolo combined.png" width="71%">
</div><br>


## VOC 2012 Results

<div style="text-align:center">
<img src="/images/yolo VOC 2012 Results.png" width="97%">
</div><br>

## Generalizability: Person Detection in Artwork

<div style="text-align:center">
<img src="/images/Generalizability.png" width="99%">
</div><br>

