---
layout: post
title:  "常用优化器"
date:   2019-08-20 15:07:01 +0800
categories: 人工智能
tag: 深度学习基础
---

* content
{:toc}


****

> **未经许可，严禁任何形式转载**

****

**参考**：

- [深度学习中 7 种最优化算法的可视化与理解](<https://zhuanlan.zhihu.com/p/41799394>)
- [深度学习最全优化方法总结比较](<https://zhuanlan.zhihu.com/p/22252270>)

****

## 引入

### 优化中的问题

在深度学习中，有很多种优化算法，这些算法需要在极高维度（通常参数有数百万个以上）也即数百万维的空间进行梯度下降，从最开始的初始点开始，寻找最优化的参数，通常这一过程可能会遇到多种的情况，诸如：

1. 提前遇到局部最小值从而卡住，再也找不到全局最小值了
2. 遇到极为平坦的地方：“平原”，在这里梯度极小，经过多次迭代也无法离开。同理，鞍点也是一样的，在鞍点处，各方向的梯度极小，尽管沿着某一个方向稍微走一下就能离开。
3. “悬崖”，某个方向上参数的梯度可能突然变得奇大无比，在这个地方，梯度可能会造成难以预估的后果，可能让已经收敛的参数突然跑到极远地方去。

### 实例

为了可视化以及更好的理解这些优化算法，我首先拼出了一个很变态的一维函数：

$$
f(x)=(0.15x)^2 + cos(x) + sin(3x)/3 + cos(5x)/5 + sin(7x)/7
$$

其导数如下：

$$
f(x) = 0.045x - sin(x) + cos(3x) - sin(5x) + cos(7x)
$$

其函数图像如下所示，具有悬崖和大量的局部最小值，足以模拟较为复杂的优化情况了：

<div style="text-align:center">
<img src="/images/函数图像.png" width="80%"/>
</div>

## 随机梯度下降 SGD

### 说明

`mini-batch gradient descent`，标准的随机梯度下降算法。

### 计算方式与图示

**计算方式**

```python
while True：
    x = x - lr * df/dx
```

$$
w^{'} = w - lr * \frac{\partial l}{\partial w}
$$

上面的公式中，`w` 表示前一时刻的权值参数，`lr` 表示学习速率；$$ \frac{\partial l}{\partial w} $$ 表示当前梯度。

**图示**

<div style="text-align:center">
<img src="/images/SGD 图示.gif" width="80%"/>
</div>

### 优缺点

> 根据学习率的不同，可以看到不同的效果。学习率过小时，卡在局部极小值；学习率过大，压根不收敛。

## 随机梯度下降 SGD + 动量

### 说明

加入动量 (`momentum`) 之后，会累计历史**权值参数信息**。即：当前有效梯度会综合考虑历史**权值参数信息**，以及当前即时**梯度信息**。其中，历史权值信息的影响通过动量系数进行控制。

动量系数 $$m = 0$$ 时，等效于 `SGD`。

### 计算方式与图示

**计算方式**

```python
首先给出学习率 lr，动量参数 m
初始速度 v=0, 初始 x
while True:
    v = m * v - lr * df/dx
    x += v
```

> 上面公示表示，每次更新当前权值参数时，会综合考虑历史权值参数信息，以及当前梯度。

**图示**

<div style="text-align:center">
<img src="/images/SGD + M + 0.05.gif" width="80%"/>
</div><br>

<div style="text-align:center">
<img src="/images/SGD + M + 0.1.gif" width="80%"/>
</div><br>

<div style="text-align:center">
<img src="/images/SGD + M + 0.002.gif" width="80%"/>
</div>

从上图可以看出：

1. `lr` 越小越稳定，太大了很难收敛到最小值上，但是太小的话收敛就太慢了
2. 动量参数不能太小，`0.9` 以上表现比较好，但是又不能太大，太大了无法停留在最小值处

### 优缺点

1. 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的动量系数 $$ m $$ 能够进行很好的加速训练
2. 下降中后期时，在局部最小值来回震荡的时候，$$ \text {gradient} \rightarrow 0 $$，动量系数使得更新幅度增大，跳出局部最优
3. 在梯度改变方向的时候，动量能够减少更新。总而言之，`momentum` 项能够在相关方向加速 `SGD`，抑制振荡，从而加快收敛

## AdaGrad 算法

### 说明

> `AdaGrad` 算法的基本思想是根据**累计历史梯度**，来自动调节学习速率，从而自动调节权值参数的变化速度。

### 计算方式与图示

**计算方式**

```python
# 给出学习率 lr，delta=1e-7
# 累计梯度 r = 0，初始 x

while True：
      g = df / dx
      r  = r + g * g
      x = x - lr / (delta + sqrt(r)) * g
```

$$
w = w - \frac{lr}{\delta + \sqrt{r}} * g
$$

$$ \delta $$ 项用于防止分母为 0。

- 前期 $$g$$ 较小时，$$r$$ 也较小，能够放大学习速率，从而放大梯度
- 后期 $$ g $$ 较大时，$$ r $$ 也较大，能够缩小学习速率，从而缩小梯度
- 适合处理稀疏梯度

**图示**

<div style="text-align:center">
<img src="/images/AdaGrad.gif" width="80%"/>
</div><br>

### 优缺点

- 由公式可以看出，仍依赖于人工设置一个全局学习率
- 学习速率设置过大的话，会使调节部分过于敏感，对梯度的调节太大
- 中后期，累计梯度过大，使得学习速率趋于零，使得训练提前结束

## RMSProp

### 说明

`AdaGrad` 算法在前期可能会有很大的梯度，自始至终都保留了下来，这会使得后期的学习率过小。

`RMSProp` （`Root Mean Square prop`）在这个基础之上，加入了平方梯度的衰减项，只能记录最近一段时间的梯度，在找到碗状区域时能够快速收敛。

### 计算方式与图示

**计算方式**

```python
# 给出学习率 lr，delta = 1e-6，衰减速率 p
# 累计梯度 r = 0，初始 x
while True：
      g = df/dx
      r  = p * r + （ 1 - p ）* g * g   # 关键
      x = x - lr / (delta + sqrt(r)) * g
```

**图示**

<div style="text-align:center">
<img src="/images/RMSProp p=0.99.gif" width="80%"/>
</div>
<div style="text-align:center">p = 0.99</div><br>
<div style="text-align:center">
<img src="/images/RMSProp p=0.9.gif" width="80%"/>
</div>
<div style="text-align:center">p = 0.9</div><br>
<div style="text-align:center">
<img src="/images/RMSProp p=0.8.gif" width="80%"/>
</div>
<div style="text-align:center">p = 0.8</div>
### 优缺点

- 具有 `AdaGrad` 的优点，同时规避了其不足。
- 仍然依赖全局学习速率
- 适合处理非平稳目标 - 对于 `RNN` 效果很好

## Adam 算法

### **说明** 

`Adam` (`Adaptive Moment Estimation`) 本质上就是带有动量项的 `RMSprop`。

算法和之前类似，也是自适应减少学习率的算法，不同的是它更新了一阶矩和二阶矩。其中，一阶矩有点像有动量的梯度下降，而用二阶矩来降低学习率。  

> `Adam` 的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。

### 计算方式和图示

**计算方式**

```python
# 给出学习率 lr，delta = 1e-8，衰减速率 p1 = 0.9，p2 = 0.999 
# 累计梯度 r = 0，初始 x ,一阶矩 s = 0，二阶矩 r = 0
# 时间 t = 0

while True：       
    t += 1
    g = df/dx 
    s = p1 * s + ( 1 - p1 ) * g
    r = p2 * r +（ 1 - p2 ）* g * g   

    s = s / (1 - p1^t )
    r = r / (1 - p2^t )    

    x = x - lr / ( delta + sqrt(r)) * s
```

上面的公式中，`t` 较小的时候，$$1 - p1^t$$ 较小，会使得 $$s = s / (1-p1^t)$$ 急剧增大，从而让梯度更大，参数跑的更快，迅速接近期望点。运行一段时间后，t 较大，$$1 - p1^t$$ 较大，接近于 `1`，因此 s 几乎不受 `t` 影响。

**图示**

<div style="text-align:center">
<img src="/images/Adam bad.gif" width="80%"/>
</div>
<div style="text-align:center">Adam 鬼一样的表现</div><br>
上图表现不佳，主要是因为在 `t` 很小的前几步的时候，`p2 = 0.999` 太大了，导致 $$r = r / (1 - p2^t)$$ 中，$$1-p2^t$$ 接近 `0`，`r` 迅速爆炸，百步之内到了 `inf`。后来修改 `p2=0.9` 后效果就好的多了。  

<div style="text-align:center">
<img src="/images/Adam good.gif" width="80%"/>
</div>
<div style="text-align:center">Adam 神级表现</div><br>
最后还是 `Adam` 效果最好了，尽管学习率还是需要相当的调参.

### 优缺点

- 结合了 `Adagrad` 善于处理稀疏梯度和 `RMSprop` 善于处理非平稳目标的优点
- 对内存需求较小
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化 - 适用于大数据集和高维空间

## 经验之谈

- 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
- `SGD` 通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
- 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
- `RMSprop`，`Adam` 是比较相近的算法，在相似的情况下表现差不多。
- 在想使用带动量的 `RMSprop`，或者 `Adam` 的地方，大多可以使用 `Nadam`取得更好的效果