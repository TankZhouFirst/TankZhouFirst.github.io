---
layout: post
title:  "利用 GAN 生成动漫头像"
date:   2019-08-15 18:51:01 +0800
categories: 一些实践
tag: 
---

* content
{:toc}


****

> **未经许可，严禁任何形式的转载！**

****

**资源**

- 数据：https://share.weiyun.com/561Hd5s， `g9t35x`
- 源码：https://github.com/TankZhouFirst/use-gan-to-generate-Anime-avatar.git

****

## 基本介绍

本项目中，将利用 `GANs` ，学习动漫头像的数据分布，并用训练得到的生成器，从随机噪声中，生成动漫头像。

本项目的核心在于，生成器网络和判别器网络的设计，以及训练过程迭代源码的编写，其余的部分，并无难度。

示例图片如下，图片尺寸为 `96 * 96`：

<div style="text-align:center">
<img src="/images/1.jpg" width="20%"/>
<img src="/images/2.jpg" width="20%"/>
<img src="/images/3.jpg" width="20%"/>
<img src="/images/4.jpg" width="20%"/>
<p>实例头像</p>
</div><br>

## 模型

这里要使用 `GAN` 生成动漫头像，需要定义两个网络，分别为生成器网络和判别器网络。

其中，判别器网络与通常的二分类网络结构一样，没有区别。判别器网络以随机噪声为输入，最后输出尺寸为 `3 * 96 * 96` 的图像，即生成的数据。

具体网络结构如下，参考源码中 `src/model` 文件中的定义。需要注意网络的输入输出尺寸，及其数值范围。

## 一些基本组件定义

### Dataset 和 Dataloader

与正常的图像分类等定义方式一样，这里就不展开了，详见 `src/train.py/create_dataloader` 方法。

### 优化器

由于存在两个模型，所以需要分别进行定义。一般生成器使用 `Adam` 优化器，判别器使用 `SGD` 优化器。

### 损失函数

这里直接使用 `BCELoss` 作为损失函数。

需要了解生成器和判别器的优化原理：**内循环优化 `D`，外循环优化 `G`**，但是，`loss` 均来自于 `D`。

### 其他

其他部分，包括模型保存，模型恢复等等，详见源码 `src/train.py`。

## 训练过程

### 定义 `target`

首先需要为生成器定义 `targets`。生成器的输入可以来自原始数据，对应的 `target` 为 `1`，但是为了使用 `smooth label` 改善性能，因此设置为 `0.8` 或 `0.9`。

而对于来自判别器生成的数据，对应的 `target` 为 `0`，此处不可更改，根据原论文，更改它可能会导致学习到的数据分布发生偏移。

```python
self.true_labels = torch.ones(opt.batch_size) * 0.8    # 训练集合的数据对应的标签
self.fake_labels = torch.zeros(opt.batch_size)         # 生成器生成的数据对应的标签
```

### 每一步迭代

在每一个 `epoch` 中，训练的流程如下。

#### 训练判别器 D

在每 `d_iter_step` 次迭代后，进行判别器的学习，详细见注释。需要注意的一点是，在训练判别器时，需要用到生成器生成的伪数据。而这一步中，生成器只需要生成数据，不参与梯度更新，所以一定要用 `detach` 函数，进行梯度剥离。

此外，分别用两类 `batch` 训练判别器时，需要分别进行反向传播，不可合并 `loss` 后进行。

```python
if step % opt.d_iter_step == 0:
    self.optimD.zero_grad()   # 清空梯度

    # 用真实数据训练 D
    out_D_true  = self.netD(real_images)
    loss_D_true = self.criterion(out_D_true, self.true_labels)
    loss_D_true.backward()

    # 用生成的伪数据训练 D
    # 这里，先用指定维度的高斯随机噪声，输入到生成器，产生伪数据
    noise_data = torch.randn(opt.batch_size, opt.noise_dim, 1, 1)
    if opt.use_gpu and torch.cuda.is_available():
        noise_data = noise_data.cuda()

    # 这个 detach 很重要，因为在这里，生成器只负责生成 image，不负责梯度计算
    fake_img    = self.netG(noise_data)
    out_D_fake  = self.netD(fake_img.detach())
    loss_D_fake = self.criterion(out_D_fake, self.fake_labels)
    loss_D_fake.backward()

    # 更新参数
    self.optimD.step()

    # 总损失
    loss_D = loss_D_fake + loss_D_true

    # 加入到统计信息
    self.errord_meter.add(loss_D.item())
```

#### 训练生成器

在每 `g_iter_step` 之后，训练一次生成器，详细见下面注释。

首先输入随机高斯噪声，使用生成器 `G` 来生成伪数据。然后使用判别器对其进行判定。由于生成器的目的是使得，判别器尽可能误认为生成的数据为真实数据，所以对应的 `label` 应该为 `true` 样本的 `label`。

```python
if step % opt.g_iter_step == 0:
    self.optimG.zero_grad()

    # 用生成的数据来训练模型
    noise_data = torch.randn(opt.batch_size, opt.noise_dim, 1, 1)
    if opt.use_gpu and torch.cuda.is_available():
        noise_data = noise_data.cuda()

    # 生成数据
    fake_imgs = self.netG(noise_data)
    out_GD    = self.netD(fake_imgs)
    # 这里要使得 G 的输出尽可能接近训练样本，所以用 self.true_labels
    loss_G    = self.criterion(out_GD, self.true_labels)
    loss_G.backward()
    self.optimG.step()

    # 加入到统计信息
    self.errorg_meter.add(loss_G.item())

    # 统计信息
    if step % opt.summary_step == 0:
      print("[{} / {}] : loss of generator is {}".format(self.start_epoch,
                                                opt.max_epoch,
                                                self.errord_meter.value()[0]))
      print("[{} / {}] : loss of discriminator is {}".format(self.start_epoch,
                                                opt.max_epoch,
                                                self.errorg_meter.value()[0]))

      self.writer.add_scalar("loss_D", self.errord_meter.value()[0], self.global_step)
      self.writer.add_scalar("loss_G", self.errorg_meter.value()[0], self.global_step)
```

#### 验证生成模型

在训练过程中，需要查看生成器在每个 `epoch` 后的表现。因此，需要进行验证。方法是，利用固定噪声源，生成动漫头像，查看头像质量的变化。

```python
with torch.no_grad():
    fixed_fake_images = self.netG(self.fix_noises)
    # normalize 反预处理
    fixed_fake_images = fixed_fake_images.cpu()[:64] * 0.5 + 0.5

    self.writer.add_images("generated_images_{}".format(self.start_epoch),
                           fixed_fake_images)
```

### 训练过程中，生成数据的变化

直接执行 `test/train_test.py` 即可开始训练，可以通过文件 `src/config.py` 修改配置。

源码中使用了 `tensorboardX` 引入可视化，可以查看训练中曲线的变化，以及生成的图像实例。

在命令行输入 `tensorboard --logdit='runs'` 即可查看相关记录。这里，我添加了 `G` 和 `D` 的 `loss` 曲线，以及每次 `epoch` 对应的图像（`64` 张）。`tensorboard` 的运行需要安装 `tensorflow` 和 `tensorboardX` 等依赖。

训练过程中的参数配置，对训练收敛速度和训练结果影响甚大，比如说论文中所对应的 `k` 值。

下面是不同 `epoch` 内，对于同一固定随机噪声，生成器对应的输出，表现并不怎么好，也没有再花时间调了。

<div style="text-align:center">
<img src="/images/g10.png" width="50%"/>
<p>epoch = 10</p>
<img src="/images/g50.png" width="50%"/>
<p>epoch = 50</p>
<img src="/images/g100.png" width="50%"/>
<p>epoch = 100</p>
<img src="/images/g190.png" width="50%"/>
<p>epoch = 190</p>
</div><br>

如下图所示，为训练过程中，两个子网络的 `loss` 曲线，可以看出，训练效果真心不怎么样。以前训练过一次，效果比这好多了，今天的训练都不太好。

<div style="text-align:center">
<img src="/images/loss 曲线.png" width="90%"/>
<p>loss 曲线</p>
</div><br>

## 模型评估

评估过程比较简单，就是加载预训练好的参数，以服从指定均值和标准差（一般为 `0` 和 `1`）的高斯分布的随机噪声为生成器的输入，然后从中选择表现最佳（由判别器判定）的 `top-k` 张图片。

直接运行 `test/eval_test.py` 即可，需要修改部分参数（权值路径），结果存储于 `result.png` 文件中。

结果如下所示。可以发现，很鬼畜，还没有上面的结果好。应该是判别器本身就没训练好吧。有时间再调参数，今天就这样了。

<div style="text-align:center">
<img src="/images/result.png" width="80%"/>
<p>result</p>
</div><br>